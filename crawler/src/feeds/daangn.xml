
<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[당근 테크 블로그 - Medium]]></title>
        <description><![CDATA[동네를 여는 문, 당근. 로컬의 모든 것을 연결해, 동네의 숨은 가치를 깨워요. - Medium]]></description>
        <link>https://medium.com/daangn?source=rss----4505f82a2dbd---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>당근 테크 블로그 - Medium</title>
            <link>https://medium.com/daangn?source=rss----4505f82a2dbd---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Sat, 27 Dec 2025 03:46:24 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/daangn" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Karrot’s Generative AI Platform]]></title>
            <link>https://medium.com/daangn/karrots-genai-platform-5cf6e813838e?source=rss----4505f82a2dbd---4</link>
            <guid isPermaLink="false">https://medium.com/p/5cf6e813838e</guid>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[ai-tools]]></category>
            <category><![CDATA[genai]]></category>
            <category><![CDATA[ai-agent]]></category>
            <dc:creator><![CDATA[Tommy Park]]></dc:creator>
            <pubDate>Mon, 22 Dec 2025 08:42:15 GMT</pubDate>
            <atom:updated>2025-12-23T01:37:11.535Z</atom:updated>
            <content:encoded><![CDATA[<p>When we first shared <a href="https://medium.com/daangn/%EB%8B%B9%EA%B7%BC%EC%97%90%EC%84%9C-llm-%ED%99%9C%EC%9A%A9%ED%95%98%EA%B8%B0-76131ecebce1">Using LLMs at Karrot</a> in early 2024, we were just beginning our journey with large language models.</p><p>A year and a half later, the landscape has transformed dramatically. AI models have advanced beyond what we could have imagined, and Karrot has scaled to hundreds of GenAI use cases across products and data pipelines.</p><p>In this post, we’ll share the challenges we encountered while scaling GenAI adoption and the platforms we built to address them. We hope our learnings help others navigating similar paths.</p><h3>Platform Overview</h3><p>GenAI powers services across Karrot’s entire stack — from user-facing features to internal data pipelines. Behind the scenes, three core platforms enable this adoption.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*L0jFW-TABAo58z-3.png" /><figcaption>Karrot’s GenAI Platform</figcaption></figure><p>While the architecture may seem complex at first glance, LLM Router, Prompt Studio, and KarrotChat each evolved to solve distinct problems. Let’s dive into what each platform addresses and how we built them.</p><h3>LLM Router</h3><h4>The Account Management Problem</h4><p>In the early days of LLM adoption, teams provisioned their own accounts and API keys whenever they needed AI capabilities. With models from OpenAI, Anthropic, Google, and others, accounts and keys proliferated rapidly — and so did the management overhead. Several pain points emerged:</p><p><strong>Provisioning became a bottleneck.</strong> As more teams adopted AI, the account and API key setup process itself became a friction point. With numerous keys in circulation, tracking which team used which credentials for what service became increasingly difficult, raising security concerns.</p><p><strong>Rate limits created uneven availability.</strong> AI APIs typically enforce per-account rate limits. With teams operating independently, some accounts had plenty of headroom while others hit their limits, causing request failures.</p><p><strong>Cost visibility was fragmented.</strong> As accounts and keys multiplied, aggregating usage and costs required querying multiple sources. Getting a unified view of Karrot’s total AI spend became time-consuming.</p><h4>Unifying Through an API Gateway</h4><p>We built <strong>LLM Router</strong> to solve these challenges. The core concept: funnel all AI API calls through a single gateway. API keys and accounts are managed centrally by LLM Router, while individual services authenticate with just a service ID — no separate credentials required.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CcojDlnE9H__nPM6.png" /></figure><p>This unlocked several improvements:</p><p><strong>Eliminated provisioning overhead.</strong> The account and API key setup process disappeared entirely. We recalled most previously issued keys (with a few exceptions), and teams can now access any supported API immediately.</p><p><strong>Centralized operations.</strong> With all traffic flowing through one point, our infrastructure team took over unified account management. Rate limit monitoring and quota adjustments became their responsibility, freeing product teams to focus on building features.</p><p><strong>Unified cost visibility.</strong> Every request through LLM Router is attributed to its originating service. This data feeds into Karrot’s infrastructure cost management platform, providing at-a-glance visibility into which services consume which models — and at what cost.</p><h4>One Interface, Every Model</h4><p>For LLM Router to serve as the universal gateway, it needed to support OpenAI, Google, Anthropic, and even models we self-host via vLLM. The challenge: each provider has different API formats and capabilities. Without abstraction, client code would need changes every time we switched models.</p><p>We standardized on the OpenAI interface. All models — regardless of provider — are accessible through the OpenAI SDK. Whether you’re calling `claude-4.5-sonnet` or `gpt-5.2`, you simply change the model name. LLM Router handles provider-specific translation internally. When new models launch, we add them to LLM Router once. Teams can immediately use them by specifying the new model name — zero code changes required.</p><pre>from openai import OpenAI<br><br>client = OpenAI(base_url=&quot;https://llm_router&quot;) # Point to LLM Router<br>completion = client.chat.completions.create(<br> model=&quot;claude-4.5-sonnet&quot;, # Any supported model<br> messages=[<br> {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}<br> ]<br>)</pre><h3>Prompt Studio</h3><h4>The Experimentation Bottleneck</h4><p>Early in our LLM journey, building AI-powered features always required code. Validating an idea — “Can this prompt solve this problem?” — meant first requesting engineering support. How well an approach works, how to craft the prompt, which model fits best — these vary by use case and require iterative experimentation. When every iteration required an engineer, rapid experimentation wasn’t possible.</p><h4>No-Code AI Feature Development</h4><p><strong>Prompt Studio</strong> is the web-based platform we built to solve this. Anyone can create and test AI features without writing code. Enter a prompt, select a model, click run. If results don’t match expectations, revise and run again — iterating toward the desired behavior.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bU53EeG08BZek7WV.png" /><figcaption>Prompt Studio</figcaption></figure><p>The platform supports all models from OpenAI, Google, and Anthropic, plus internally-hosted models. Switch freely between them to compare outputs and find the best fit. This enabled PMs and other non-engineering roles to build AI features independently, iterating as fast as they needed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vPLZhwk0RRKevzUa.png" /><figcaption>Model Comparison</figcaption></figure><p>Once a prompt shows promise, it needs systematic evaluation. A few successful examples don’t guarantee consistent quality across diverse inputs. Prompt Studio’s <strong>Evaluation</strong> feature lets you upload test sets — hundreds to thousands of examples — generate results in batch, and measure performance quantitatively. This answers: “How well does this prompt perform across varied scenarios?”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4clCH2wz3mbZ75eo.png" /><figcaption>Evaluation</figcaption></figure><p>When a prompt is production-ready, deployment is straightforward. Engineers integrate the Prompt Studio API only once. After that, teams ship prompt improvements directly from the UI — no code changes needed. Multiple versions live within a single project, making it easy to compare iterations and roll back if needed. Selecting which version serves production traffic is a single click.</p><p>This architecture enables anyone to own AI features end-to-end — continuously testing and improving without engineering dependencies. Features like AI Writing Assistance shipped quickly through Prompt Studio and continue to evolve.</p><p>Prompt Studio has become the repository of AI knowledge across Karrot. Teams can reference how others structure their prompts, and best practices naturally accumulate. It’s evolved into the central hub where our collective AI expertise lives.</p><h4>Building Reliable AI APIs</h4><p>One challenge we faced: ensuring high availability when depending on external AI services. Rate limits, latency spikes, and occasional service disruptions can affect any cloud API. When a critical feature depends on a single model, any service interruption impacts that feature directly.</p><p>Handling reliability per-feature is costly and inconsistent. Prompt Studio provides multiple layers of resilience:</p><p><strong>Retry</strong>: Transient failures trigger automatic retries with exponential backoff.</p><p><strong>Region Fallback</strong>: When rate limits or errors hit in one region, requests automatically retry against the same model in a different region. If `gemini-2.5-flash` in `us-central1` fails, we retry in the `global` region.</p><p><strong>Model Fallback</strong>: When all retries fail, requests fall back to a pre-configured alternative model from a different provider. If Google’s `gemini-2.5-flash` is unavailable even across regions, we route to OpenAI’s GPT-5.</p><p><strong>Circuit Breaker</strong>: During sustained outages — when a model continuously fails — the circuit breaker activates. The system blocks requests to that model immediately, routing directly to fallback. It periodically health-checks the blocked model and restores traffic when it recovers. This fail-fast pattern minimizes latency during sustained provider outages.</p><p>These layered safeguards ensure AI-powered services remain operational even when individual providers experience issues.</p><h4>Beyond Text Generation</h4><p>AI capabilities now extend well beyond text. Models like OpenAI’s GPT-Image and Sora, along with Google’s NanoBanana, generate images and video with remarkable quality. Teams use these generation models through the same Prompt Studio workflow: select a model, test prompts, deploy when ready.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*w6znDb7pXmVTsZWE.png" /><figcaption>Image Generation</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*x4F2iVXZbFTGAzLk.png" /><figcaption>Video Generation</figcaption></figure><p>This unified approach enables diverse GenAI applications. We rapidly prototyped and launched campaigns and features like Karrot AI Photo Studio, Dream Home, and AI Try-on through Prompt Studio.</p><h4>Agents and the MCP Ecosystem</h4><p>Interest in AI Agents — systems that use tools and execute multi-step workflows — is growing rapidly. Agents receive a task along with available tools, then autonomously select and invoke tools as needed. The capabilities are bounded only by the tools you provide.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*LcMuz7BXT4P5vOFQ.png" /></figure><p>Building effective Agents requires two things: well-designed tools and an execution loop that orchestrates tool calls and AI reasoning until the task completes. Prompt Studio provides both:</p><p><strong>MCP Hub</strong>: A registry for discovering and sharing internally-developed MCPs (Model Context Protocol tools). Register a useful MCP, and anyone at Karrot can find and use it. The hub hosts tools for querying internal data, fetching documents, and more.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NN2tBBG0nQ4P8oUQ.png" /><figcaption>MCP Hub</figcaption></figure><p><strong>Agent Builder</strong>: Combine tools from the MCP Hub to create Agents without code. Write task instructions, select a model and MCPs, and you’re done. Prompt Studio handles the orchestration loop automatically and captures every Agent execution as a Trace — enabling step-by-step debugging: “Why did the LLM call this tool?” “What was the tool response?”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4nJF1PGn5wbXelSp.png" /><figcaption>Agent builder</figcaption></figure><p>Agents you build in Prompt Studio become immediately available in KarrotChat.</p><h3>KarrotChat</h3><h4>Our Internal Agent Platform</h4><p>Prompt Studio is where you build AI features and Agents — <strong>KarrotChat</strong> is where you use them.</p><p>Through KarrotChat, teams can discover and interact with Agents from Prompt Studio. Users can also select any supported model for direct conversation, and enabling MCPs from the hub requires just a toggle. Useful conversations can be shared across teams.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*YcaSTKSFg5Wgfwq9.png" /><figcaption>Discovering Agents</figcaption></figure><h4>Case Study: DANA — Data Analysis Agent</h4><p>What happens when tools that access internal data meet capable AI and domain expertise? Karrot accumulates rich analytical data in BigQuery. But actually performing analysis was challenging — you needed to know which tables hold what information, what columns mean, which joins to perform, and how to write BigQuery SQL. This knowledge lived with a small group of specialists.</p><p>Prompt Studio let us connect MCPs with AI and capture data analysis expertise in Agent instructions. Now anyone can ask questions in KarrotChat and perform sophisticated analysis.</p><p><strong>DANA</strong> — Data Analyst of NA Product team — exemplifies this transformation. It combines the BigQuery MCP, detailed documentation of the data model, and carefully crafted prompts. Team members now get answers and insights without understanding the underlying schema.</p><p>Ask “Give me daily post counts for the past week,” and DANA finds the right table, runs the query, and returns the answer. For deeper questions like “Which category was the most popular over the past 10 days?” it joins tables, aggregates data, and surfaces insights.</p><p>No SQL knowledge required. No schema expertise needed. Since launch, DANA has become part of the team’s daily workflow — helping everyone make faster, more confident data-driven decisions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iDFMG7xwjVrhhFkn579srw.png" /><figcaption>DANA in KarrotChat</figcaption></figure><p>Internal Agents like DANA are multiplying. Knowledge that once lived with specialists now spreads across the organization through Agents — measurably boosting productivity across teams.</p><h4>Wrapping Up</h4><p>Karrot has embraced AI across operations and services. Throughout 2025, we’ve built numerous AI features and internal Agents, all running on this platform. With multiple services depending on shared infrastructure, we invest heavily in reliability, observability, and security.</p><p>Our GenAI platform evolved in alignment with Karrot’s core values: autonomy, rapid experimentation, and transparent knowledge sharing. Teams adopt AI independently while the platform provides company-wide visibility and governance. Effective prompts and Agents naturally propagate across the organization. This cultural foundation has enabled Karrot to adapt quickly to the GenAI era.</p><p>As adoption grows, so does responsibility. The platform now handles hundreds of millions of requests — stability and security have never been more critical. AI technology advances daily, with new models and capabilities emerging constantly.</p><p>The LLM Infrastructure team work to bring these advances to the platform first — so every product team at Karrot can build on them. We’re hiring engineers to help shape the future of AI infrastructure here. If that sounds interesting, we’d love to talk.</p><ul><li><a href="https://about.daangn.com/jobs/7498021003/">Senior Software Engineer — ML Infra (Seoul, Korea)</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5cf6e813838e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/daangn/karrots-genai-platform-5cf6e813838e">Karrot’s Generative AI Platform</a> was originally published in <a href="https://medium.com/daangn">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[당근의 GenAI 플랫폼]]></title>
            <link>https://medium.com/daangn/%EB%8B%B9%EA%B7%BC%EC%9D%98-genai-%ED%94%8C%EB%9E%AB%ED%8F%BC-ee2ac8953046?source=rss----4505f82a2dbd---4</link>
            <guid isPermaLink="false">https://medium.com/p/ee2ac8953046</guid>
            <category><![CDATA[ai-agent]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[ai-tools]]></category>
            <dc:creator><![CDATA[Tommy Park]]></dc:creator>
            <pubDate>Fri, 19 Dec 2025 10:28:14 GMT</pubDate>
            <atom:updated>2025-12-22T03:06:23.033Z</atom:updated>
            <content:encoded><![CDATA[<p>안녕하세요, 당근 Tech Core의 ML Applications팀과 LLM Infra TF에서 일하고 있는 Tommy예요. 저희 팀은 ‘AI 활용에 가장 앞선 당근’이라는 비전 아래 여러 제품 팀이 AI를 더 잘 활용할 수 있도록 돕고 있어요. 2024년 초에는 <a href="https://medium.com/daangn/%EB%8B%B9%EA%B7%BC%EC%97%90%EC%84%9C-llm-%ED%99%9C%EC%9A%A9%ED%95%98%EA%B8%B0-76131ecebce1">당근에서 LLM 활용하기</a>라는 글로 당근이 LLM 활용을 시작했던 몇 가지 사례와 그 과정에서 무엇이 필요했는지 공유했었는데요.</p><p>그로부터 1년 반이 지난 지금, AI 모델은 당시와 비교할 수 없을 만큼 발전했고 당근에서 GenAI를 활용하는 사례는 수백가지로 다양해졌어요. 이 글에서는 당근이 GenAI를 적극적으로 활용해오면서 마주한 문제들을 짚어두고 이를 해결하기 위해 어떤 플랫폼들을 구축해왔는지 소개하려고 해요. 비슷한 고민을 하고 있는 분들이 시행착오를 줄이고 인사이트를 얻어가셨으면 좋겠어요.</p><h3>당근의 GenAI 플랫폼</h3><p>당근의 많은 서비스들과 데이터 파이프라인 전반에서 GenAI를 활용하고 있어요. 그 뒤에는 GenAI 활용을 돕는 몇 가지 플랫폼들이 자리잡고 있어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZVk-D6ZItzHcbGzMd8m8aQ.png" /><figcaption>당근의 GenAI 플랫폼</figcaption></figure><p>조금 복잡해 보일 수 있지만 LLM Router, Prompt Studio, KarrotChat은 각각 서로 다른 문제에 집중하며 발전해왔어요. 이 플랫폼들이 어떤 문제를 해결하려고 했고, 어떤 방식으로 구축됐는지 하나씩 살펴볼게요.</p><h3>LLM Router</h3><h4>LLM 도입 초기 계정 관리의 문제</h4><p>LLM을 처음 도입하던 시기에는 AI API 호출이 필요할 때마다 각 팀에서 직접 계정을 만들고 API 키를 발급했어요. OpenAI, Anthropic, Google 등 여러 제공사의 모델을 쓰다 보니 계정과 키가 자연스럽게 늘어났고, 그만큼 관리해야 할 것들도 많아졌죠. 이 과정에서 여러 문제가 드러나기 시작했어요.</p><ul><li>AI를 사용하려는 조직이 많아지면서 계정과 API키 발급 자체가 병목이 되었어요. 발급한 API 키가 많아지면서 어느 팀이 어떤 서비스를 위해 계정과 키를 사용하는지 관리하기 어려워졌고 보안에 대한 우려도 커졌어요.</li><li>AI API는 보통 계정별로 사용량 제한(Rate Limit)이 걸려있어요. 여러 계정을 각자 사용하다 보니 어떤 계정은 여유가 남아있는데, 다른 계정은 이미 한도에 도달해 호출이 실패하는 상황이 발생했어요.</li><li>전체 비용과 사용량을 파악하기도 쉽지 않았어요. 계정과 API 키가 많아지면서 호출수와 비용을 확인하려면 여러 곳을 조회해야 했고, 당근 전체의 AI API 비용을 집계하는 데도 시간이 많이 들었어요.</li></ul><h4>AI Gateway로 통합하기</h4><p>이런 문제를 해결하기 위해 <strong>LLM Router</strong>를 도입했어요. 핵심 아이디어는 모든 AI API 호출을 하나의 관문으로 통과하게끔 모으는 거예요. API 키와 계정은 LLM Router에서만 통합 관리하고, 각 서비스들은 별도의 키나 계정 없이 서비스ID 만으로 LLM Router를 통해 AI API를 사용할 수 있도록 했어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oyB-F4TNd_25s5XEUMmJXA.png" /></figure><p>LLM Router를 도입한 이후로 여러가지가 달라졌어요.</p><ul><li>계정과 API 키 발급 프로세스가 사라졌어요. 기존에 발급했던 키도 일부 예외적인 경우를 제외하고 대부분 회수할 수 있었어요. 사용하는 팀 역시 번거로운 절차 없이 원하는 API를 바로 사용할 수 있게 되었어요.</li><li>모든 호출을 한 곳으로 모으면서 계정 관리를 인프라 담당 조직에서 통합해서 맡을 수 있게 됐어요. 사용량 제한을 모니터링하고 필요한 경우 한도를 조정하는 일도 인프라 조직이 직접 담당하게 되면서, 각 서비스 팀은 운영 부담에서 벗어나 기능과 서비스에 더 집중할 수 있게 되었어요.</li><li>사용량과 비용을 파악하는 것도 훨씬 쉬워졌어요. LLM Router로 들어오는 모든 호출은 어느 서비스에서 요청했는지 추적할 수 있고, 이 정보는 당근의 인프라 비용 관리 플랫폼과 연동돼, 이제 어떤 서비스가 어떤 모델을 얼마나 쓰고 있는지 한눈에 파악할 수 있게 되었어요.</li></ul><h4>하나의 인터페이스로 모든 모델 지원</h4><p>LLM Router가 모든 AI API 호출의 관문 역할을 하려면 OpenAI, Google, Anthropic은 물론, vLLM으로 직접 서빙하는 모델까지 모두 지원해야 했어요. 문제는 각 제공사마다 API 형태와 지원하는 기능이 모두 다르다는 점이었어요. 그대로 두면 사용하는 모델에 따라 클라이언트 코드도 함께 바뀔 수밖에 없었어요. 이 문제를 해결하기 위해 LLM Router로 호출하는 클라이언트는 모델을 바꿀 때 마다 코드를 변경할 필요가 없도록 설계했어요. 제공사에 관계없이 모두 동일한 인터페이스를 제공하기 위해서 OpenAI 인터페이스를 표준으로 정하고 OpenAI SDK를 통해서 모든 모델의 기능을 사용할 수 있도록 했어요. 예를 들어 ‘claude-4.5-sonnet’을 쓰든, ‘gpt-5.2’을 쓰든, 같은 코드에서 모델 이름만 바꿔요청하면 LLM Router가 내부적으로 해당 모델에 맞는 방식으로 호출을 처리해요.</p><p>이제 새로운 모델이 출시되더라도 LLM Router에만 빠르게 추가하면 돼요. 각 팀은 별도의 구현 없이 새 모델 이름을 지정하는 것만으로 바로 사용할 수 있게 되었어요.</p><pre>from openai import OpenAI<br>client = OpenAI(base_url=&quot;https://llm_router&quot;) # base_url 변경<br><br>completion = client.chat.completions.create(<br>  model=&quot;claude-4.5-sonnet&quot;,  # 모든 모델 지원<br>  messages=[<br>    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}<br>  ]<br>)</pre><h3>Prompt Studio</h3><h4>AI 기능을 만들 때의 문제</h4><p>LLM 도입 초기에는 AI로 새로운 기능을 만들어보려면 매번 코드가 필요했어요. ‘이 프롬프트로 이 문제를 해결할 수 있을지 해볼까?’ 라는 아이디어를 검증하려면 먼저 엔지니어에게 코드를 요청을 해야 했어요. 아이디어가 실제로 얼마나 잘 동작하는지, 프롬프트를 어떻게 써야 하는지, 어떤 모델이 적합하고 효율적인지는 문제마다 달라요. 그래서 실제로 호출해보고 확인하는 과정을 여러 번 반복해야 했어요. 그때마다 엔지니어에게 요청하거나 코드를 수정해야 하는 구조에서는, 빠르게 실험하고 검증하기가 쉽지 않았어요.</p><h4>누구나 코드 없이 AI 기능을 만드는 곳</h4><p><strong>Prompt Studio</strong>는 이런 문제를 해결하기 위해 만든 웹 기반 AI 플랫폼이에요. 코드를 몰라도 누구나 AI 기능을 만들고 테스트할 수 있어요. 프롬프트를 입력하고 모델을 고른 후 실행 버튼만 누르면 돼요. 결과가 기대와 다르면 프롬프트를 바로 수정해 다시 실행하면서 점점 개선해나갈 수 있어요. OpenAI, Google, Anthropic 은 물론, 사내에서 서빙하는 다양한 모델까지 모두 지원해요. 원하는 모델을 자유롭게 바꿔가며 결과를 비교할 수 있고, 문제에 가장 잘 맞는 선택지를 찾을 수 있어요. 덕분에 PM 등 비개발자 직군에서도 엔지니어에 의존하지 않고도 많은 AI 기능을 개발하고, 원하는 만큼 빠르게 이터레이션을 돌릴 수 있게 되었어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2GBQd53gn_zNLnz0F7s5lw.png" /><figcaption>Prompt Studio</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6DZmBgfpyzm-2QW-ey1MfQ.png" /><figcaption>Prompt Studio에서 지원하는 다양한 모델 비교</figcaption></figure><p>프롬프트가 어느 정도 완성되면 이제는 체계적으로 평가해볼 필요가 있어요. 몇 가지 예시에서는 잘 동작하는 것처럼 보여도, 실제로는 다양한 입력에 대해서 일관된 품질이 나오지 않는 경우도 있어요. Prompt Studio는 이런 상황을 위해 수백에서 수천 개의 테스트셋을 업로드해, 한 번에 결과를 생성하고 평가하는 Evaluation 기능을 제공해요. 이를 통해 ‘이 프롬프트가 다양한 상황에서 얼마나 잘 동작하는가’를 정량적으로 측정할 수 있어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rbYmybEdjAPqiu9AumQbuA.png" /><figcaption>Prompt Studio의 Evaluation 기능</figcaption></figure><p>프롬프트가 충분히 잘 동작한다면 바로 적용해볼 수도 있어요. 엔지니어는 해당 기능을 위해 Prompt Studio API 연동을 한번만 해두면 돼요. 이후에는 코드 변경 없이, Prompt Studio에서 직접 프롬프트를 개선하고 바로 적용할 수 있어요. 하나의 프로젝트 안에서 여러 버전을 함께 관리할 수 있어서, 프롬프트나 모델을 바꿀 때 각 변경 사항을 서로 비교하며 쉽게 오갈 수 있어요. Prompt Studio API 호출 시 어떤 배포 버전을 사용할지 UI에서 바로 지정할 수 있어서, 실제 반영도 클릭 한 번이면 가능해요.</p><p>이 구조 덕분에 엔지니어의 도움 없이도 누구나 오너십을 가지고 AI 기능을 지속적으로 테스트하고 개선할 수 있게 됐어요. AI 글쓰기와 같은 기능도 Prompt Studio를 통해 빠르게 적용할 수 있었고 지속적으로 개선해 오고 있어요.</p><p>Prompt Studio에는 회사 전반에서 사용되는 수많은 프롬프트가 모여 있어요. 다른 팀이 어떤 방식으로 프롬프트를 구성했는지 참고할 수도 있고, 자연스럽게 조직 전체의 AI 활용 노하우가 쌓여가죠. Prompt Studio는 당근 안에서 AI 활용 경험치가 축적되는 허브 역할을 하고 있어요.</p><h4>안정적인 AI API를 위해</h4><p>AI API를 서비스에 적용하면서 겪는 어려움 중 하나는 API가 생각보다 불안정하다는 점이에요. 사용량 제한에 걸려 오류가 발생하는 일은 아주 흔하고, 모델 제공사 측 장애로 응답이 느려지거나 특정 모델이 일정 시간 동안 아예 작동하지 않는 경우도 있어요. 예를 들어 사용자 기능에 GPT-5 모델을 사용하고 있는데, OpenAI에 장애가 발생해 모든 요청이 실패한다면 해당 기능도 같이 멈추게 돼요. 이런 안정성 문제를 기능마다 개별적으로 대응하려면 번거롭고, 구현과 운영에 드는 비용도 커져요. <strong>Prompt Studio는 이런</strong> 문제를 해결하기 위해 여러 단계의 안전 장치를 제공하고 있어요.</p><ul><li><strong>Retry</strong>: 우선 모델이 일시적인 실패를 하는 경우 재시도 가능한 오류라면 자동으로 재시도 해요.</li><li><strong>Region Fallback</strong>: 특정 리전에서 사용량 제한이나 오류가 발생하면, 다른 리전의 같은 모델로 자동으로 재시도 해요. 예를 들어 us-central1의 gemini-2.5-flash에서 사용량 제한 오류가 발생하면, global 리전의 gemini-2.5-flash 모델로 같은 요청을 다시 시도하는 방식이에요.</li><li><strong>Model Fallback</strong>: 모든 재시도도 실패하고 장애가 나는 경우에는 다른 제공사의 모델로 폴백하는 기능도 제공해요. 만약 Google의 gemini-2.5-flash가 Region Fallback에서도 오류를 내면, OpenAI의 GPT-5 같은 미리 지정해둔 다른 모델로 대신 호출해주는 방식이에요. 이를 통해서 특정 모델에 장애가 발생하더라도 서비스 영향을 최소화 할 수 있어요.</li><li><strong>Model Circuit breaker</strong>: 특정 모델이 일정 시간동안 지속적으로 오류를 발생시키는 전면 장애 상황에서는 Circuit breaker가 발동돼요. 이 경우 해당 모델로의 요청 자체를 차단하고 바로 Model Fallback으로 처리해요. 차단된 동안에는 모델의 상태를 자동으로 점검하고, 정상으로 돌아오면 다시 복구돼요. 특정 모델이 긴 시간동안 전면 장애가 발생하는 경우 Retry에 들어가는 지연 시간을 최소화 하기 위한 장치예요.</li></ul><p>이러한 다양한 단계의 안전장치를 통해 AI API를 사용하는 서비스들이 최대한 안정적으로 동작하도록 대비하고 있어요.</p><h4>이미지와 비디오 생성까지 모든 종류의 GenAI 지원</h4><p>최근에는 OpenAI의 GPT-Image와 Sora, Google의 NanoBanana처럼 이미지와 비디오를 생성하는 AI도 빠르게 발전하고 있어요. 당근에서는 Prompt Studio를 통해 텍스트 생성뿐 아니라 이미지와 비디오 생성 모델도 바로 사용할 수 있어요. 텍스트 생성 AI와 동일하게 원하는 모델을 선택해 프롬프트를 테스트하고, 검증이 끝나면 바로 배포할 수 있어요.</p><p>이런 구조 덕분에 텍스트 생성 뿐 아니라 이미지와 비디오 생성까지 다양한 Gen AI 활용이 가능해졌어요. 당근 AI 사진관, 꿈의 집, AI Try-on 같은 다양한 이미지 생성 캠페인과 기능들도 Prompt Studio를 기반으로 빠르게 실험하고 운영할 수 있었어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aPaJzJg72cGC8Wc6NTQDyw.png" /><figcaption>Prompt Studio의 이미지 생성 기능</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*26bCeYXfpQiXRn6N2qwgiA.png" /><figcaption>Prompt Studio의 비디오 생성 기능</figcaption></figure><h4>당근의 Agent와 MCP 생태계</h4><p>AI가 도구를 사용하며 여러 단계에 걸친 작업을 수행할 수 있는 <strong>Agent</strong>에 대한 관심과 수요가 빠르게 늘고 있어요. Agent는 AI에게 작업을 요청할 때 사용할 수 있는 도구 목록을 함께 전달하고, AI가 상황에 따라 적절한 도구를 선택해 호출하도록 하는 방식이에요. 어떤 도구를 주느냐에 따라서 Agent가 할 수 있는 일은 무궁무진하죠. 유용한 Agent를 만들기 위해서는 먼저 잘 설계된 도구들이 충분히 준비되어 있어야 하고, 문제가 해결될 때까지 도구 호출과 AI 호출을 반복하는 Agent 루프도 필요해요. 이렇게 도구와 실행 흐름이 함께 갖춰질 때, Agent는 단순한 응답을 넘어 실제 작업을 수행할 수 있게 돼요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JHzMUqyjyPYlhMjQ12vLBw.png" /></figure><p><strong>Prompt Studio</strong>는 Agent를 쉽게 구성할 수 있도록 다음 기능들을 제공하고 있어요.</p><ul><li>MCP 허브: 사내에서 개발된 다양한 <strong>MCP</strong>들을 등록하고 탐색할 수 있는 MCP 허브를 제공해요. 유용한 MCP를 발견하거나 새로 만들었다면 이곳에 등록하면 돼요. 그러면 당근 구성원 누구나 쉽게 찾아보고 사용할 수 있어요. 사내 데이터를 조회하는 도구나 문서를 읽어오는 도구 등 다양한 MCP들이 이 허브를 통해 공유되고 있어요.</li><li>Agent 만들기: MCP 허브에 등록된 도구들을 조합해 누구나 Agent를 만들 수 있어요. 수행할 작업에 대한 지시사항을 작성하고 사용할 모델과 MCP를 선택하면 바로 완성이에요. 도구 호출과 AI 호출을 반복하는 Agent 루프는 Prompt Studio에서 자동으로 처리해요. Agent가 실행되는 전체 과정은 Trace로 기록되어서, 문제가 생겼을 때 “LLM이 왜 이 도구를 호출했지?”, “도구 실행 결과가 뭐였지?”를 단계별로 확인하며 디버깅할 수 있어요.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*75zNCxtHu4FUuB48U-ue5A.png" /><figcaption>Prompt Studio의 MCP 허브</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bOp-PUwfqMOSGsPGRR-91Q.png" /><figcaption>Prompt Studio의 Agent 생성 기능</figcaption></figure><p>이렇게 만든 Agent는 <strong>KarrotChat</strong>에서 누구나 바로 사용할 수 있어요.</p><h3>KarrotChat</h3><h4>당근 사내 AI/Agent 채팅 플랫폼</h4><p>Prompt Studio가 AI 기능과 Agent를 만드는 곳이라면, <strong>KarrotChat</strong>은 만들어진 Agent를 직접 사용하는 채팅 플랫폼이에요. 사내 Agent용 ChatGPT라고 보면 이해하기 쉬워요. KarrotChat에서는 Prompt Studio에서 구성해둔 Agent들을 탐색하고 바로 사용할 수 있어요. Agent뿐만 아니라 원하는 모델을 직접 선택해서 대화할 수 있고, MCP 허브에 등록된 MCP도 간단히 켜서 사용할 수 있어요. 유용한 대화는 동료들과 서로 공유할 수도 있어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ocGWq7phABHwIjcFj5JiNg.png" /><figcaption>KarrotChat에서 Agent 탐색하기</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vZUpHYcS2RXRB470uq3zGQ.png" /><figcaption>KarrotChat에서 Agent와 대화하기</figcaption></figure><h4>당근알바팀의 데이터 분석 Agent 사례</h4><p>사내 데이터에 접근할 수 있는 도구, 똑똑한 AI, 그리고 동료들의 노하우가 만나면 어떤 일이 가능할까요? 당근은 다양한 분석을 할 수 있는 데이터를 빅쿼리에 많이 쌓고 있어요. 하지만 실제로 이를 활용해서 분석하는 일은 쉽지 않았어요. 어떤 테이블에 어떤 정보가 있는지, 각 칼럼들은 어떤 의미인지, 원하는 정보를 보기 위해서 어떤 테이블을 조인해야 하는지, 그리고 무엇보다 빅쿼리 SQL을 작성할 수 있는지까지. 이런 지식들이 있어야만 데이터 분석이 가능했어요.</p><p>Prompt Studio에서 MCP와 AI를 연결하고, 데이터 분석 노하우를 지시사항으로 담은 Agent를 구성할 수 있게 되면서 상황이 달라졌어요. 이제는 누구나 KarrotChat에서 원하는 정보를 물어보고 깊이있는 데이터 분석을 할 수 있게 되었어요.</p><p>예를 들어, 당근알바팀의 ‘데이터캣’은 이런 변화를 잘 보여주는 사례예요. 데이터캣은 빅쿼리 MCP와 당근알바 빅쿼리 데이터 구조에 대한 상세한 가이드, 그리고 프롬프트 엔지니어링을 결합해서 만든 Agent예요. 당근알바 팀원들은 데이터 구조에 대한 배경 지식 없이도 데이터캣을 통해서 다양한 정보를 얻고 분석할 수 있게 되었어요. 뿐만 아니라 데이터캣은 팀원들이 최근에 사용한 쿼리들을 기억하고, 도메인 지식을 학습하면서 사용할수록 점점 개인에게 맞게 똑똑해지는 기능도 가지고 있어요.</p><p>예를 들어 “어제 게시된 당근알바 공고는 몇 개야?” 와 같은 간단한 질문에 바로 답할 수 있어요. 데이터캣이 알아서 적절한 테이블을 찾고, 쿼리를 만들고, 실행해서 결과를 알려줘요. “최근 열흘 동안 특별히 인기 있었던 공고들의 제목은 뭐야?” 같은 분석 요청도 가능해요. 여러 테이블을 조인하고, 집계하고, 결과를 분석해 패턴을 찾아 인사이트를 제공해요. SQL을 몰라도, 테이블 구조를 몰라도 누구나 데이터 기반의 의사결정을 할 수 있는 환경이 만들어진 거예요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*92-iA64M-18LIu5LjE03Fg.png" /><figcaption>KarrotChat에서 데이터캣과 대화</figcaption></figure><p>데이터캣처럼 당근 동료들이 매일 사용하는 사내 Agent들도 점점 늘어나고 있어요. 이전에는 소수의 사람만 가지고 있던 지식이나 노하우가 Agent를 통해 회사 전체로 전파되고, 결과적으로 모두의 생산성도 눈에 띄게 향상되고 있어요.</p><h3>마치며</h3><p>당근은 AI를 업무와 서비스 전반에서 적극적으로 활용해오고 있어요. 2025년 한 해 동안에도 많은 AI 기능과 사내Agent들이 만들어졌고, 지금도 이 플랫폼 위에서 운영되고 있어요. 여러 서비스가 공통 플랫폼에 의존하는 만큼 안정성과 가시성과 보안을 확보하는 데에도 많은 노력을 기울이고 있답니다.</p><p>당근 GenAI 플랫폼은 자율, 빠른 실험, 투명한 공유라는 당근의 핵심 문화와도 잘 맞는 방향으로도 발전해왔어요. 각 팀은 자율적으로 AI를 활용하되, 플랫폼을 통해 전사적인 가시성과 거버넌스를 함께 확보해요. 빠르게 실험하고 배포할 수 있는 환경이 갖춰져 있고 좋은 프롬프트와 Agent는 자연스럽게 공유되어요. 이런 문화적 기반 위에서 당근은 GenAI 활용에 빠르게 적응하는 회사로 성장할 수 있었어요.</p><p>플랫폼 위에 올라가는 기능이 많아질수록 책임도 함께 커지고 있어요. 이제는 수억번의 호출이 처리되는 인프라가 되었고, 그만큼 안정성과 보안의 중요성도 더 커졌어요. AI 기술은 빠르게 발전하고 있고, 그에 맞춘 새로운 모델과 기능도 매일같이 등장하고 있으니까요.</p><p>ML 인프라팀과 ML Applications팀은 당근의 모든 제품 팀이 이런 변화를 빠르게 활용하고 지렛대로 삼을 수 있도록 플랫폼 차원에서 가장 먼저 제공하려고 해요. 지금은 당근의 AI 플랫폼을 함께 만들어갈 동료도 찾고 있어요. 관심 있다면 아래 링크를 참고해 주세요.</p><ul><li><a href="https://about.daangn.com/jobs/7498021003/">Senior Software Engineer, Machine Learning — ML 인프라</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ee2ac8953046" width="1" height="1" alt=""><hr><p><a href="https://medium.com/daangn/%EB%8B%B9%EA%B7%BC%EC%9D%98-genai-%ED%94%8C%EB%9E%AB%ED%8F%BC-ee2ac8953046">당근의 GenAI 플랫폼</a> was originally published in <a href="https://medium.com/daangn">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Running Elasticsearch on Kubernetes the Easy Way, Part 2 — Data Node Warm-Up]]></title>
            <link>https://medium.com/daangn/running-elasticsearch-on-kubernetes-the-easy-way-part-2-data-node-warm-up-0d81d433c5c1?source=rss----4505f82a2dbd---4</link>
            <guid isPermaLink="false">https://medium.com/p/0d81d433c5c1</guid>
            <category><![CDATA[elasticsearch]]></category>
            <category><![CDATA[k8s]]></category>
            <category><![CDATA[english]]></category>
            <category><![CDATA[eck]]></category>
            <dc:creator><![CDATA[Dongsun Shin]]></dc:creator>
            <pubDate>Tue, 16 Dec 2025 07:03:54 GMT</pubDate>
            <atom:updated>2025-12-16T07:03:44.592Z</atom:updated>
            <content:encoded><![CDATA[<h3>Running Elasticsearch on Kubernetes the Easy Way, Part 2 — Data Node Warm-Up</h3><p>Hi, we’re Ellie and Jarry from the Search Platform team at Karrot. Our team is responsible for handling the massive search traffic generated across Karrot’s various services-quickly and reliably-while building a platform that enables new search experiences. In <a href="https://medium.com/daangn/%EB%8B%B9%EA%B7%BC%EB%A7%88%EC%BC%93-%EA%B2%80%EC%83%89-%EC%97%94%EC%A7%84-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EB%A1%9C-%EC%89%BD%EA%B2%8C-%EC%9A%B4%EC%98%81%ED%95%98%EA%B8%B0-bdf2688df267">Part 1</a>, we shared our journey of migrating our search infrastructure to Kubernetes (ECK), achieving deployment automation and reducing deployment time. What once took 5 hours was cut down to 90 minutes, and anyone could now safely deploy to our search clusters.</p><p>At the end of Part 1, we mentioned the following:</p><blockquote><em>“When deploying during peak traffic times, we observed spikes in CPU usage and latency. (…) Since there are short-term solutions like simply avoiding peak hours for deployments, we decided to proceed while accepting this as a non-issue.”</em></blockquote><p>Back then, we thought, “Just avoid peak hours-no big deal.” But over time, we realized that <strong>the constraint of having to avoid peak hours was itself the problem</strong>. Operators still had to constantly watch the clock, and once a deployment started, they still had to nervously monitor dashboards. In this article, we’ll explain why we came to see this as a problem and how we solved it.</p><h3>1. Challenges Remaining After ECK Adoption</h3><h4>1.1. How Much Has Our Search Infrastructure Grown in Two Years?</h4><p>When we wrote Part 1, we had a single search cluster handling about 1,000 queries per second (1K QPS) at peak.</p><p>Two years later, Karrot has grown and so has our search infrastructure. We now operate four clusters, each serving different purposes, with peak traffic like this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UiBiUdOOWlKvvfq5efy7Gw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PXh-9Jo2X31bDgSZQexLvw.png" /></figure><p>As we explained in Part 1, our ECK-based search clusters operate using the following architecture: we define Elasticsearch Custom Resources in a Kubernetes environment and run them as StatefulSets.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/884/1*DIEwUBuI9oJCq7zgJRjm5g.png" /><figcaption>ECK-based Search Cluster Architecture</figcaption></figure><p>It wasn’t just traffic that increased. With four clusters now in operation, the operational burden grew as well. As covered in Part 1, deploying a single cluster took roughly 90 minutes. With four clusters, total deployment time stretched beyond 6 hours, and someone had to monitor the entire process.</p><h4>1.2. The Limits of “Just Deploy Outside Peak Hours”</h4><p>In Part 1, we said we achieved deployment automation with ECK-but there was an important caveat: <strong>“as long as we avoid peak hours.”</strong></p><p>At first, this didn’t seem like a big deal. We could just deploy in the early morning or late at night. But as we scaled to four clusters and deployments became more frequent, the practical issues became clear.</p><p>First, <strong>the available deployment windows kept shrinking</strong>. After avoiding peak hours, lunch breaks, and the times right before and after work, only a few hours each day were left for deployments. Situations arose where urgent patches were blocked because “it’s peak time right now.”</p><p>Second, <strong>deployments started feeling like “big events.”</strong> Before clicking the ArgoCD sync button, we had to check: “Is traffic okay right now? Is it safe to deploy at this hour?” Even during deployment, we had to keep watching the latency graphs. We wondered: if we’ve automated deployments, why does it still feel so stressful?</p><p>Ultimately, the real problem we needed to solve was this:</p><blockquote><em>“Let’s create a state where we can deploy with peace of mind, whether it’s peak time or not.”</em></blockquote><p><strong>So why were we so afraid of peak hours?</strong></p><p>It wasn’t just because traffic was high. The real issue was the <strong>latency spikes that occurred during rolling restarts</strong>. Let’s dig into the root causes.</p><h3>2. The Real Reason Rolling Restarts Were Scary</h3><p>A Kubernetes StatefulSet rolling restart updates Pods one at a time, in order. As soon as a Pod becomes Ready, it moves on to the next.</p><p>The problem is that for systems like Elasticsearch that <strong>rely heavily on caches</strong>, this approach isn’t safe. A Pod being Ready doesn’t mean it’s actually ready to handle traffic — frequently accessed data must be loaded into memory (page cache, query cache, etc.) before the node can respond quickly.</p><p>At Karrot, we actually experienced a major outage due to this issue during an ECK rolling restart. That incident became the starting point for building our warm-up system.</p><h4>2.1. A Real Incident Story</h4><p>One day, an Elastic Operator version issue triggered a rolling restart across all Elasticsearch StatefulSets. For a typical Kubernetes service, Pods would go down and come back up one by one without much trouble.</p><p>But Elasticsearch was different. As the rolling restart progressed, the next Pod restarted as soon as the previous one became Ready, but from Elasticsearch’s perspective, it wasn’t actually ready. The restarted nodes had completely empty caches and had to serve traffic in a severely degraded state. During this process, some shards had both their primary and replica affected in succession, degrading availability, and ultimately causing a major outage across the search service.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SpAUOhMUMu22a0ybBalrWA.png" /><figcaption>[Rolling Restart Incident] Search API error rate at 60%, latency spiking to 3s</figcaption></figure><p>This experience taught us that the expectation ‘Kubernetes will handle rolling restarts safely’ does not apply to Elasticsearch.</p><h4>2.2. What Happens When a Data Node Restarts</h4><p>When a data node restarts, several things happen simultaneously inside Elasticsearch.</p><p>First, the restarted data node joins the Elasticsearch cluster with a completely empty file system cache. In this state, queries must read data from disk, significantly increasing search latency.</p><p>Second, while the data node is down, the load on that shard is concentrated on other replica shards.</p><p>The moment these two issues overlap is when the cluster is most unstable.</p><p><strong>The Problem with Cache — Cold New Nodes</strong></p><p>Elasticsearch uses multiple levels of caching — page cache, query cache, field data cache, request cache, and more. But when a data node starts fresh, all these caches are empty. Every cache miss requires reading segments from disk, and as this delay accumulates, requests start piling up in the search thread pool queue. The result: p50 and p90 latency can spike to several times their normal values.</p><p><strong>The Problem of Load Concentration on Remaining Nodes</strong></p><p>When a data node goes down, the replica shards it held become Unassigned. The ES cluster transitions to a yellow state, meaning “all primary shards are alive, but some replicas are unassigned.” The service doesn’t stop immediately, but two problems emerge:</p><ul><li>The remaining replica shards must handle all requests, increasing their load.</li><li>Shards without replicas become a single point of failure. If the node holding that primary also dies, the cluster goes red — unable to process search or indexing requests.</li></ul><p>In summary, request volume stays the same while the number of nodes available to handle it decreases, concentrating load on the remaining nodes. Then, when a cache-cold node rejoins, latency spikes for requests routed to it.</p><p>When both of these problems occur together, rolling restarts in Elasticsearch become particularly scary.</p><h3>3. Defining Goals and Solution Strategy</h3><p>After analyzing the problem, we defined our target state:</p><blockquote><em>“Create a state where clicking the Elasticsearch deploy button at any time won’t bring down the search service.”</em></blockquote><p>Specifically, we wanted three things:</p><ul><li>No need for humans to monitor and adjust timing during deployment</li><li>Search p99 latency stays under 1s even during rolling restarts</li><li>The system handles unexpected node restarts automatically</li></ul><h4>3.1. Why Kubernetes Native Mechanisms Weren’t Enough</h4><p>You might wonder: “Can’t we just tweak readinessProbe or add a postStart hook for warm-up?” But due to how Elasticsearch works, these approaches don’t help.</p><p>The key is <strong>when Elasticsearch data node joins the cluster</strong>. As soon as a data node Pod enters Running state, it joins the Elasticsearch cluster. This happens completely independently of Kubernetes readinessProbe or postStart hooks. The moment the Elasticsearch process starts, it tells the master node “I’m ready,” and the master immediately begins assigning shards to this node.</p><p>Even before the readinessProbe passes, the node is already part of the Elasticsearch cluster. Being removed from the Kubernetes Service’s Endpoints doesn’t matter — Elasticsearch uses its own node discovery and routing, sending requests directly to the node within the cluster, bypassing the Kubernetes Service. postStart hooks are similar — the ES process can join the cluster before the hook completes; it doesn’t wait.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4CBPaSD7jH0QaQsb0sVe_w.png" /></figure><p>Fundamentally, neither Kubernetes nor Elasticsearch settings alone can automate the state of “don’t receive traffic until warm-up is complete.” So our solution strategy boils down to one line:</p><blockquote><em>“Let nodes restart freely, but don’t let them receive search traffic until warm-up is complete.”</em></blockquote><p>The implementation of this idea is our warm-up system, with <strong>search-coordinator</strong> as its core component. We’ll explain how it works in detail in the next section.</p><h3>4. search-coordinator Proxy Architecture</h3><p>Now let’s look at how we made it possible to click the Elasticsearch deploy button without worry.</p><p>The core idea is simple: we placed a proxy called <strong>search-coordinator</strong> between our servers and Elasticsearch, creating a structure where only warmed-up data nodes participate in search. Thanks to this architecture, even when data nodes restart, search traffic always flows only to nodes that have completed warm-up.</p><h4>4.1. Architecture Overview</h4><p>Previously, services connected directly to Elasticsearch, giving us no control over traffic. Now, all requests go through search-coordinator, giving us traffic control. We operate one ECK cluster and one search-coordinator per namespace in a 1:1 structure. We added the search-coordinator module to the existing ECK-only architecture as shown below:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Sv6HKVdPsbWtQN5JcM6WwQ.png" /><figcaption>[Before] ECK-based Search Cluster</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Zl82b_mDxbarGSMNHnc8vw.png" /><figcaption>[After] ECK-based Search Cluster with Search Coordinator</figcaption></figure><p>search-coordinator serves two roles.</p><p>First, it’s an <strong>HTTP proxy</strong>. It receives search/indexing requests and forwards them to ES. Since it communicates via internal DNS within the same Kubernetes cluster, additional latency is negligible.</p><p>Second, it’s a <strong>warm-up orchestrator</strong>. It determines which nodes can participate in search and manages that state. Nodes receiving termination signals are immediately removed from search targets, and newly started nodes are only added to search targets after warm-up is complete.</p><h4>4.2. Controlling Search Target Nodes with prefer_nodes</h4><p>The list of data nodes eligible for search is managed through Central Dogma.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/dde6600cbb634293fe6606d58f44bf08/href">https://medium.com/media/dde6600cbb634293fe6606d58f44bf08/href</a></iframe><p>prefer_nodes is the list of data nodes currently eligible for search. Newly started nodes are not included until they pass warm-up. The warmup section defines criteria for warm-up evaluation - QPS, latency thresholds, minimum/maximum duration - which can be configured differently per cluster.</p><p>When search-coordinator receives a search request, it only sends it to Elasticsearch data nodes registered in Central Dogma. This enabled us to build a structure where search traffic doesn’t flow to nodes that are still warming up.</p><h4>4.3. search-coordinator as the Single State Manager</h4><p>There’s a clear reason we limited Central Dogma reads and writes to search-coordinator alone.</p><p>First, we wanted to maintain a <strong>Single Source of Truth</strong> for node routing state. We wanted to keep “which nodes are search-serving targets” in one place. If multiple servers each queried Elasticsearch to determine “which nodes are currently usable,” states would diverge based on polling timing, and debugging points would multiply exponentially.</p><p>Second, we wanted to <strong>fully encapsulate routing policy in one component</strong>. If decisions like including/excluding data nodes from search targets, warm-up success/failure, and exception handling all happen within search-coordinator, other modules don’t need to know ES’s internal state. Conversely, if applications or batch scripts directly modify the node list, tracking “who changed what, when, and why” becomes difficult.</p><h3>5. search-coordinator Warm-Up Orchestration Architecture</h3><p>Let’s look in more detail at the process from when a node goes down to when it rejoins. Elasticsearch data nodes run as Pods in a Kubernetes StatefulSet. search-coordinator operates in sync with the lifecycle of these data node Pods.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LfN4WQHC5Nqjog1yW-yxwg.png" /><figcaption>Data Node Lifecycle (<strong>prestop</strong> → <strong>exclude</strong> → <strong>warmup</strong> → <strong>warmup pass</strong> → <strong>include</strong> → <strong>serve traffic</strong>)</figcaption></figure><h4>5.1. When a Node Goes Down: prestop &amp; exclude Phases</h4><p>Phases (1) prestop and (2) exclude in the diagram above are the process of safely removing a data node from serving.</p><p>When a data node goes down, search-coordinator performs two main tasks. Just before the ES data node Pod terminates, a preStop script runs and sends information about the soon-to-terminate node to search-coordinator. Then search-coordinator first removes (excludes) that node from prefer_nodes in Central Dogma to stop search traffic from flowing to it. Next, it adds the same node to a warm-up queue based on a Redis Sorted Set. We use timestamps as scores so we can later dequeue the oldest nodes first.</p><p><strong>Fallback for preStop Hook Failures</strong></p><p>preStop hooks can fail or not be called at all. To handle this, we have a search-coordinator-controller module that subscribes to Elasticsearch data node Pod events via a Kubernetes Informer. If preStop wasn’t called but the Pod enters Terminating state, the Informer detects this and the controller performs the same tasks. This provides a safety net that doesn’t rely solely on preStop hooks but also validates against the Pod’s actual lifecycle.</p><h4>5.2. Real Traffic-Based Warm-Up: warmup Phase</h4><p>Phase (3) warmup in the diagram is the process of populating the cache for a cache-cold node.</p><p>When a node comes back up, warm-up begins in earnest. However, if multiple search-coordinator Pods warm up the same node simultaneously, it can cause problems, so we first need to determine “who will warm up this node.”</p><p><strong>Using Redis Distributed Locks to Assign Warm-Up Responsibility</strong></p><p>We designed warm-up so that exactly one search-coordinator Pod handles it. If multiple Pods simultaneously attempt warm-up on the same data node, duplicate queries hit the same shards, creating unnecessary load and in some cases, ES might return 429 Too Many Requests errors. Conversely, a single Pod can generate sufficient QPS for warm-up.</p><p>So we use a Redis distributed lock to decide “who is responsible for this node’s warm-up.” All warm-up workers poll the Redis warm-up queue, and when they detect the same node, only the Pod that acquires the lock first proceeds with warm-up. Other Pods skip immediately if they’re not the lock owner, guaranteeing single search-coordinator warm-up per node. Lock tokens are formatted as podIP/podName-timestamp-randomHex for easy tracking of which Pod holds the lock.</p><p><strong>Collecting Warm-Up Queries</strong></p><p>We decided to use production traffic patterns directly for warm-up queries. We stream all search request logs to a Kafka topic, and the search-coordinator worker responsible for warm-up consumes this topic in real-time to build a query pool. When warm-up starts, it pulls requests from this pool and sends them concentrated to the warm-up target node.</p><p>Queries dequeued aren’t used just once and discarded — <strong>they’re reused up to 5 times</strong>. When Elasticsearch builds query caches, it applies a <a href="https://www.elastic.co/blog/elasticsearch-caching-deep-dive-boosting-query-speed-one-cache-at-a-time#:~:text=There%20is%20also%20a%20condition%20for%20the%20minimum%20frequency%20to%20be%20eligible%20for%20caching%2C%20so%20that%20a%20single%20invocation%20will%20not%20result%20in%20the%20cache%20being%20filled">minimum frequency condition</a>, so a single request isn’t enough to create a cache entry.</p><p>Looking at Lucene’s <a href="https://github.com/apache/lucene/blob/683f59f66b9f8d48c8f28840b9d7722b6f079b86/lucene/core/src/java/org/apache/lucene/search/UsageTrackingQueryCachingPolicy.java#L116C3-L138C4">minFrequencyToCache</a>() method used by Elasticsearch, the same query needs to come in at least 2 times (or 5 times depending on conditions) to become a caching candidate. If we sent every query just once, we&#39;d increase disk I/O without actually building caches. So we force cache creation with a &quot;real traffic + repeated identical queries&quot; combination.</p><h4>5.3. Warm-Up Completion and Traffic Serving: warmup pass &amp; include &amp; serve traffic Phases</h4><p>Phases (4) warmup pass, (5) include, and (6) serve traffic in the diagram are the process of evaluating whether warm-up is complete for a node, and if passed, adding it to prefer_nodes to receive actual search traffic.</p><p>Query send rate follows the QPS (Queries Per Second) setting defined in Central Dogma. Response latency is tracked by storing the most recent N requests in a Circular Buffer and calculating p50 and p90 latency at configured intervals to check if they meet the criteria.</p><p>Warm-up success requires three conditions: total request count meets the minimum threshold, minimum execution time has elapsed, and recent response p50 and p90 latency are both within thresholds. When all conditions are met, warm-up is judged successful, the node is removed from the Redis warm-up queue, and added to prefer_nodes in Central Dogma to receive search traffic.</p><p><strong>Warm-Up Failure and Retry</strong></p><p>If maximum execution time is reached without meeting criteria, warm-up is judged as failed. In this case, the node is not removed from the warm-up queue. The worker releases the lock and exits. Another search-coordinator Pod then acquires the lock and retries warm-up. Even with temporary network delays or Kafka lag, the system can recover through self-retry.</p><h4>5.4. Safeguards to Prevent Shard Shortage</h4><p>To warm up a data node, we first need to exclude it from search targets. The problem is that if this is applied to multiple nodes simultaneously, situations can arise where “both primary and replica for this shard are warming up, making search completely impossible.” So we added safeguards with the goal of “warm up, but never create a situation where shards become unavailable.”</p><p><strong>Limiting the Number of Nodes That Can Be Excluded from Search</strong></p><p>We limited simultaneous warm-up to a maximum of 2 nodes — meaning at most 2 nodes can be excluded from search targets at once. This isn’t an arbitrary number but calculated based on our shard replication policy. Our indexing system enforces at least 2 replica shards per shard. So each shard consists of 1 primary and 2 replicas. Limiting excluded nodes to 2 ensures that even in the worst case, at least 1 node holding that shard always remains available.</p><p>To enforce this limit, we periodically check the warm-up queue. If 3 or more nodes are queued, we stop warm-up for the oldest node (by timestamp) and immediately return it to the prefer_nodes search target set. In this case, warm-up isn&#39;t 100% complete, but since caches warm up quickly once real traffic starts, latency stabilizes soon.</p><h3>6. Operational Results</h3><p>In actual operation, we achieved results meeting all three criteria we set during project goal-setting.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7qPFhzkAN2AW3iJhMy9ncA.png" /></figure><h4>6.1. No Need for Humans to Monitor and Adjust Timing During Deployment</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0CmPE7T6W_dpLqrpf9u9Mg.png" /></figure><p>Previously, one person could only handle one cluster at a time, so deploying all four took nearly half a day. Now, we can run rolling restarts on each cluster simultaneously, and search-coordinator handles the exclude → warmup → include process automatically, enabling total deployment completion in 1–2 hours.</p><p>The biggest change is the significant reduction in psychological burden around deployments. Before, we had to steel ourselves: ‘Today I need to mentally prepare for a deployment.’ Now it’s something we can do whenever needed. It’s no longer a tense operation that requires picking a specific late-night window — we can run rolling restarts regardless of whether it’s peak time or not.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vKoQq5QBKngOo4wgdjNmmA.png" /><figcaption>First case of Elasticsearch data node warm-up</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Vg_UOftSan5xSTnfRCfigw.png" /></figure><h4>6.2. Search p99 Latency Stays Under 1s Even During Rolling Restarts</h4><p>Before introducing the warm-up system, p90 latency would spike to 3–5x normal levels when rolling restarts began. After introduction, latency stays stable at normal levels even during rolling restarts. Notably, p99 latency also remains servable under 1s.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dj2epnhulWtBFRXlDVuy0w.png" /><figcaption>[Before] Rolling Restart Latency Without Warm-Up</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*12JSkqEj1T0qxdJWsUVu1w.png" /><figcaption>[After] Rolling Restart Latency With Warm-Up</figcaption></figure><h4>6.3. The System Handles Unexpected Node Restarts Automatically</h4><p>Another important change is handling unexpected situations. The figure below shows a case where we arbitrarily restarted a data node and the exclude → warmup → include process completed successfully.</p><p>Before, when an ES data node restarted due to infrastructure issues, all we could do was watch and wait for “when the cache refills and latency returns to normal.” Now, the moment a node goes down, it’s automatically excluded from search targets, and after coming back up, it only receives traffic after passing warm-up. Thanks to this, we now have the confidence that unexpected restarts won’t immediately cascade into full-blown outages.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0KHRdl22H4MPYpku_-EZzA.png" /><figcaption>Successful warm-up notification for data node 17 after restart</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*_QgiVcdkt9l6FT9s.png" /><figcaption>Metrics showing automatic warm-up progress for data node 17 after restart</figcaption></figure><h3>7. Limitations and Next Steps</h3><p>In this project, our goal was to ensure that even if nodes are temporarily excluded from search targets during warm-up, specific shards never become completely unsearchable. This significantly improved stability during node restarts, but some areas remain unsolved.</p><p>For example, if the entire ES cluster becomes unavailable or network issues prevent access to ES itself, search-coordinator’s intervention capability is very limited. Also, when search query DSL structures change significantly, there can be a gap between query patterns accumulated in Kafka and actual traffic, potentially reducing warm-up effectiveness.</p><p>Currently, our warm-up system focuses on data node restart scenarios. Going forward, we plan to expand our coverage to handle full cluster failures and other unpredictable situations without severely impacting search.</p><p>Another next step is extensibility. While this system was built for Karrot’s internal environment, we’re also considering how to package it so other teams or companies using ECK can easily adopt it.</p><h3>Conclusion</h3><p>In summary, here’s the problem we solved: after adopting ECK, latency spikes during rolling restarts meant we had to avoid deploying during peak hours. By placing search-coordinator in front of Elasticsearch and controlling traffic so only warmed-up nodes receive search requests, everything changed. Now, clicking the ArgoCD sync button triggers the system to complete deployment safely on its own, regardless of whether it’s peak time or not.</p><p>As a result, the Search Platform team can spend time building better search experiences instead of worrying about “when should we deploy?”</p><h3>Join Us</h3><p>The Karrot Search Platform team will continue improving how we operate our Kubernetes-based search engine. We’re working to build a robust platform that can reliably handle massive traffic while delivering better search results. If you’d like to join us in solving these challenges, check out the job posting below.</p><p>🥕 <a href="https://about.daangn.com/jobs/6653346003/">Software Engineer, Backend — Search Platform</a></p><p>Thanks for reading!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=0d81d433c5c1" width="1" height="1" alt=""><hr><p><a href="https://medium.com/daangn/running-elasticsearch-on-kubernetes-the-easy-way-part-2-data-node-warm-up-0d81d433c5c1">Running Elasticsearch on Kubernetes the Easy Way, Part 2 — Data Node Warm-Up</a> was originally published in <a href="https://medium.com/daangn">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[당근 검색 엔진, 쿠버네티스로 쉽게 운영하기 2편 — 데이터 노드 웜업 적용]]></title>
            <link>https://medium.com/daangn/%EB%8B%B9%EA%B7%BC-%EA%B2%80%EC%83%89-%EC%97%94%EC%A7%84-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EB%A1%9C-%EC%89%BD%EA%B2%8C-%EC%9A%B4%EC%98%81%ED%95%98%EA%B8%B0-2%ED%8E%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%85%B8%EB%93%9C-%EC%9B%9C%EC%97%85-%EC%A0%81%EC%9A%A9-f687a6c2c00a?source=rss----4505f82a2dbd---4</link>
            <guid isPermaLink="false">https://medium.com/p/f687a6c2c00a</guid>
            <category><![CDATA[elasticsearch]]></category>
            <category><![CDATA[search]]></category>
            <category><![CDATA[k8s]]></category>
            <category><![CDATA[elasticsearch-cluster]]></category>
            <category><![CDATA[eck]]></category>
            <dc:creator><![CDATA[Dongsun Shin]]></dc:creator>
            <pubDate>Mon, 15 Dec 2025 05:42:56 GMT</pubDate>
            <atom:updated>2025-12-15T06:23:15.087Z</atom:updated>
            <content:encoded><![CDATA[<h3>당근 검색 엔진, 쿠버네티스로 쉽게 운영하기 2편 — 데이터 노드 웜업 적용</h3><p>안녕하세요, 당근 검색 플랫폼팀 Ellie, Jarry예요. 검색플랫폼팀은 당근의 여러 서비스에서 발생하는 방대한 검색 트래픽을 빠르고 안정적으로 처리하고, 더 나아가 새로운 검색 경험을 가능하게 하는 플랫폼을 만드는 팀이에요. 지난 글인 <a href="https://medium.com/daangn/%EB%8B%B9%EA%B7%BC%EB%A7%88%EC%BC%93-%EA%B2%80%EC%83%89-%EC%97%94%EC%A7%84-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EB%A1%9C-%EC%89%BD%EA%B2%8C-%EC%9A%B4%EC%98%81%ED%95%98%EA%B8%B0-bdf2688df267">1편</a>에서는 저희가 검색 인프라를 쿠버네티스(ECK)로 이관하면서 배포 자동화와 배포 시간 단축을 이뤘던 이야기를 공유했어요. 5시간 걸리던 배포가 90분으로 줄었고, 누구나 안전하게 검색 클러스터를 배포할 수 있게 됐죠.</p><p>그리고 1편을 마무리할 때 이런 이야기를 했는데요.</p><blockquote><em>“피크 타임 트래픽일 때 배포하면 CPU 사용률과 레이턴시가 튀는 현상이 발생하는 것을 확인할 수 있었어요. (…) 피크 타임을 피해서 배포하는 등의 단기적으로 쉽게 해결할 수 있는 방안도 있기 때문에, 이 문제는 논이슈로 감안하고 진행하기로 했어요.”</em></blockquote><p>당시만 해도 “피크 타임만 피하면 되지”라고 생각했어요. 하지만 시간이 지나면서, <strong>“피크 타임을 피해야 한다”는 제약 자체가 문제</strong>라는 걸 깨달았어요. 운영자가 배포 시간을 계속 신경 써야 했고, 배포가 시작되면 여전히 긴장하며 지켜봐야 했으니까요. 이번 글에서는 왜 이걸 문제라고 생각하게 되었는지, 그리고 어떻게 해결했는지 이야기해 볼게요.</p><h3>1. ECK 도입 후에도 남은 과제</h3><h4>1.1. 검색 인프라, 2년 만에 얼마나 커졌을까</h4><p>1편을 쓸 당시에는 검색 클러스터가 하나였고, 피크 타임 트래픽은 초당 약 1,000건(1K QPS) 정도였어요.</p><p>2년이 지난 지금은 어떨까요? 당근이 성장하면서 검색 인프라도 함께 커졌어요. 지금은 목적에 따라 네 개의 클러스터를 운영하고 있고, 각 클러스터가 받는 피크 타임 트래픽도 아래처럼 크게 달라졌어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bnLzt8X_mNZQVtXqPiFMkA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AtiGCKJRLD1MjVHxv2GYSg.png" /></figure><p>1편에서 말했듯이, ECK 기반 검색 클러스터는 다음과 같은 구조로 동작하고 있어요. 쿠버네티스 환경에서 Elasticsearch Custom Resource를 정의하고, 이를 기반으로 StatefulSet으로 운영하는 구조예요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/884/1*DIEwUBuI9oJCq7zgJRjm5g.png" /><figcaption>ECK 기반 검색 클러스터 구성도</figcaption></figure><p>트래픽만 늘어난 게 아니에요. 클러스터가 네 개로 늘어나면서 운영 부담도 함께 커졌어요. 1편에서 다뤘던 것처럼, 한 클러스터를 배포하는 데는 대략 90분 정도가 필요했어요. 그런데 클러스터가 네 개로 늘어나면서 전체 배포에 6시간 이상이 걸리게 됐고, 그 긴 시간 동안 담당자가 계속 붙어서 모니터링해야 했죠.</p><h4>1.2. “피크 타임을 피해서 배포하세요”의 한계</h4><p>1편에서 우리는 ECK를 도입해 배포 자동화를 이뤘다고 말했지만, 한 가지 중요한 포인트가 있었어요. 바로 “<strong>피크 타임만 피하면</strong>” 이라는 전제 조건이에요.</p><p>처음에는 이게 큰 문제처럼 느껴지지 않았어요. 새벽이나 오전 시간대에 배포하면 되니까요. 그런데 클러스터가 네 개로 늘어나고, 배포할 일이 잦아지면서 현실적인 문제가 드러났어요.</p><p>첫째, <strong>배포 가능한 시간이 점점 줄었어요</strong>. 피크 타임을 피하고, 점심시간도 피하고, 퇴근 전후도 피하다 보면 배포할 수 있는 시간이 하루에 몇 시간 되지 않았어요. 긴급 패치가 필요할 때 “지금은 피크 타임이라 못 해요”라고 말해야 하는 상황이 생겼어요.</p><p>둘째, <strong>배포가 점점 “큰 이벤트”처럼 느껴지기 시작했어요</strong>. ArgoCD sync 버튼을 누르기 전에 “지금 트래픽 괜찮지?”, “이 시간에 해도 되지?”를 확인해야 했고, 배포 중에도 레이턴시 그래프를 계속 지켜봐야 했어요. 배포 자동화를 했는데도 왜 이렇게 아직도 긴장하면서 배포해야 할까?라는 의문이 들기 시작했죠.</p><p>결국, 우리가 진짜 풀어야 할 문제는 이거였어요.</p><blockquote><strong><em>“피크 타임이든 아니든, 언제든 마음 편하게 배포할 수 있는 상태를 만들자.”</em></strong></blockquote><p><strong>그렇다면 왜 우리는 피크 타임을 그렇게 두려워했을까요?</strong></p><p>단순히 트래픽이 많아서만은 아니었어요. 진짜 문제는 배포 과정, 그중에서도 <strong>롤링 리스타트 중에 레이턴시가 급증하는 현상</strong> 때문이었어요. 그럼, 이제 롤링 리스타트 중에 왜 레이턴시가 올라갔는지, 그 근본 원인을 하나씩 파헤쳐 볼게요.</p><h3>2. 롤링 리스타트가 무서웠던 진짜 이유</h3><p>쿠버네티스 StatefulSet 롤링 리스타트는 Pod를 하나씩 차례대로 업데이트하는 기능이에요. Pod가 Ready 상태가 되면 바로 다음 Pod를 업데이트하죠.</p><p>하지만 여기서 문제는 Elasticsearch처럼 캐시에 크게 의존하는 시스템에서는 이 방식이 안전하지 않다는 거예요. Pod가 Ready 상태가 됐다고 해서 바로 트래픽을 처리할 수 있는 게 아니고, 자주 조회되는 데이터가 메모리(page cache, query cache 등)에 올라와야 비로소 빠른 응답이 가능하거든요.</p><p>당근도 실제로 ECK의 Rolling Restart 과정에서 이 문제로 큰 장애를 겪었어요. 그 경험이 웜업 시스템을 만들게 된 출발점이 됐죠.</p><h3>2.1. 실제 장애 이야기</h3><p>어느 날, Elastic Operator 버전 이슈로 모든 Elasticsearch StatefulSet이 Rolling Restart 되는 상황이 생겼어요. 보통의 쿠버네티스 서비스라면 Pod가 한 대씩 내려갔다가 올라오면서 큰 문제없이 진행됐을 거예요.</p><p>하지만 Elasticsearch의 현실은 달랐어요. Rolling Restart가 진행되면서 Pod가 Ready 상태가 되자마자 다음 Pod가 재시작됐는데, ES 입장에서는 아직 준비가 안 된 상태였어요. 재시작된 노드들은 캐시가 완전히 비어 있었고, 쿼리 성능이 크게 저하된 상태로 곧바로 트래픽을 받아야 했죠. 이 과정에서 일부 샤드는 primary와 replica가 연달아 영향을 받으면서 가용성까지 떨어졌고, 결국 검색 서비스 전반에 큰 장애가 발생했어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SpAUOhMUMu22a0ybBalrWA.png" /><figcaption>[Rolling Restart 장애 시점] 검색 API 전체 에러율 60%, 레이턴시 3s까지 치솟던 상황</figcaption></figure><p>이 경험을 통해 “롤링 리스타트는 쿠버네티스가 알아서 안전하게 처리해 줄 거야”라는 기대가, 적어도 Elasticsearch에서는 통하지 않는다는 사실을 확실히 배웠어요.</p><h4>2.2. 데이터 노드 재시작 시 벌어지는 일</h4><p>데이터 노드가 재시작될 때 Elasticsearch 내부에서는 몇 가지 일이 동시에 일어나요.</p><p>첫째, 재시작된 데이터 노드는 file system cache가 전부 비어 있는 상태로 Elasticsearch 클러스터에 합류해요. 이 상태에서 쿼리를 받으면 디스크에서 데이터를 읽어야 하므로 검색 latency가 크게 증가해요.</p><p>둘째, 데이터 노드가 내려가 있는 동안 해당 샤드의 부하가 다른 복사본 샤드로 집중돼요. 이 두 가지가 겹치는 순간이 클러스터가 가장 불안정한 상태예요.</p><p><strong>캐시가 비어 있는 새 노드의 문제</strong></p><p>Elasticsearch는 page cache 외에도 query cache, field data cache, request cache 등 여러 레벨의 캐시를 활용해요. 하지만 데이터 노드가 새로 뜨면 이 캐시들이 전부 비어 있는 상태로 시작해요. Cache miss가 발생할 때마다 디스크에서 segment를 읽어야 하고, 이 지연이 쌓이면서 search thread pool 큐에 요청이 밀리기 시작해요. 결과적으로 p50, p90 latency가 평소보다 몇 배씩 튀게 되죠.</p><p><strong>남은 노드로 부하가 집중되는 문제</strong></p><p>데이터 노드 하나가 내려가면 그 노드가 들고 있던 replica 샤드는 Unassigned 상태가 돼요. 이때 ES 클러스터는 yellow 상태로 전환되는데, 이건 “primary 샤드는 모두 살아있지만, 일부 replica는 할당되지 않은 상태”를 뜻해요. 당장 서비스가 멈추진 않지만, 두 가지 문제가 생겨요.</p><ul><li>남은 replica 샤드들이 모든 요청을 처리해야 해서 부하가 가중돼요.</li><li>replica가 없는 샤드는 single point of failure가 돼요. 이 상태에서 해당 primary를 가진 노드마저 죽으면 red 상태, 즉 검색/색인 요청을 처리할 수 없는 상태가 돼요.</li></ul><p>정리하자면, 요청량은 그대로인데 처리할 노드 수가 줄어들어 남은 노드에 부하가 집중되는 거예요. 이후 cold cache 상태의 노드가 다시 join하면, 캐시가 비어 있는 노드로 라우팅 된 요청의 latency가 급증하기 시작하죠. 그리고 이 두 가지 문제가 겹치는 순간, Elasticsearch에서 롤링 리스타트가 유난히 무섭게 느껴져요.</p><h3>3. 목표와 해결 전략 정하기</h3><p>위처럼 문제를 분석하고 나서, 저희는 도달하고자 하는 목표 상태를 정했어요.</p><blockquote><em>“언제든 Elasticsearch 배포 버튼을 눌러도, 검색 서비스가 무너지지 않는 상태를 만들자.”</em></blockquote><p>구체적으로는 세 가지를 원했어요.</p><ul><li>배포할 때 사람이 모니터링하면서 타이밍을 조절할 필요가 없을 것</li><li>Rolling Restart 중에도 검색 p99 latency 1s 이내일 것</li><li>예기치 못한 노드 재시작에도 시스템이 알아서 대응할 것</li></ul><h4>3.1. 쿠버네티스 기본 메커니즘으로는 왜 해결하지 못할까?</h4><p>“쿠버네티스 readinessProbe를 조절하거나, postStart hook으로 웜업하면 되지 않나요?”라는 의문이 들 수도 있어요. 하지만 Elasticsearch의 동작 방식 때문에 이 방법은 통하지 않아요.</p><p>핵심은 <strong>Elasticsearch가 클러스터에 join하는 시점</strong>이에요. 데이터 노드 Pod가 Running 상태가 되면, 곧바로 Elasticsearch 클러스터에 join해요. 이건 쿠버네티스의 readinessProbe나 postStart hook과는 완전히 별개의 일이에요. Elasticsearch 프로세스가 뜨자마자 마스터 노드에 “나 준비됐어”라고 알리고, 마스터 노드는 바로 이 노드에 샤드를 배치하기 시작해요.</p><p>readinessProbe 통과 전이라고 하더라도 Elasticsearch 클러스터 관점에서는 이미 데이터 노드로 편입된 상태예요. 쿠버네티스 Service의 Endpoints에서 빠져 있어도 소용 없어요. Elasticsearch는 자체 노드 디스커버리와 라우팅을 사용하기 때문에, 쿠버네티스 Service를 거치지 않고 클러스터 내부에서 직접 해당 노드로 요청을 보내요. postStart hook도 마찬가지예요. hook이 완료되기 전에 ES 프로세스는 이미 클러스터에 join할 수 있어요. hook 완료를 기다려주지 않죠.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8SoPE6Icw8X6nVyNi89e2Q.png" /></figure><p>근본적으로 쿠버네티스나 Elasticsearch 설정만으로는 “웜업이 끝날 때까지 트래픽을 받지 않는” 상태를 자동화하기 어려워요. 그래서 해결 전략은 한 줄로 정리할 수 있어요.</p><blockquote><em>“노드는 마음껏 재시작하되, 웜업이 끝나기 전까지는 검색 트래픽을 받지 않게 하자.”</em></blockquote><p>이 아이디어를 실제로 구현한 것이 바로 웜업 시스템이고, 그 중심에 있는 핵심 컴포넌트가 search-coordinator예요. 다음 섹션에서 어떻게 동작하는지 자세히 설명할게요.</p><h3>4. search-coordinator Proxy 아키텍처</h3><p>이제 어떻게 하면 안심하고 Elasticsearch 배포 버튼을 누를 수 있게 되었는지 방법을 살펴볼게요.</p><p>핵심 아이디어는 간단해요. 서버와 Elasticsearch 사이에 search-coordinator라는 프록시를 두고, 웜업이 끝난 데이터 노드만 검색에 참여시키는 구조를 만든 거예요. 이 구조 덕분에 데이터 노드가 재시작되더라도 검색 트래픽은 항상 웜업이 완료된 노드로만 흘러가게 됐어요.</p><h4>4.1. 아키텍처 개요</h4><p>기존에는 서비스가 Elasticsearch에 직접 연결되어 있어서, 트래픽을 통제할 권한이 없었어요. 현재는 모든 요청이 search-coordinator를 거쳐 가면서 트래픽 제어권이 생겼어요. 네임스페이스 하나당 ECK 클러스터 하나, search-coordinator 하나를 1:1 구조로 운영하고 있죠. 아래와 같이 ECK 클러스터만 존재하던 구조에 search-coordinator 모듈을 새롭게 추가했어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Sv6HKVdPsbWtQN5JcM6WwQ.png" /><figcaption>[Before] ECK 기반 검색 클러스터</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Zl82b_mDxbarGSMNHnc8vw.png" /><figcaption>[After] ECK 기반 검색 클러스터 with Search Coordinator</figcaption></figure><p>search-coordinator는 두 가지 역할을 해요.</p><p>첫째는 <strong>HTTP 프록시</strong>예요. 검색/색인 요청을 받아서 ES로 전달하는데, 같은 쿠버네티스 클러스터에서 내부 DNS로 통신하기 때문에 추가 지연은 거의 없어요.</p><p>둘째는 <strong>웜업 오케스트레이터</strong>예요. 어떤 노드가 검색에 참여할 수 있는지를 판단하고 그 상태를 관리해요. 종료 시그널을 받은 노드는 즉시 검색 대상에서 제외하고, 새로 올라온 노드는 웜업이 끝난 뒤에만 검색 대상에 추가해요.</p><h4>4.2. prefer_nodes로 검색 대상 노드 제어하기</h4><p>검색에 참여할 수 있는 데이터 노드 목록은 Central Dogma로 관리해요.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ef2740661882b28b22cc61d288771eb8/href">https://medium.com/media/ef2740661882b28b22cc61d288771eb8/href</a></iframe><p>prefer_nodes는 “현재 검색 대상이 되는 데이터 노드 목록”이에요. 새로 뜬 노드는 웜업을 통과하기 전까지 이 목록에 포함되지 않아요. warmup 섹션에는 QPS, latency 임계값, 최소/최대 시간처럼 웜업 판단에 필요한 기준을 정의하고, 클러스터별로 다른 값을 설정할 수 있어요.</p><p>search-coordinator는 검색 요청을 받으면 Central Dogma에 등록된 Elasticsearch 데이터 노드로만 검색 요청을 보내요. 덕분에 웜업 중인 노드로는 검색 트래픽이 흐르지 않는 구조를 만들 수 있었어요.</p><h4>4.3. 단일 상태 관리자로서의 search-coordinator</h4><p>Central Dogma를 읽고 쓰는 주체를 search-coordinator 하나로만 제한한 데에는 분명한 이유가 있어요.</p><p>첫째, 노드 라우팅 상태의 Single Source of Truth를 유지하고 싶었어요. “어떤 노드가 검색 서빙 대상인지”에 대한 정보를 한 곳에만 두고 싶었거든요. 여러 서버가 각자 Elasticsearch 상태를 조회해서 “지금 쓸 수 있는 노드”를 판단하기 시작하면 폴링 타이밍마다 상태가 어긋나고, 디버깅 포인트도 기하급수적으로 늘어나요.</p><p>둘째, 라우팅 정책을 한 컴포넌트에 온전히 캡슐화하고 싶었어요. 검색 대상 데이터노드 포함/제외, 웜업 성공 여부, 예외 처리 같은 결정이 search-coordinator 안에서만 이루어지면, 다른 모듈은 ES 내부 상태를 알 필요가 없어요. 반대로 애플리케이션이나 배치 스크립트가 직접 노드 목록을 수정하게 되면 “누가 언제 어떤 이유로 데이터 노드를 검색 대상에서 제외하거나 추가했는지” 추적하기 어려워지거든요.</p><h3>5. search-coordinator 웜업 오케스트레이션 아키텍처</h3><p>이제 노드 한 대가 내려갔다가 다시 합류할 때까지의 과정을 조금 더 상세히 살펴볼게요. Elasticsearch 데이터 노드는 쿠버네티스 StatefulSet의 Pod로 운영돼요. search-coordinator는 이 데이터 노드 Pod의 라이프사이클에 맞춰 동작해요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LfN4WQHC5Nqjog1yW-yxwg.png" /><figcaption>데이터노드 라이프사이클 (<strong>prestop</strong> → <strong>exclude</strong> → <strong>warmup</strong> → <strong>warmup pass </strong>→ <strong>include</strong> → <strong>serve traffic</strong>)</figcaption></figure><h4>5.1. 노드가 내려갈 때: prestop &amp; exclude 단계</h4><p>위 도식의 (1)prestop, (2)exclude 단계는 데이터 노드가 안전하게 서빙에서 제외되는 과정이에요.</p><p>데이터 노드가 내려갈 때 search-coordinator는 크게 두 가지 작업을 수행해요. ES 데이터 노드 Pod가 종료되기 직전에 preStop 스크립트가 실행되면서 종료 예정인 노드 정보가 search-coordinator로 전달돼요. 그러면 search-coordinator는 먼저 Central Dogma의 prefer_nodes에서 해당 노드를 제거(exclude)해서 검색 트래픽이 더 이상 흐르지 않게 만들어요. 그다음 같은 노드를 Redis Sorted Set 기반의 웜업 대기열에 추가해요. score로 타임스탬프를 사용해서 나중에 가장 오래된 노드부터 순서대로 꺼낼 수 있게 했어요.</p><p><strong>preStop hook 실패에 대비한 Fallback</strong></p><p>preStop hook이 실패하거나 아예 호출되지 않는 경우도 있어요. 이런 상황을 대비해 search-coordinator-controller 모듈을 두고, 쿠버네티스 Informer로 Elasticsearch 데이터 노드 Pod 이벤트를 구독하고 있어요. preStop이 호출되지 않았는데 Pod가 Terminating으로 진입하면 Informer가 이를 감지하고, controller가 동일한 작업을 대신 수행해요. preStop hook만 의존하지 않고 Pod의 실제 라이프사이클을 기준으로 한 번 더 검증하는 안전망이에요.</p><h4>5.2. 실 트래픽 기반 웜업: warmup 단계</h4><p>위 도식의 (3)warmup 단계는 cold cache 상태의 노드에 대해 캐시를 채워주는 과정이에요.</p><p>노드가 다시 올라오면 본격적인 웜업이 시작돼요. 다만 여러 search-coordinator Pod가 동시에 같은 노드를 웜업하면 오히려 문제가 생길 수 있어서, 먼저 “누가 이 노드를 웜업할지”를 정하는 단계가 필요했어요.</p><p><strong>Redis 분산 락으로 웜업 담당자 정하기</strong></p><p>웜업은 여러 search-coordinator Pod 중 딱 하나의 Pod만 담당하도록 했어요. 동일한 데이터 노드에 여러 Pod가 동시에 웜업을 시도하면, 같은 샤드로 중복 쿼리가 몰리면서 불필요한 부하가 생기고, 경우에 따라 오히려 ES에서 429 Too Many Requests가 발생해 요청이 거부될 수 있거든요. 반대로 하나의 Pod만 붙여도 웜업에 필요한 QPS는 충분히 만들 수 있었어요.</p><p>그래서 “이 노드 웜업은 누가 담당하는지”를 결정하기 위해 Redis 분산 락을 사용했어요. 모든 웜업 워커가 Redis 웜업 대기열을 폴링하다가 동일한 노드를 감지하면, 가장 먼저 락을 획득한 Pod만 해당 노드 웜업을 진행해요. 나머지 Pod들은 락의 주인이 아니면 바로 스킵해서, 한 노드에 대해 항상 단일 search-coordinator만 웜업하도록 보장해요. 락 토큰은 podIP/podName-timestamp-randomHex 형식으로 만들어, 어떤 Pod가 락을 잡았는지 쉽게 추적할 수 있게 했어요.</p><p><strong>웜업 쿼리 수집</strong></p><p>저희는 웜업 쿼리로 운영 트래픽 패턴을 그대로 사용하기로 했어요. 모든 검색 요청 로그를 Kafka 토픽에 쌓고 있어서, 웜업을 담당하는 search-coordinator 워커가 이 토픽을 실시간으로 consume해서 쿼리 풀을 만들어요. 웜업이 시작되면 이 풀에서 요청을 꺼내 웜업 대상 노드로 집중해서 보내요.</p><p>이때 큐에서 꺼낸 요청을 한 번만 쓰고 버리지는 않아요. <strong>최대 5회까지 재사용</strong>해요. Elasticsearch는 쿼리 캐시를 만들 때 <a href="https://www.elastic.co/blog/elasticsearch-caching-deep-dive-boosting-query-speed-one-cache-at-a-time#:~:text=There%20is%20also%20a%20condition%20for%20the%20minimum%20frequency%20to%20be%20eligible%20for%20caching%2C%20so%20that%20a%20single%20invocation%20will%20not%20result%20in%20the%20cache%20being%20filled">최소 빈도 조건</a>을 적용해서, 단 한 번의 요청만으로는 캐시가 생성되지 않아요.</p><p>실제로 Elasticsearch에서 사용하는 Lucene 코드의 <a href="https://github.com/apache/lucene/blob/683f59f66b9f8d48c8f28840b9d7722b6f079b86/lucene/core/src/java/org/apache/lucene/search/UsageTrackingQueryCachingPolicy.java#L116C3-L138C4">minFrequencyToCache(</a>) 메서드를 보면 동일한 쿼리가 최소 2회(혹은 조건에 따라 5회) 이상 들어와야 캐싱 대상이 돼요. 모든 쿼리를 한 번씩만 보내면 disk I/O만 늘고 캐시는 거의 쌓이지 않는 상태가 되는 거죠. 그래서 저희는 &quot;실제 트래픽 + 동일 쿼리 반복&quot; 조합으로 캐시가 실제로 만들어지도록 강제했어요.</p><h4>5.3 웜업 완료 판정 및 트래픽 서빙: warmup pass &amp; include &amp; serve traffic 단계</h4><p>위 도식의 (4)warmup pass, (5)include, (6)server traffic 단계는 웜업이 완료된 노드에 대해 웜업 통과 여부를 판단하고 통과한 노드를 prefer_nodes에 추가해 실제 검색 트래픽을 받게 되는 과정이에요.</p><p>쿼리를 보내는 속도는 Central Dogma에 정의된 QPS(Query Per Second) 설정을 따라요. 응답 Latency는 최근 N개의 요청을 Circular Buffer에 적재하고, 설정한 주기마다 p50, p90 Latency를 계산해서 기준을 만족하는지 확인해요.</p><p>웜업 성공 조건은 세 가지예요. 총요청 수가 최소 개수 기준을 충족하고, 최소 실행 시간 이상 경과했으며, 최근 응답의 p50, p90 Latency가 모두 임계값 이내여야 해요. 이 조건을 모두 만족하면 웜업을 성공으로 판정하고, Redis 웜업 대기열에서 해당 노드를 제거한 뒤 Central Dogma의 prefer_nodes에 추가해 검색 트래픽을 받도록 해요.</p><p><strong>웜업 실패와 재시도</strong></p><p>웜업 최대 실행 시간을 채웠는데도 기준을 만족하지 못하면 웜업 실패로 판단해요. 이때는 웜업 대기열에서 노드를 제거하지 않고, 해당 워커는 락을 해제한 뒤 종료해요. 그러면 다른 search-coordinator Pod가 다시 락을 잡고 웜업을 재시도해요. 일시적인 네트워크 지연이나 Kafka 지연이 있더라도 시스템이 스스로 재시도하면서 복구할 수 있도록 구조를 설계했어요.</p><h4>5.4. 샤드 부족을 막기 위한 안전장치</h4><p>데이터 노드를 웜업하려면 일단 검색 대상에서 제외해야 해요. 문제는 이걸 여러 노드에 동시에 적용하면 이 샤드는 primary도, replica도 전부 웜업 중이라서 검색이 아예 안 되는 상황이 생길 수 있다는 점이에요. 그래서 “웜업은 하되, 샤드가 부족해지는 상황은 절대 만들지 않는다”를 목표로 안전장치를 넣었어요.</p><p><strong>검색 대상에서 빠질 수 있는 노드 수 제한</strong></p><p>동시에 웜업 가능한 노드를 최대 2개로 제한했어요. 즉, 검색 대상에서 동시에 빠질 수 있는 노드를 최대 2개로 제한했어요. 이 값은 임의로 정한 게 아니라 샤드 복제본 정책에 맞춰 계산한 값이에요. 색인 시스템에서는 모든 인덱스가 샤드 당 replica 샤드를 최소 2개를 갖도록 강제하고 있어요. 그래서 하나의 샤드는 primary 1개와 replica 2개로 구성돼요. 검색 대상에서 빠지는 노드를 최대 2개로 제한하면, 최악의 상황에도 해당 샤드를 가진 노드가 최소 1개는 항상 남게 돼요.</p><p>그리고 이 제한을 지키기 위해 웜업 대기열을 주기적으로 확인해요. 대기열에 노드가 3개 이상 쌓이면 timestamp 기준으로 가장 오래된 노드의 웜업을 중단하고 바로 검색 대상 집합인 prefer_nodes로 복귀시켜요. 이 경우 웜업이 100% 완료된 상태는 아니지만, 실 트래픽을 받기 시작하면 캐시가 빠르게 워밍되기 때문에 latency도 금방 안정돼요.</p><h3>6. 운영 결과</h3><p>실제 운영 결과, 프로젝트 목표 설정 시 잡았던 세 가지 기준에 모두 부합하는 성과를 이루었어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*acJXHXPlmjmXfMnD2hXg8A.png" /></figure><h4>6.1. 배포할 때 사람이 모니터링하면서 타이밍을 조절할 필요가 없을 것</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sIWKif1GldbKWyvBY5nSMA.png" /></figure><p>이전에는 한 사람이 한 번에 한 클러스터만 맡을 수 있어서, 클러스터 4개를 모두 배포하려면 거의 반나절을 써야 했어요. 지금은 각 클러스터를 동시에 Rolling Restart로 돌릴 수 있고, search-coordinator가 노드 제외 → 웜업 → 복귀 과정을 알아서 처리해 줘서 전체 배포를 1~2시간 안에 끝낼 수 있는 구조가 됐어요.</p><p>무엇보다 큰 변화는 배포에 대한 심리적 부담이 크게 줄었다는 점이에요. 예전에는 “오늘은 마음 단단히 먹고 배포 한번 해야겠다”라고 준비해야 했다면, 이제는 필요할 때 언제든 진행할 수 있는 작업이 됐어요. 특정 새벽 시간대를 잡아서 긴장하며 진행해야 하는 작업이 아니라, 피크 타임이든 아니든 시간에 관계없이 Rolling Restart를 진행할 수 있어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1qZCAyhpXj6-sLsBp07DwQ.png" /><figcaption>elasticsearch 데이터노드 웜업 첫 사례</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pcFrC3uXWRzcapmWVXYFdA.png" /></figure><h4>6.2. Rolling Restart 중에도 검색 p99 latency 1s 이내일 것</h4><p>웜업 시스템을 도입하기 전에는 Rolling Restart가 시작되면 p90 레이턴시가 평소의 3~5배까지 치솟았어요. 도입 후에는 Rolling Restart 중에도 평소 수준을 안정적으로 유지하게 됐어요. 특히 p99 레이턴시 또한 1s 이내로 서빙 가능해요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dj2epnhulWtBFRXlDVuy0w.png" /><figcaption>[Before] 웜업 적용 전 Rolling Restart 레이턴시</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*12JSkqEj1T0qxdJWsUVu1w.png" /><figcaption>[After] 웜업 적용 후 Rolling Restart 레이턴시</figcaption></figure><h4>6.3. 예기치 못한 노드 재시작에도 시스템이 알아서 대응할 것</h4><p>또 하나의 중요한 변화는 예기치 못한 상황에 대한 대응이에요. 아래 그림은 데이터 노드 한 대를 임의로 재시작했을 때, exclude → warmup → include 과정이 성공적으로 진행된 상황을 보여줘요.</p><p>예전에는 ES 데이터 노드가 인프라 문제로 재시작되면 “언제 캐시가 다시 차서 latency가 정상으로 돌아오나”를 지켜보는 수밖에 없었어요. 지금은 노드가 내려가는 순간 자동으로 검색 대상에서 제외되고, 다시 올라온 뒤에는 웜업을 거쳐 충분히 준비된 상태에서만 트래픽을 받아요. 그 덕분에 예기치 못한 재시작이 발생해도 곧바로 전면 장애로 이어지지 않는다는 안정감을 갖게 됐어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0KHRdl22H4MPYpku_-EZzA.png" /><figcaption>17번 데이터노드 재시작 시 성공적으로 웜업 완료된 Notification</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vd9NhRvIJAXKs_Xn8XpQOw.png" /><figcaption>17번 데이터노드 재시작 시 자동으로 웜업 진행된 메트릭</figcaption></figure><h3>7. 한계와 다음 스텝</h3><p>이번 프로젝트에서는 웜업 과정에서 일시적으로 노드를 검색 대상에서 제외하더라도, 특정 샤드가 완전히 검색 불가능한 상태가 되지 않도록 만드는 것을 목표로 했어요. 그 덕분에 노드 재시작 상황에서는 안정성을 크게 끌어올릴 수 있었지만, 여전히 해결하지 못한 영역도 남아 있어요.예를 들어, ES 클러스터 전체가 불능 상태가 되거나 네트워크 장애로 ES 자체에 접근할 수 없는 경우에는 <strong>search-coordinator</strong>가 개입할 수 있는 범위가 매우 제한적이에요. 또 검색 쿼리 DSL 구조가 크게 바뀌는 상황에서는 Kafka에 축적된 기존 쿼리 패턴과 실제 트래픽 사이에 괴리가 생겨 웜업 효과가 충분히 발휘되지 않을 가능성도 있어요.</p><p>현재 웜업 시스템은 데이터 노드 재시작 상황에 초점을 맞추고 있어요. 앞으로는 클러스터 전체 장애나 예측하기 어려운 상황에서도 검색이 크게 흔들리지 않도록 대응 범위를 넓혀갈 예정이에요. 또 하나의 다음 스텝은 확장성이에요. 지금은 당근 내부 환경에 맞춰 만들어진 시스템이지만, ECK를 사용하는 다른 팀이나 회사에서도 쉽게 적용할 수 있도록 패키징하는 방안도 고민하고 있어요.</p><h3>마무리</h3><p>정리하면, 저희는 이런 문제를 해결했어요. ECK를 도입했지만, 롤링 리스타트 시 레이턴시 스파이크가 발생해, 피크 타임에는 배포를 피해야 했어요. search-coordinator를 Elasticsearch 앞에 두고 웜업이 끝난 노드만 검색 트래픽을 받도록 제어하면서 상황이 달라졌어요. 이제는 ArgoCD sync 버튼을 누르면 시스템이 알아서 안전하게 배포를 완료해줘요. 피크 타임이든 아니든 상관없이요.</p><p>그 결과, 검색 플랫폼팀은 “배포 언제 하지?”라는 고민 대신, 더 좋은 검색 경험을 만드는 데 시간을 쓸 수 있게 됐어요.</p><h3>함께해요</h3><p>당근 검색 플랫폼 팀은 앞으로도 쿠버네티스 기반의 검색 엔진을 더 효율적으로 운영하기 위해 끊임없이 개선해 나갈 예정이에요. 수많은 트래픽을 안정적으로 소화하면서, 더 좋은 검색 결과를 제공할 수 있는 튼튼한 플랫폼을 만들기 위해 노력하고 있고요. 이런 문제를 해결해 나가는 과정에 함께하고 싶으시다면, 아래 공고를 함께 살펴보세요.</p><p>🥕 <a href="https://about.daangn.com/jobs/6653346003/">Software Engineer, Backend — 검색 플랫폼</a></p><p>읽어주셔서 감사해요!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f687a6c2c00a" width="1" height="1" alt=""><hr><p><a href="https://medium.com/daangn/%EB%8B%B9%EA%B7%BC-%EA%B2%80%EC%83%89-%EC%97%94%EC%A7%84-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EB%A1%9C-%EC%89%BD%EA%B2%8C-%EC%9A%B4%EC%98%81%ED%95%98%EA%B8%B0-2%ED%8E%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%85%B8%EB%93%9C-%EC%9B%9C%EC%97%85-%EC%A0%81%EC%9A%A9-f687a6c2c00a">당근 검색 엔진, 쿠버네티스로 쉽게 운영하기 2편 — 데이터 노드 웜업 적용</a> was originally published in <a href="https://medium.com/daangn">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Karrot’s Journey to CDC with MongoDB]]></title>
            <link>https://medium.com/daangn/karrots-journey-to-cdc-with-mongodb-e052b1c3ec9c?source=rss----4505f82a2dbd---4</link>
            <guid isPermaLink="false">https://medium.com/p/e052b1c3ec9c</guid>
            <category><![CDATA[change-data-capture]]></category>
            <category><![CDATA[apache-flink]]></category>
            <category><![CDATA[mongodb]]></category>
            <category><![CDATA[english]]></category>
            <dc:creator><![CDATA[Seungki Kim]]></dc:creator>
            <pubDate>Tue, 09 Dec 2025 03:35:55 GMT</pubDate>
            <atom:updated>2025-12-09T03:40:31.882Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*E6AG4l7beCwV4ri2cm9kkQ.png" /><figcaption>AI Generated Image</figcaption></figure><p>Hello, I’m Daniele, a Software Engineer on the Data Team at Karrot.</p><p>Our team is responsible for reliably loading data from various production-grade databases into our Data Warehouse (BigQuery), enabling quick and easy access to data on organization level.</p><p>Pouring all data into a single BigQuery DW has several advantages. Firstly, running analytical queries directly on production DBs can impact service traffic, so it usually goes without saying that analytic workload isolation is crucial. We can also leverage BigQuery’s powerful distributed processing capabilities to unlock incredibly fast analysis of large-scale data. Finally, consolidating various data sources scattered across services into one place enables cross-service analysis.</p><p>Today, I want to share the challenges we faced especially with MongoDB and how we built MongoDB CDC to solve them.</p><h3>Background</h3><p>Karrot uses various DBMSs suited to each service’s characteristics, including MySQL, PostgreSQL, DynamoDB, and MongoDB. We load data from all these DBs into BigQuery to establish an analytics foundation. For MongoDB specifically, we had been using the Spark Connector for dumps.</p><p>However, as our service grew, so did our data volume. For MongoDB, the existing approach couldn’t satisfy both requirements:</p><ul><li>Generally trying our best to minimize load on the source database.</li><li>Our internal 2-hour data delivery SLO (which would force some load on the source databsae)</li></ul><p>This trade-off became unsustainable. After evaluating several options, we decided to implement MongoDB CDC.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dfQwJBOo6YiYSvdFcyIb4g.png" /><figcaption>CPU Spikes</figcaption></figure><h3>Why CDC?</h3><h4>Goals</h4><ul><li>Improve the dump method for specific tables that are both large and frequently updated</li><li>Stabilize database CPU usage below 60%</li><li>Enable existing dump jobs to complete within the 2-hour SLO</li></ul><p><strong>CDC (Change Data Capture)</strong> was the most efficient approach to achieve these goals.</p><p>CDC works by directly reading database change logs (binlog, oplog, etc.) to capture changed data. It can detect INSERT, UPDATE, and DELETE operations without requiring additional columns in the table. To construct the final table using captured change data, you first take a full snapshot, then merge subsequent changes.</p><p>You might think, “Why not just incrementally query data based on timestamps?” However, such incremental querying methods require some assumptions on timestamp fields like created_at or updated_at to be consistent and correct, which, unfortunately, cannot be guaranteed to be true in all cases. For scalability across various tables, we needed an approach independent from such assumptions on the table schema.</p><p>We first selected the top 5 collections based on data size and update volume as CDC migration candidates. This approach would allow stable data synchronization without bulk queries, significantly reducing DB load while meeting SLO.</p><h4>Comparing Technology Options</h4><p>We evaluated several technology stacks capable of implementing CDC.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*p5zwHPtDXgb2IRGBt7DpRA.png" /><figcaption>Kafka Connect vs Debezium vs Flink</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rt8RmnLA6JIboAprM2WvTA.png" /></figure><h3>Final Choice: Flink CDC</h3><p>After evaluating multiple candidates, we chose <strong>Flink CDC</strong>. Here’s why:</p><h4>1. Native MongoDB Change Stream Support</h4><p>Flink CDC natively supports MongoDB’s Change Stream.</p><p>We could reliably read change logs without developing custom connectors, and Change Stream’s resume token (pointing to the last processed position) naturally integrates with Flink’s checkpoint, enabling accurate resumption from the exact point after failures.</p><p>This allowed us to build a CDC-based pipeline without significantly increasing operational complexity.</p><h4>2. Robust State Management and Stable Checkpoint Mechanism</h4><p>Flink uses a checkpoint strategy that periodically saves state as snapshots to distributed file systems (HDFS/GCS/S3, etc.). Since state is preserved in external storage, we could safely restart from previous points while maintaining <strong>Exactly-Once</strong> processing guarantees even after failures.</p><p>This was particularly important for reliability in large-scale pipeline operations.</p><h4>3. Integrated Pipeline: CDC → Transform → Sink in One</h4><p>CDC tools like Debezium specialize in change data extraction, but require separate systems for post-processing or transformation. Flink CDC handles everything in a single Job:</p><ol><li>Extract change data via CDC</li><li>Perform necessary data cleaning/transformation/filtering</li><li>Reliably load to sinks like BigQuery</li></ol><p>This end-to-end integrated configuration reduced the number of pipelines and lowered operational complexity. The ability to handle data model restructuring and format conversion on the MongoDB to BigQuery path without a separate transformation layer was particularly attractive.</p><h4>4. Excellent Scalability Based on Parallel Processing</h4><p>Flink can subdivide work into computational units and execute them in parallel. Increasing TaskManager count scales throughput nearly linearly, allowing stable response to CDC event spikes by scaling the entire pipeline.</p><p>Scalability was a critical evaluation criterion in our service environment with rapid data growth.</p><h3>Implementation Process</h3><h4>Architecture Design</h4><p>We designed the CDC data pipeline architecture to be relatively simple.</p><p>Our priority was quickly resolving the DB load issue, and we determined that current traffic levels could be handled without intermediate queues like Kafka.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TDDZkfRH4iStPR369fdyWg.png" /><figcaption>Pipeline Architecture (High-level)</figcaption></figure><p><strong>MongoDB’s CDC Mechanism</strong></p><p>MongoDB has a special log called <strong>Oplog</strong>. All write operations (insert, update, delete) are recorded in this log. The <strong>Change Stream</strong> feature allows real-time subscription to this Oplog.</p><p>Change Stream is a high-level API that safely wraps Oplog, enabling applications to directly consume change events.</p><p>Flink CDC subscribes to this Change Stream to receive change events. The overall data flow is:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UId8EReD9SZ6C1cKW_Qm3A.png" /><figcaption>Pipeline Architecture (Detailed)</figcaption></figure><ol><li>Data change occurs in MongoDB (insert, update, delete)</li><li>Change Stream generates change events</li><li>Flink CDC to the Change Stream and receives events</li><li>If necessary, flink performs transformation and processing</li><li>Transformed data is sent to BigQuery</li><li>Data is now available for analysis in BigQuery</li></ol><p><strong>Batch Pipeline Structure</strong></p><p>The backend batch pipeline runs hourly and consists of four stages:</p><ol><li><strong>Schema Evolution</strong>: Compare schema repository with BigQuery tables and auto-add missing fields</li><li><strong>Extract CUD Latest</strong>: Extract recent change events from CDC source and deduplicate</li><li><strong>Merge to Raw</strong>: Merge into raw table in JSON format</li><li><strong>Materialize to Final</strong>: Apply schema to materialize final table</li></ol><p><strong>Why Hourly Batch?</strong></p><p>We chose hourly batch over real-time streaming for several reasons. First, there was no real-time requirement — meeting the 2-hour delivery SLO was sufficient, so we chose a simpler design rather than investing resources in real-time pipelines.</p><p>Additionally, generous time windows allow stable recovery of late-arriving events, and clearly defined failure intervals make reprocessing easier during incidents. Batch processing also makes it easier to guarantee idempotency.</p><p>Since it’s a CDC-based architecture, we left room for future streaming conversion if real-time requirements emerge. However, at this point, we chose to avoid unnecessary complexity and achieve both SLO compliance and operational stability within given constraints.</p><p>Reaching this design involved four key considerations:</p><ol><li><strong>How can we process transactions without gaps?</strong></li><li><strong>How should the initial snapshot phase work?</strong></li><li><strong>How do we handle schema evolution from NoSQL to SQL (MongoDB to BigQuery)?</strong></li><li><strong>How do we verify CDC consistency, and what criteria determine production readiness?</strong></li></ol><h4>1. MongoDB Transaction Processing</h4><p>The first thing we needed to verify when introducing CDC was transaction order guarantee. Without order guarantee, CDC itself becomes meaningless.</p><p>For example:</p><ul><li>If events occur as INSERT → UPDATE → DELETE, the row should be deleted in the final table</li><li>If INSERT → UPDATE → UPDATE, upsert with the last UPDATE state</li></ul><p>Change events for the same primary key must be collected in order to correctly reflect them in the final table.</p><p>We conducted a PoC to verify Change Stream guarantees this. Results confirmed that Change Stream operates on oplog basis, with each event containing a timestamp (ts_ms), allowing event collection in transaction order.</p><p>Here’s an example of Oplog for transactions:</p><pre>// 1. Insert<br>{<br>   &quot;_id&quot;: &quot;1&quot;,<br>   &quot;fullDocument&quot;:&quot;{\\&quot;_id\\&quot;: \\&quot;1\\&quot;, \\&quot;v\\&quot;: \\&quot;A\\&quot;}&quot;,<br>   &quot;operationType&quot;:&quot;insert&quot;,<br>   &quot;ts_ms&quot;:1,<br>   ...<br>}<br><br>// 2. Update (v: A → B)<br>{<br>   &quot;_id&quot;: &quot;1&quot;,<br>   &quot;fullDocument&quot;:&quot;{\\&quot;_id\\&quot;: \\&quot;1\\&quot;, \\&quot;v\\&quot;: \\&quot;C\\&quot;}&quot;,<br>   &quot;operationType&quot;:&quot;update&quot;,<br>   &quot;ts_ms&quot;:2,<br>   &quot;updateDescription&quot;:{<br>      &quot;updatedFields&quot;:&quot;{\\&quot;v\\&quot;: \\&quot;B\\&quot;}&quot;<br>   },<br>   ...<br>}<br><br>// 3. Update (v: B → C)<br>{<br>   &quot;_id&quot;: &quot;1&quot;,<br>   &quot;fullDocument&quot;:&quot;{\\&quot;_id\\&quot;: \\&quot;1\\&quot;, \\&quot;v\\&quot;: \\&quot;C\\&quot;}&quot;,<br>   &quot;operationType&quot;:&quot;update&quot;,<br>   &quot;ts_ms&quot;:3,<br>   &quot;updateDescription&quot;:{<br>      &quot;updatedFields&quot;:&quot;{\\&quot;v\\&quot;: \\&quot;C\\&quot;}&quot;<br>   },<br>   ...<br>}</pre><p>When Change Stream’s fullDocument option is set to &quot;updateLookup&quot;, fullDocument contains the complete document data. When consecutive changes occur within the same transaction, all events&#39; fullDocument are delivered with the same final state.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1ywefqwNRyMJgG1DHoDj3A.png" /></figure><p>Since the Flink MongoDB CDC connector defaults to true, no additional configuration was needed. Ultimately, we only needed to process the fullDocument field of the last event per primary key, simplifying transaction handling.</p><h4>2. Initial Snapshot Phase</h4><p>CDC only captures changes after pipeline activation. Therefore, an initial snapshot phase is needed to retrieve data from the past.</p><p>Flink CDC offers an Initial Snapshot mode that automates this process. However, our testing showed that the default settings couldn’t complete within the Oplog retention period. Even after scaling up resources and tuning, DB load only increased while still being slower than Spark.</p><p>Further tuning might have improved things, but given our limited timeline, we decided it was more practical to switch to tools we were already familiar with. The remaining options were:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ttfhlMcBWGWsnY1xc5E02A.png" /></figure><p>We were already operating a Spark cluster environment, and after comparing methods, Spark’s performance was significantly better for simple read operations, making Spark Job an easy choice for the initial snapshot phase.</p><h4>3. Schema Evolution</h4><p>The biggest challenge in building MongoDB CDC was schema management.</p><p>MongoDB has flexible schemas — developers can add new fields whenever they want. But BigQuery requires explicit schema definitions. Ensuring that MongoDB schema changes are stably reflected in BigQuery tables was our biggest concern.</p><p>We considered two approaches for loading data into schema-based BigQuery:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wHuCEMIcikJjNjRj3p0zLw.png" /><figcaption>Static Schema &amp; Full Schema Evolution</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*p53CogWmB6l_-GMRkPHJjw.png" /></figure><p>Both methods had pros and cons, so we first organized our requirements:</p><p><strong>[Data Team Requirements]</strong></p><ul><li>Pipeline must operate stably</li><li>Schema changes must not cause unexpected problems (e.g., BigQuery Breaking Changes)</li></ul><p><strong>[Service Team Requirements]</strong></p><ul><li>Schema changes shouldn’t take too long to reflect</li><li>Schema changes should be easy to make</li></ul><p>Based on this, we adopted Option 1 (Static Schema). We determined that managing schemas within a controlled scope was necessary to prevent unexpected problems. To also satisfy “quick and easy reflection,” we designed the following structure:</p><p><strong>1. Automation of the Request-Approve-Evolve Framework for Schema Changes</strong></p><p>When the service team members request schema additions, the central data team (which is, us) reviews and approves. Approved changes are automatically detected and new schemas are added to BigQuery. Schema changes are reflected without manual deployment.</p><p><strong>2. Two-Stage Table Separation</strong></p><p>Even when schemas are added, the process of filling field values in existing data is necessary. To handle this efficiently, we separated tables into two stages:</p><p><strong>Stage 1: JSON Raw Table</strong></p><ul><li>Stores data in original JSON format</li><li>Preserves raw data regardless of schema changes, enabling reprocessing without Full Dump when schemas change later</li><li>Clustering applied by primary key — reduces scan range in join queries during CDC change updates, minimizing computation costs</li></ul><p><strong>Stage 2: Final Table</strong></p><ul><li>Upon schema changes, the new schema is applied to the stage 1 raw data and the final table is overwritten by the result.</li></ul><p>Now when schemas change, since raw data is preserved in Stage 1, we only need to regenerate Stage 2 without any expensive re-runs of the snapshot phase.. The schema evolution process that previously took 2–3 hours was reduced to <strong>under 20 minutes</strong>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EL-7T0HXRPKwbwH2bLDSvw.png" /><figcaption>Two-Stage Table Architecture (Raw → Final)</figcaption></figure><h4>4. Consistency Checks</h4><p>More important than building the CDC system was <strong>whether we could trust the data</strong>.</p><p>Since we planned zero-downtime migration for tables already in service before switching from Full Dump to CDC, thorough consistency verification was essential. We first defined consistency criteria, then ran both the existing Full Dump and CDC pipelines simultaneously (dual write) to compare and verify results. We automated this process with alerts for continuous monitoring.</p><p><strong>Check Items</strong></p><ul><li><strong>Record count match</strong>: Does data count during Full Dump match the CDC data count during dual write?</li><li><strong>Data freshness</strong>: Is CDC data arriving properly every hour?</li><li><strong>Duplicate ID check</strong>: Are there duplicate IDs in the ID column?</li></ul><p>These items were used as ongoing monitoring metrics, and for CDC migration consistency verification, we conducted stricter validation. We compared checksums to confirm whether <strong>all fields have identical values</strong> for the same ID, verifying 100% consistency.</p><p>Finally, we confirmed this consistency was maintained without issues for 2 weeks before determining migration readiness.</p><h3>Operations: Learning from Experience</h3><p>Building and deploying a system is just the beginning. What really matters is <strong>operating it stably</strong>.</p><h4>Monitoring System</h4><p>We monitored the following metrics to check CDC system status:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*un2VzE913WeyK4yWrPgxxw.png" /><figcaption>Grafana Monitoring Dashboard for Flink MongoDB CDC</figcaption></figure><p><strong>Core Metrics</strong></p><ul><li><strong>Flink Job Status</strong> Is the Job running normally? Are enough TaskManagers attached? Any restarts or failures?</li><li><strong>Data Throughput</strong> How many records/bytes per second are being processed across the entire MongoDB → Flink → BigQuery pipeline?</li><li><strong>MongoDB Read Load</strong> Is CDC causing excessive load on MongoDB? Check CPU, memory, and network usage.</li><li><strong>BigQuery Load Success Rate</strong> Are records being loaded normally at the Sink stage? Any errors?</li><li><strong>Backpressure</strong> Where in the pipeline are bottlenecks occurring? Any processing delays?</li><li><strong>Checkpoint Stability</strong> Are Flink checkpoints completing stably? Is duration or size increasing abnormally?</li></ul><p>Through actual operations and failure experiences, we set up automatic alerts for SLO violations on Flink Job Status and Backpressure.</p><h3>Fault Tolerance</h3><p>We experienced various failure scenarios during operations. Here’s how Flink behaves in each:</p><p><strong>Scenario 1: Flink Job Failure</strong> Flink Kubernetes Operator automatically restarts the Job and resumes processing from the last checkpoint. Recovery typically takes under 3 minutes.</p><p><strong>Scenario 2: MongoDB Connection Lost</strong> Reconnection attempts use exponential backoff. Temporary network instability auto-recovers; alerts are sent if it persists over 10 minutes.</p><p><strong>Scenario 3: BigQuery Load Failure</strong> Failed data is stored in BigQuery via a Dead Letter Queue defined in the Flink application, with alerts sent within 1 hour. Manual reprocessing follows problem resolution.</p><h3>Conclusion</h3><p>Through this project, we reaffirmed that “trust” is ultimately what matters most in data pipelines. Investing time in thorough PoC and consistency verification was the biggest factor in gaining team confidence, and choosing technology suited to our situation rather than the latest technology was also a good decision.</p><p>Going forward, we have the following goals for faster and more stable data delivery:</p><ul><li>Minimize end-to-end latency within available resources</li><li>Increase throughput relative to cost through efficient resource utilization</li></ul><p>This post’s scope is limited to CDC with MongoDB, but there are so much more going on at Karrot to ensure consistency, stability, and performance in data delivery.</p><p>If you’re interested in data pipelines and infrastructure, join the Data Team and help build data infrastructure for Karrot’s users!</p><p><strong>👉 Karrot Data Team Job Openings</strong></p><ul><li><a href="https://team.daangn.com/jobs/4300801003/"><strong>Software Engineer, Data</strong></a></li><li><a href="https://team.daangn.com/jobs/7507320003/"><strong>Data Analytics Engineer</strong></a></li></ul><h3>References</h3><ul><li><a href="https://nightlies.apache.org/flink/flink-cdc-docs-release-3.1/docs/connectors/flink-sources/mongodb-cdc/">https://nightlies.apache.org/flink/flink-cdc-docs-release-3.1/docs/connectors/flink-sources/mongodb-cdc/</a></li><li><a href="https://www.mongodb.com/docs/manual/changestreams/">https://www.mongodb.com/docs/manual/changestreams/</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e052b1c3ec9c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/daangn/karrots-journey-to-cdc-with-mongodb-e052b1c3ec9c">Karrot’s Journey to CDC with MongoDB</a> was originally published in <a href="https://medium.com/daangn">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[매번 다 퍼올 필요 없잖아? 당근의 MongoDB CDC 구축기]]></title>
            <link>https://medium.com/daangn/%EB%A7%A4%EB%B2%88-%EB%8B%A4-%ED%8D%BC%EC%98%AC-%ED%95%84%EC%9A%94-%EC%97%86%EC%9E%96%EC%95%84-%EB%8B%B9%EA%B7%BC%EC%9D%98-mongodb-cdc-%EA%B5%AC%EC%B6%95%EA%B8%B0-302ae8a0dc23?source=rss----4505f82a2dbd---4</link>
            <guid isPermaLink="false">https://medium.com/p/302ae8a0dc23</guid>
            <category><![CDATA[mongodb]]></category>
            <category><![CDATA[flink-cdc]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[change-data-capture]]></category>
            <category><![CDATA[apache-flink]]></category>
            <dc:creator><![CDATA[Seungki Kim]]></dc:creator>
            <pubDate>Mon, 08 Dec 2025 08:29:14 GMT</pubDate>
            <atom:updated>2025-12-08T08:35:37.757Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*E6AG4l7beCwV4ri2cm9kkQ.png" /><figcaption>AI Generated Image</figcaption></figure><p>안녕하세요, 당근 데이터 가치화 팀의 Software Engineer, Data 다니엘레예요.</p><p>저희 팀은 각 서비스 팀이 운영 DB에서 사용하는 데이터를 Data Warehouse(BigQuery)로 안정적으로 적재해, 전사 구성원들이 데이터를 쉽고 빠르게 활용할 수 있도록 지원하고 있어요.</p><p>운영 DB에 분석 쿼리를 직접 실행하면 트랜잭션 처리에 영향을 줄 수 있기 때문에, 분석 워크로드를 분리해 서비스 안정성을 확보하는 것이 중요해요. 또한 BigQuery의 강력한 분산 처리 능력을 활용하면 대용량 데이터도 빠르게 분석할 수 있고, 서비스별로 분산되어 있던 다양한 데이터 소스를 한곳에 모아 cross-service 분석도 가능해져요.</p><p>오늘은 여러 덤프 작업 중 MongoDB를 다루며 마주한 문제와 이를 해결하기 위해 MongoDB CDC를 구축하게 된 과정을 이야기하고자 해요.</p><h3>배경</h3><p>당근은 중고 거래, 동네 생활, 알바, 중고차 등 각 서비스 특성에 따라 MySQL, PostgreSQL, DynamoDB, MongoDB 등 다양한 데이터베이스를 사용하고 있어요. 데이터 가치화팀은 이 데이터를 모두 BigQuery로 적재해 분석 기반을 마련하고 있으며, 특히 MongoDB는 Spark Connector를 활용해 덤프해 오고 있었어요.</p><p>하지만 서비스가 성장하면서 데이터 규모도 함께 커졌고, 기존 MongoDB 방식으로는 두 가지 요구사항을 동시에 충족하기 어려워졌어요.</p><ul><li>2시간 내 데이터 전달이라는 SLO를 맞추려면 DB 부하가 커지고</li><li>DB 부하를 낮추면 2시간 SLO 안에 데이터를 모두 적재하기 어려운</li></ul><p>이러한 trade-off가 더는 유지하기 어려운 지점에 도달한 거죠. 그래서 여러 방안을 검토한 끝에 저희 팀은 MongoDB CDC를 도입하기로 했어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dfQwJBOo6YiYSvdFcyIb4g.png" /><figcaption>CPU Spike 현상</figcaption></figure><h3>왜 CDC인가?</h3><h4>목표</h4><ul><li>대용량이며 업데이트가 빈번한 특정 테이블의 덤프 방식을 개선해요.</li><li>데이터베이스 CPU 사용률을 60% 이하로 안정화해요.</li><li>기존 덤프 작업이 2시간 SLO 내에 완료되도록 해요.</li></ul><p>이 목표를 가장 효율적으로 달성할 수 있는 접근이 바로 <strong>CDC(Change Data Capture)</strong>였어요.</p><p>CDC는 데이터베이스의 변경 로그(binlog, oplog 등)를 직접 읽어 변경된 데이터를 캡처하는 방식이에요. 테이블에 별도의 컬럼이 없어도 INSERT, UPDATE, DELETE를 모두 감지할 수 있죠. 이렇게 캡처한 변경 데이터를 활용해 최종 테이블을 구성하려면, 전체 스냅샷을 한 번 가져온 뒤의 변경분을 병합하는 구조라, 대량 조회 없이도 최신 상태를 유지할 수 있다는 장점이 있어요.</p><p>물론 “그냥 증분 적재 방식(Incremental Load)으로 하면 되는 거 아냐?”라고 생각할 수 있지만, 증분 적재 방식은 created_at이나 updated_at처럼 변경 시점을 알 수 있는 필드가 있어야만 가능해요. 또 비즈니스 로직 때문에 해당 필드가 갱신되지 않은 채 다른 값만 수정되는 경우도 있어요. 다양한 테이블을 안정적으로 다루려면 스키마 의존성이 낮은 방식이 필요했고, 그 기준에 CDC가 가장 적합했어요.</p><p>저희 팀은 먼저 컬렉션의 데이터 사이즈와 업데이트양을 기준으로 상위 5개 컬렉션을 CDC 전환 대상으로 선정했어요. 이를 통해 전체 데이터를 반복 조회하지 않고도, 안정적으로 데이터를 동기화할 수 있고, DB 부하를 크게 줄이면서 SLO를 충족할 수 있을 것으로 기대했어요.</p><h4>후보 기술 검토</h4><p>먼저, CDC를 구현할 수 있는 여러 기술 스택을 검토했어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*p5zwHPtDXgb2IRGBt7DpRA.png" /><figcaption>Kafka Connect vs Debezium vs Flink</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-B6JSE8YxUtSeuBHV_aNFA.png" /></figure><h3>최종 선택: Flink CDC</h3><p>여러 후보 기술을 검토한 끝에, <strong>Flink CDC</strong>를 선택했어요. 그 이유는 다음과 같아요.</p><h4>1. MongoDB Change Stream 네이티브 지원</h4><p>Flink CDC는 MongoDB의 Change Stream을 네이티브로 지원해요. 그래서 별도의 커넥터 개발 없이 변경 로그를 안정적으로 읽어올 수 있었고, Change Stream의 resume token(마지막으로 처리한 지점을 가리키는 토큰)과 Flink의 checkpoint가 자연스럽게 연동돼 장애가 발생해도 정확한 지점부터 재개할 수 있었어요.</p><p>덕분에 운영 복잡도를 크게 높이지 않으면서 CDC 기반 파이프라인을 구축할 수 있다는 것이 가장 큰 장점이었어요.</p><h4>2. 강력한 상태 관리와 안정적인 체크포인트 메커니즘</h4><p>Flink는 분산 파일시스템(HDFS/GCS/S3 등)에 상태를 주기적으로 스냅샷 형태로 저장하는 체크포인트 전략을 사용해요. 상태를 외부 스토리지에 보존하기 때문에 장애가 발생해도 <strong>정확히 한 번(Exactly-Once)</strong> 처리 보장을 유지한 채 이전 시점부터 안전하게 재시작할 수 있어요.</p><p>이 점이 특히 대규모 파이프라인 운영에서 신뢰도를 높여주는 요소였어요.</p><h4>3. CDC → Transform → Sink까지 이어지는 통합 파이프라인</h4><p>Debezium처럼 CDC에 특화된 도구는 변경 데이터 추출에 특화되어 있지만, 추출 이후의 데이터 정제나 변환을 위해 별도 시스템이 필요해요. 반면 Flink CDC는 하나의 Job 안에서 변환, 적재까지 모두 처리할 수 있어요.</p><ol><li>CDC로 변경 데이터 추출</li><li>필요한 데이터 정제·변환·필터링 수행</li><li>BigQuery 등 Sink로 안정적 적재</li></ol><p>이런 End-to-End 통합 구성 덕분에 파이프라인 수를 줄이고 운영 복잡도를 낮출 수 있었어요. 특히 MongoDB → BigQuery로 가는 구간에서 데이터 모델 재구성이나 형식 변환이 자주 필요했는데, 별도의 변환 레이어 없이 처리할 수 있다는 점이 매력적이었어요.</p><h4>4. 병렬 처리 기반의 확장성</h4><p>Flink는 작업을 연산 단위로 세분화해 병렬로 실행할 수 있어요. TaskManager 수를 늘리면 처리량이 선형에 가깝게 증가하고, CDC 이벤트가 폭증하더라도 파이프라인 전체를 확장해 안정적으로 대응할 수 있어요.</p><p>데이터 증가 속도가 빠른 서비스 환경에서 확장성이 중요한 평가 기준이었어요.</p><h3>구축 과정</h3><h4>아키텍처 설계</h4><p>CDC 데이터 파이프라인의 아키텍처는 비교적 단순하게 설계했어요.</p><p>당장의 DB 부하 문제를 빠르게 해소하는 게 우선이었고, 현재 트래픽 수준에서는 Kafka 같은 중간 큐 없이도 충분히 처리할 수 있다고 판단했기 때문이에요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TDDZkfRH4iStPR369fdyWg.png" /><figcaption>Pipeline Architecture (High-level)</figcaption></figure><p><strong>MongoDB의 CDC 메커니즘</strong></p><p>MongoDB는 모든 write 연산(insert, update, delete)을 기록하는 특별한 로그, <strong>Oplog</strong>을 가지고 있어요. 그리고 이 Oplog를 실시간으로 구독할 수 있는 <strong>Change Stream</strong> 기능을 제공해요. Change Stream은 Oplog를 안전하게 감싸서 애플리케이션이 바로 사용할 수 있는 형태의 변경 이벤트로 전달하는 고수준 API예요.</p><p>Flink CDC는 이 Change Stream을 직접 구독해 변경 이벤트를 받아와요. 전체 데이터 플로우는 다음과 같아요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UId8EReD9SZ6C1cKW_Qm3A.png" /><figcaption>Pipeline Architecture (Detailed)</figcaption></figure><ol><li>MongoDB에서 데이터 변경 발생 (insert, update, delete)</li><li>Change Stream이 변경 이벤트 생성</li><li>Flink CDC가 이벤트를 구독하고 수신</li><li>필요한 데이터 변환 및 가공 처리</li><li>변환된 데이터를 BigQuery로 전송</li><li>Data Lake에서 분석 가능한 상태로 제공</li></ol><p><strong>배치 파이프라인 구조</strong></p><p>뒷단의 배치 파이프라인은 매시간(hourly) 실행되며, 크게 네 단계로 구성돼요.</p><ol><li><strong>Schema Evolution</strong>: 스키마 저장소와 BigQuery 테이블을 비교해서 누락된 필드를 자동 추가</li><li><strong>Extract CUD Latest</strong>: CDC 원본에서 최근 변경 이벤트를 추출하고 중복 제거</li><li><strong>Merge to Raw</strong>: JSON 원본 형태의 raw 테이블에 병합</li><li><strong>Materialize to Final</strong>: 스키마를 적용해 최종 테이블로 구체화</li></ol><p><strong>왜 hourly 배치인가?</strong></p><p>실시간 스트리밍이 아닌 hourly 배치 방식을 선택한 데는 이유가 있어요. 우선 실시간성이 요구되지 않았어요. 2시간 내 delivery라는 SLO만 충족하면 충분했기 때문에, 실시간 파이프라인에 리소스를 쓰기보다 구조를 단순하게 유지하는 쪽을 선택했어요.</p><p>또한 시간 윈도우를 넉넉하게 가져가면 지연 도착한 이벤트까지 안정적으로 회수할 수 있고, 장애 시에도 실패 구간을 명확히 정의해 재처리하기 쉬워요. 배치 단위로 처리하면 멱등성을 보장하기도 수월하고요.</p><p>CDC 기반 아키텍처이기 때문에 향후 실시간 요구사항이 생기더라도 스트리밍 방식으로 확장할 여지도 충분하지만, 현시점에서는 복잡성을 불필요하게 높이지 않고, SLO와 운영 안정성을 동시에 확보하는 방향을 택했어요.</p><p>이 설계에 도달하기까지 네 가지 고민이 있었어요.</p><ol><li><strong>트랜잭션을 빈틈없이 처리할 수 있을까?</strong></li><li><strong>Initial Full Dump는 어떻게 동작시킬 것인가?</strong></li><li><strong>NoSQL to SQL(MongoDB to BigQuery) 변환 과정에서 스키마 진화는 어떻게 대응할까?</strong></li><li><strong>CDC 정합성은 어떤 기준으로 검증해야 서빙 가능한 수준이라고 판단할 수 있을까?</strong></li></ol><p>그럼, 이제부터 하나씩 살펴볼게요.</p><h4>첫 번째 고민: MongoDB 트랜잭션 처리</h4><p>CDC를 도입할 때 가장 먼저 확인해야 했던 건 트랜잭션 순서 보장이었어요. 순서가 보장되지 않으면 CDC의 의미가 없어지기 때문이에요.</p><p>예를 들어볼게요.</p><ul><li>INSERT → UPDATE → DELETE 순서면, 최종 테이블에서 해당 row가 삭제되어야 하고</li><li>INSERT → UPDATE → UPDATE 순서라면, 마지막 UPDATE 상태로 upsert 하면 돼요.</li></ul><p>즉 동일한 primary key에서 발생한 변경 이벤트가 순서대로 수집돼야 최종 테이블에 정확하게 반영할 수 있어요.</p><p>그래서 Change Stream이 이 순서를 보장하는지 PoC를 진행했어요. 확인 결과, Change Stream은 oplog 기반으로 동작하며 각 이벤트에 타임스탬프(ts_ms)가 포함돼 있어 트랜잭션 발생 순서대로 이벤트를 수집할 수 있었어요.</p><p>아래는 트랜잭션에 대한 Oplog 예시에요.</p><pre>// 1. Insert<br>{<br>   &quot;_id&quot;: &quot;1&quot;,<br>   &quot;fullDocument&quot;:&quot;{\\&quot;_id\\&quot;: \\&quot;1\\&quot;, \\&quot;v\\&quot;: \\&quot;A\\&quot;}&quot;,<br>   &quot;operationType&quot;:&quot;insert&quot;,<br>   &quot;ts_ms&quot;:1,<br>   ...<br>}</pre><pre>// 2. Update (v: A → B)<br>{<br>   &quot;_id&quot;: &quot;1&quot;,<br>   &quot;fullDocument&quot;:&quot;{\\&quot;_id\\&quot;: \\&quot;1\\&quot;, \\&quot;v\\&quot;: \\&quot;C\\&quot;}&quot;,<br>   &quot;operationType&quot;:&quot;update&quot;,<br>   &quot;ts_ms&quot;:2,<br>   &quot;updateDescription&quot;:{<br>      &quot;updatedFields&quot;:&quot;{\\&quot;v\\&quot;: \\&quot;B\\&quot;}&quot;<br>   },<br>   ...<br>}</pre><pre>// 3. Update (v: B → C)<br>{<br>   &quot;_id&quot;: &quot;1&quot;,<br>   &quot;fullDocument&quot;:&quot;{\\&quot;_id\\&quot;: \\&quot;1\\&quot;, \\&quot;v\\&quot;: \\&quot;C\\&quot;}&quot;,<br>   &quot;operationType&quot;:&quot;update&quot;,<br>   &quot;ts_ms&quot;:3,<br>   &quot;updateDescription&quot;:{<br>      &quot;updatedFields&quot;:&quot;{\\&quot;v\\&quot;: \\&quot;C\\&quot;}&quot;<br>   },<br>   ...<br>}</pre><p>Change Stream의 fullDocument 옵션을 &quot;updateLookup&quot;으로 설정하면 fullDocument에 전체 문서 데이터가 담겨요. 이때 동일한 트랜잭션 안에서 연속적으로 여러 번 변경이 발생하더라도, 모든 이벤트의 fullDocument가 마지막 상태로 동일하게 전달돼요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1ywefqwNRyMJgG1DHoDj3A.png" /></figure><p>Flink MongoDB CDC 커넥터는 기본값이 true이기 때문에 따로 설정할 필요도 없어요. 결국 primary key 기준으로 마지막 이벤트의 fullDocument 필드만 처리하면 되기 때문에, 트랜잭션 처리가 훨씬 단순해졌어요.</p><h4>두 번째 고민: Initial Full Dump</h4><p>CDC는 파이프라인 가동 이후의 변경 분만 가져올 수 있어요. 따라서 CDC를 시작하기 전에 존재하던 데이터를 한 번은 일괄로 가져오는 Initial Full Dump 과정이 필요해요.</p><p>Flink CDC는 이 과정을 자동화한 Initial Snapshot 모드를 제공하지만, 테스트해 보니, 기본 설정으로는 Oplog 보존 기간 안에 작업을 끝내기 어려웠어요. 리소스를 늘리고 여러 튜닝을 시도했지만, DB 부하만 높아지고 성능은 Spark보다 느린 상황이었어요.</p><p>추가 튜닝으로 개선할 수 있는 여지는 있었겠지만, 제한된 일정 안에서는 익숙한 방식으로 방향을 전환하는 게 현실적이라고 판단했고, 남은 선택지를 다시 검토하게 되었어요.</p><p>그럼, 이제부터 하나씩 살펴볼게요.</p><h4>첫 번째 고민: MongoDB 트랜잭션 처리</h4><p>CDC를 도입할 때 가장 먼저 확인해야 했던 건 트랜잭션 순서 보장이었어요. 순서가 보장되지 않으면 CDC의 의미가 없어지기 때문이에요.</p><p>예를 들어볼게요.</p><ul><li>INSERT → UPDATE → DELETE 순서면, 최종 테이블에서 해당 row가 삭제되어야 하고</li><li>INSERT → UPDATE → UPDATE 순서라면, 마지막 UPDATE 상태로 upsert 하면 돼요.</li></ul><p>즉 동일한 primary key에서 발생한 변경 이벤트가 순서대로 수집돼야 최종 테이블에 정확하게 반영할 수 있어요.</p><p>그래서 Change Stream이 이 순서를 보장하는지 PoC를 진행했어요. 확인 결과, Change Stream은 oplog 기반으로 동작하며 각 이벤트에 타임스탬프(ts_ms)가 포함돼 있어 트랜잭션 발생 순서대로 이벤트를 수집할 수 있었어요.</p><p>아래는 트랜잭션에 대한 Oplog 예시에요.</p><pre>// 1. Insert<br>{<br>   &quot;_id&quot;: &quot;1&quot;,<br>   &quot;fullDocument&quot;:&quot;{\\&quot;_id\\&quot;: \\&quot;1\\&quot;, \\&quot;v\\&quot;: \\&quot;A\\&quot;}&quot;,<br>   &quot;operationType&quot;:&quot;insert&quot;,<br>   &quot;ts_ms&quot;:1,<br>   ...<br>}</pre><pre>// 2. Update (v: A → B)<br>{<br>   &quot;_id&quot;: &quot;1&quot;,<br>   &quot;fullDocument&quot;:&quot;{\\&quot;_id\\&quot;: \\&quot;1\\&quot;, \\&quot;v\\&quot;: \\&quot;C\\&quot;}&quot;,<br>   &quot;operationType&quot;:&quot;update&quot;,<br>   &quot;ts_ms&quot;:2,<br>   &quot;updateDescription&quot;:{<br>      &quot;updatedFields&quot;:&quot;{\\&quot;v\\&quot;: \\&quot;B\\&quot;}&quot;<br>   },<br>   ...<br>}</pre><pre>// 3. Update (v: B → C)<br>{<br>   &quot;_id&quot;: &quot;1&quot;,<br>   &quot;fullDocument&quot;:&quot;{\\&quot;_id\\&quot;: \\&quot;1\\&quot;, \\&quot;v\\&quot;: \\&quot;C\\&quot;}&quot;,<br>   &quot;operationType&quot;:&quot;update&quot;,<br>   &quot;ts_ms&quot;:3,<br>   &quot;updateDescription&quot;:{<br>      &quot;updatedFields&quot;:&quot;{\\&quot;v\\&quot;: \\&quot;C\\&quot;}&quot;<br>   },<br>   ...<br>}</pre><p>Change Stream의 fullDocument 옵션을 &quot;updateLookup&quot;으로 설정하면 fullDocument에 전체 문서 데이터가 담겨요. 이때 동일한 트랜잭션 안에서 연속적으로 여러 번 변경이 발생하더라도, 모든 이벤트의 fullDocument가 마지막 상태로 동일하게 전달돼요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1ywefqwNRyMJgG1DHoDj3A.png" /></figure><p>Flink MongoDB CDC 커넥터는 기본값이 true이기 때문에 따로 설정할 필요도 없어요. 결국 primary key 기준으로 마지막 이벤트의 fullDocument 필드만 처리하면 되기 때문에, 트랜잭션 처리가 훨씬 단순해졌어요.</p><h4>두 번째 고민: Initial Full Dump</h4><p>CDC는 파이프라인 가동 이후의 변경 분만 가져올 수 있어요. 따라서 CDC를 시작하기 전에 존재하던 데이터를 한 번은 일괄로 가져오는 Initial Full Dump 과정이 필요해요.</p><p>Flink CDC는 이 과정을 자동화한 Initial Snapshot 모드를 제공하지만, 테스트해 보니, 기본 설정으로는 Oplog 보존 기간 안에 작업을 끝내기 어려웠어요. 리소스를 늘리고 여러 튜닝을 시도했지만, DB 부하만 높아지고 성능은 Spark보다 느린 상황이었어요.</p><p>추가 튜닝으로 개선할 수 있는 여지는 있었겠지만, 제한된 일정 안에서는 익숙한 방식으로 방향을 전환하는 게 현실적이라고 판단했고, 남은 선택지를 다시 검토하게 되었어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zvdU1h4Dme1vNnVGGG1ZKg.png" /></figure><p>데이터 가치화 팀은 이미 Spark 클러스터를 운영하고 있었고, 여러 방식을 비교해 보니 단순 읽기 작업에서 Spark가 가장 뛰어난 성능을 보여줬어요. 그래서 Initial Full Dump는 Spark Job으로 처리하는 방식으로 어렵지 않게 결정할 수 있었어요.</p><h4>세 번째 고민: Schema Evolution</h4><p>MongoDB CDC를 구축하면서 가장 큰 고민은 바로 스키마 관리였어요.</p><p>MongoDB는 스키마가 자유롭기 때문에 엔지니어가 원하면 언제든 새로운 필드를 추가할 수 있어요. 하지만 BigQuery는 명확한 스키마 정의가 필요해요. 결국 MongoDB에서 스키마가 바뀌었을 때 이를 BigQuery 테이블에도 안정적으로 반영할 수 있어야 했어요.</p><p>이를 해결하기 위해 스키마 기반인 BigQuery에 데이터를 적재하는 방식을 두 가지로 나눠 검토했어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wHuCEMIcikJjNjRj3p0zLw.png" /><figcaption>Static Schema &amp; Full Schema Evolution</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dGY124ij1tSsVTQJ6rj91Q.png" /></figure><p>두 방법 모두 장단점이 있기 때문에 먼저 충족해야 할 요구사항을 정리했어요.</p><p><strong>[데이터 가치화 팀의 요구사항]</strong></p><ul><li>파이프라인이 안정적으로 운영되어야 한다.</li><li>스키마 변경이 예상치 못한 문제(e.g. BigQuery Breaking Change)를 일으키면 안 된다.</li></ul><p><strong>[서비스 팀의 요구사항]</strong></p><ul><li>스키마 변경이 반영되기까지 너무 오래 걸리면 안 된다.</li><li>스키마 변경이 쉽게 이루어져야 한다.</li></ul><p>이 기준으로 판단해 1번 방법(정적 스키마)을 채택했어요. 스키마를 통제 가능한 영역 안에서 관리해야 예상치 못한 문제를 방지할 수 있다고 생각했기 때문이에요. 대신에 “빠르고 쉬운 반영”이라는 서비스 팀의 요구사항도 충족시키기 위해 다음과 같은 구조를 설계했어요.</p><p><strong>1. 스키마 변경 승인 및 반영 자동화</strong></p><p>유저가 스키마 추가를 요청하면, 데이터 가치화팀이 리뷰 후 승인해요. 승인된 변경 사항은 자동으로 감지되어 BigQuery에 새 스키마가 추가돼요. 별도의 수동 배포 과정 없이도 변경이 반영되는 구조예요.</p><p><strong>2. 테이블을 두 스테이지로 분리</strong></p><p>스키마가 추가되더라도 기존 데이터에 해당 필드 값을 채워 넣는 과정이 필요해요. 이를 효율적으로 처리하기 위해 테이블을 두 스테이지로 분리했어요.</p><p><strong>Stage 1: JSON 원본 테이블</strong></p><ul><li>데이터를 JSON 원본 형태 그대로 저장해 스키마 변경과 무관하게 보존</li><li>나중에 스키마가 바뀌어도 Full Dump 없이 재가공 가능</li><li>Primary key 기준으로 클러스터링을 적용해 CDC 변경 분 업데이트 시 조인 쿼리의 스캔 범위를 줄여 연산 비용 최소화</li></ul><p><strong>Stage 2: 최종 테이블</strong></p><ul><li>스키마 저장소에 변경이 생기면, Stage 1의 JSON 원본에 새 스키마를 적용해 최종 테이블을 다시 생성해요.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EL-7T0HXRPKwbwH2bLDSvw.png" /><figcaption>Two-Stage Table Architecture (Raw → Final)</figcaption></figure><p>이 구조를 통해 스키마가 변경되더라도 Stage 1에 원본 데이터가 그대로 남아 있어요. 그래서 Full Dump 없이 Stage 2만 재생성하면 변경 사항이 반영돼요. 기존에는 스키마 반영에 2~3시간이 걸렸지만, 이 방식으로 <strong>20분 이내</strong>로 줄일 수 있었어요.</p><h4>네 번째 고민: 정합성 체크</h4><p>CDC 시스템을 만드는 것보다 더 중요한 건 그 데이터를 신뢰할 수 있느냐였어요.</p><p>Full Dump에서 CDC로 전환할 때, 서빙 중인 테이블을 무중단으로 교체하려면 철저한 정합성 검증이 필요했어요. 그래서 먼저 정합성 기준을 명확히 정의하고, 기존 Full Dump 파이프라인과 CDC 파이프라인을 동시에 운영(dual write)하면서 두 결과를 비교 검증했어요. 이 비교 작업 과정은 알림으로 자동화해 지속적으로 모니터링했어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/996/1*17_Wd3lMe9eQ8-CB_fnDqA.png" /><figcaption>정합성 모니터링 알림 예시</figcaption></figure><p><strong>체크 항목</strong></p><ul><li><strong>레코드 수 일치</strong>: Full 덤프 기준, 데이터 수와 병행 운영(dual write) 중인 CDC 데이터 수가 일치하는가?</li><li><strong>데이터 신선도</strong>: CDC가 시간마다 잘 들어오고 있는가?</li><li><strong>중복 ID 확인</strong>: ID 컬럼에 있어서 중복 데이터가 존재하지 않는가?</li></ul><p>위 항목들은 상시 모니터링 지표로 활용했고, 실제 CDC 전환을 위한 정합성 검증 단계에서는 더 엄격한 기준을 적용했어요. 같은 ID에 대해 <strong>모든 필드가 동일한 값을 갖는지</strong> checksum으로 비교해서 100% 정합한 데이터인지 확인했어요.</p><p>최종적으로 이러한 정합성이 2주 동안 문제없이 유지되는지를 확인했고, 그 결과를 바탕으로 마이그레이션 가능 여부를 판단했어요.</p><h3>실전 운영: 경험에서 배우다</h3><p>시스템을 만들고 배포하는 건 시작일 뿐이고, 진짜 중요한 건 <strong>안정적으로 운영하는 것</strong>이에요.</p><h4>모니터링 체계</h4><p>CDC 시스템의 상태를 확인하기 위해 다음 지표를 모니터링했어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*un2VzE913WeyK4yWrPgxxw.png" /><figcaption>Flink MongoDB CDC 그라파나 모니터링 대시보드</figcaption></figure><p><strong>핵심 지표</strong></p><ul><li><strong>Flink Job 상태<br></strong>: Job이 정상적으로 실행 중인지, TaskManager가 충분히 붙어 있는지, 재시작이나 실패는 없는지 확인해요.</li><li><strong>데이터 처리량(Throughput)<br></strong>: MongoDB → Flink → BigQuery로 이어지는 전체 파이프라인에서 초당 얼마나 많은 레코드·바이트를 처리하고 있는지 모니터링해요.</li><li><strong>MongoDB 읽기 부하<br></strong>: CDC로 인해 MongoDB에 과도한 부하가 발생하지 않는지 CPU·메모리·네트워크 사용량을 확인해요.</li><li><strong>BigQuery 적재 성공률<br></strong>: Sink 단계에서 레코드가 정상적으로 적재되고 있는지, 에러가 발생하지 않는지 확인해요.</li><li><strong>Backpressure<br></strong>: 파이프라인 어느 구간에서 병목이 발생하는지, 처리 지연이 생기지 않는지 확인해요.</li><li><strong>Checkpoint 안정성<br></strong>: Flink의 체크포인트가 안정적으로 완료되는지, 소요 시간이나 크기가 비정상적으로 증가하지 않는지를 확인해요.</li></ul><p>운영하면서 실제 장애 상황을 몇 차례 경험했고, 이러한 경험을 바탕으로 특히 이 지표 중 Flink Job 상태와 Backpressure에 있어서는 SLO(Service Level Objective)를 위반하면 자동으로 알림이 발송되도록 설정했어요.</p><h4>Fault Tolerance</h4><p>운영하면서 여러 장애 시나리오를 경험했고, 각 시나리오에서 Flink는 이렇게 동작했어요.</p><p><strong>시나리오 1: Flink Job 실패</strong> Flink Kubernetes Operator가 자동으로 Job을 재시작하고, 마지막 checkpoint부터 처리를 재개해요. 보통 3분 이내에 복구됐어요.</p><p><strong>시나리오 2: MongoDB 연결 끊김</strong> Exponential backoff로 재연결을 시도해요. 일시적인 네트워크 불안정은 자동으로 복구되고, 문제가 10분 이상 지속되면 알림이 발송돼요.</p><p><strong>시나리오 3: BigQuery 적재 실패</strong> 실패한 데이터는 Flink 애플리케이션에 정의해둔 Dead Letter Queue를 통해 BigQuery에 저장되고, 1시간 이내 알림이 발송돼요. 문제를 해결한 뒤 수동으로 재처리해요.</p><h3>마무리</h3><p>이번 프로젝트를 진행하며 데이터 파이프라인에서 가장 중요한 건 결국 “신뢰”라는 걸 다시 한번 느꼈어요. 충분한 PoC와 정합성 검증에 시간을 투자한 것이 구성원들의 신뢰를 얻는 데 가장 큰 역할을 했고, 최신 기술보다 우리 상황에 맞는 기술을 선택한 것도 좋은 판단이었어요.</p><p>앞으로는 더 빠르고 안정적인 데이터 전달을 위해 다음 목표를 가지고 있어요.</p><ul><li>주어진 자원 안에서 End to End 소요 시간을 최소화한다.</li><li>리소스를 효율적으로 활용해 비용 대비 처리량을 높인다.</li></ul><p>오늘은 MongoDB CDC에 관해서만 소개해 드렸지만, 데이터 딜리버리 인프라의 안정성을 갖추기 위해 보이지 않는 곳에서도 여전히 많은 작업이 이루어지고 있어요.</p><ul><li><a href="https://medium.com/daangn/100-%ED%8C%80%EC%9B%90%EC%9D%98-%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EC%97%90-%EC%98%81%ED%96%A5%EC%9D%84-%EC%A3%BC%EB%8A%94-data-scientist-decision-5c939e8a3ea9"><strong>100+ 팀원의 의사결정에 영향을 주는 Data Scientist, Decision : 실험플랫폼</strong></a></li><li><a href="https://medium.com/daangn/%EC%8B%A0%EB%A2%B0%ED%95%A0-%EC%88%98-%EC%9E%88%EB%8A%94-%EC%A7%80%ED%91%9C-%EB%A7%8C%EB%93%A4%EA%B8%B0-bbf39dc4a6b3"><strong>신뢰할 수 있는 지표 만들기</strong></a></li><li><a href="https://medium.com/daangn/%EC%8B%A0%EB%A2%B0%ED%95%A0-%EC%88%98-%EC%9E%88%EB%8A%94-%EC%A7%80%ED%91%9C-%EB%A7%8C%EB%93%A4%EA%B8%B0-2-karrotmetrics-%EA%B8%B0%EC%88%A0%ED%8E%B8-526f517fda29"><strong>신뢰할 수 있는 지표 만들기 2 : KarrotMetrics 기술편</strong></a></li></ul><p>데이터 파이프라인과 인프라 구축에 관심 있는 분들이 계신다면, 당근 데이터 가치화 팀에 합류하여 2,000만 사용자를 위한 데이터 인프라를 함께 만들어가요!</p><p><strong>👉 당근 데이터 가치화 팀 채용 공고 바로가기</strong></p><ul><li><a href="https://team.daangn.com/jobs/4300801003/"><strong>Software Engineer, Data</strong></a></li><li><a href="https://team.daangn.com/jobs/7507320003/"><strong>Data Analytics Engineer</strong></a></li></ul><h3>참조</h3><ul><li><a href="https://nightlies.apache.org/flink/flink-cdc-docs-release-3.1/docs/connectors/flink-sources/mongodb-cdc/">https://nightlies.apache.org/flink/flink-cdc-docs-release-3.1/docs/connectors/flink-sources/mongodb-cdc/</a></li><li><a href="https://www.mongodb.com/docs/manual/changestreams/">https://www.mongodb.com/docs/manual/changestreams/</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=302ae8a0dc23" width="1" height="1" alt=""><hr><p><a href="https://medium.com/daangn/%EB%A7%A4%EB%B2%88-%EB%8B%A4-%ED%8D%BC%EC%98%AC-%ED%95%84%EC%9A%94-%EC%97%86%EC%9E%96%EC%95%84-%EB%8B%B9%EA%B7%BC%EC%9D%98-mongodb-cdc-%EA%B5%AC%EC%B6%95%EA%B8%B0-302ae8a0dc23">매번 다 퍼올 필요 없잖아? 당근의 MongoDB CDC 구축기</a> was originally published in <a href="https://medium.com/daangn">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Mapping Karrot’s Data: How We Built Column-Level Lineage]]></title>
            <link>https://medium.com/daangn/mapping-karrots-data-how-we-built-column-level-lineage-5914d67a554c?source=rss----4505f82a2dbd---4</link>
            <guid isPermaLink="false">https://medium.com/p/5914d67a554c</guid>
            <category><![CDATA[data-governance]]></category>
            <category><![CDATA[english]]></category>
            <category><![CDATA[data-lineage]]></category>
            <category><![CDATA[karrot]]></category>
            <category><![CDATA[big-data]]></category>
            <dc:creator><![CDATA[Jin-won Park]]></dc:creator>
            <pubDate>Fri, 05 Dec 2025 03:25:14 GMT</pubDate>
            <atom:updated>2025-12-10T01:40:17.361Z</atom:updated>
            <content:encoded><![CDATA[<h3>Building Column-Level Data Lineage with SQL Parsing</h3><p>Our team is the central data organization at Karrot, and we focus on building a data environment that people across the company can trust and actually use.</p><p>Recently, we kicked off a journey to improve data reliability and transparency.</p><p>In this post, We would like to share the first big outcome from that effort: how we built <strong>column-level data lineage using SQL parsing</strong>.</p><p><strong>This post is for you if you are</strong></p><ul><li>A data engineer or analyst running data pipelines on cloud warehouses like BigQuery</li><li>A data platform / governance owner who wants to centrally manage SQL queries written across many teams</li><li>Someone who already has table-level lineage, but is wondering how to extend it to <strong>column-level</strong></li></ul><h3>The problem: when you can’t see how data flows</h3><p>At Karrot, many different teams create derived tables in BigQuery. Data engineers, analysts, product managers —almost everyone transform data into the shape they need.</p><p>But we had a major problem. It was very hard to understand</p><ul><li>‘Which tables affect which other tables?’</li><li>‘Which upstream sources does this table ultimately depend on?’</li></ul><p>Because the overall flow of data wasn’t visible, continuing work or troubleshooting issues often became painful.</p><p>Here are a couple of real incidents we experienced.</p><p><strong>Case 1: The nightmare of cascading failures</strong></p><p>One morning, a flood of incident alerts started pouring into Slack.</p><p>A single table’s pipeline had failed —</p><p>but that table was referenced by many other tables, and they all started failing in a chain reaction.</p><p>The bigger issue?</p><p>We had no quick way to see which tables were impacted.</p><p>So we had to manually dig through queries one by one to find dependencies.</p><p>It took hours to fully understand the blast radius and fix the issue.</p><p><strong>Case 2: The ripple effect of a MySQL schema change</strong></p><p>We saw something similar with MySQL schema changes.</p><p>When we wanted to drop a single column from MySQL,</p><p>we had no reliable way to answer, <em>“Which tables and columns will break if we delete this?”</em></p><p>Just figuring out the impact took a lot of time.</p><p>After going through these experiences, it became clear. We needed a system that could reliably trace and manage how data flows.</p><blockquote><em>That system is </em><strong><em>data lineage</em></strong></blockquote><h4>Why column-level lineage?</h4><p>Once we decided to build lineage, the next question was:</p><blockquote><em>“How granular should we go?”</em></blockquote><p>Should we only track <strong>table-level</strong> lineage or should we go all the way to <strong>column-level</strong>?</p><h4>The limits of table-level lineage</h4><p>Table-level lineage was relatively easy to build.</p><p>BigQuery provides a JOBS View that includes referenced tables and destination tables.</p><p>You can use that to infer dependencies between tables.</p><p>But in actual day-to-day work, it wasn’t enough.</p><p>Knowing only that “Table A references Table B” doesn’t tell you:</p><ul><li>Which specific columns in downstream tables are affected</li><li>when you delete or change one column</li><li>How sensitive fields like PII (Personally Identifiable Information) propagate through your system</li></ul><p>In other words, table-level lineage alone was too coarse-grained.</p><h4>The value of column-level lineage</h4><p>Once you track lineage at the column level, you can solve problems like:</p><p><strong>(1) Fine-grained impact analysis</strong></p><p>For example, if you modify users.email, you can immediately see how it affects downstream columns like:</p><ul><li>user_stats.email</li><li>marketing.user_email</li></ul><p><strong>(2) Data security &amp; PII tracking</strong></p><p>You can follow where a PII column originates and where it flows across the warehouse.</p><p><strong>(3) Faster root-cause analysis</strong></p><p>When there’s a data quality issue in a downstream column,</p><p>you can quickly trace it back to the source column that caused it.</p><p>The remaining question was:</p><blockquote><em>How can we extract such detailed column-level lineage in a reliable way?</em></blockquote><p>That’s where things started to get interesting.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*v1Dnu8rJXd60RsyxpyMFnw.png" /></figure><h3>Evaluating approaches for lineage extraction</h3><p>To implement column-level lineage, we considered several approaches.</p><p>We primarily cared about two things:</p><ul><li>That lineage could be extracted <strong>accurately at the column level</strong>, and</li><li>That the approach could <strong>parse queries from every team at Karrot</strong>, regardless of variation in style</li></ul><p>Here are the main options we looked at.</p><h4>1) BigQuery INFORMATION_SCHEMA-based approach</h4><p>This was the first idea we considered, but it had two major limitations:</p><p><strong>(1) No column-level tracking</strong></p><p>BigQuery’s built-in features don’t provide the fine-grained column-level dependencies we needed.</p><p><strong>(2) No view-level lineage</strong></p><p>In the BigQuery JOBS metadata, views don’t appear as referenced objects.</p><p>Even if a query selects from a view, the referenced tables recorded are the base tables behind the view.</p><p>This makes it difficult to accurately understand pipelines built on top of views.</p><p>So we couldn’t get what we needed solely from BigQuery metadata.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nQKUwUM2I-0PH5VyJqsUzA.png" /></figure><h4>2) Using an open-source framework like OpenLineage</h4><p>At Karrot, teams execute queries in many different ways:</p><ul><li>Airflow</li><li>Cron jobs</li><li>Notebooks</li><li>Internal batch systems</li><li>…and more</li></ul><p>To use OpenLineage properly, we would need to:</p><ul><li>Install a client library or plugin in every execution environment</li><li>Instrument each job to send lineage events along with execution</li></ul><p>From a central data team’s perspective, this was challenges:</p><ul><li>We’d have to ask many teams to add new dependencies to their workflows</li><li>We’d risk interfering with each team’s specific business logic and deployment patterns</li><li>Every time a new pipeline appeared, we’d need to check whether it was correctly instrumented</li></ul><p>OpenLineage is a powerful standard with a rich feature set.</p><p>But considering Karrot’s environment, we had some concerns:</p><ul><li>The cost of integrating instrumentation across heterogeneous environments</li><li>The fact that our immediate need was column-level parsing, not full end-to-end orchestration</li></ul><p>So instead of adopting OpenLineage as-is, we decided to look for a more lightweight approach better suited for our situation.</p><h3>Our choice: SQL parsing</h3><p>Karrot’s main data warehouse is <strong>BigQuery</strong>, and almost all DW-related workloads run there.</p><p>So rather than instrumenting every executor, we chose a different strategy:</p><blockquote><em>Parse the query logs left in BigQuery itself.</em></blockquote><p>In other words, instead of collecting events from each runtime,</p><p>we would pull in all queries from BigQuery and extract lineage from the SQL directly.</p><p>This was simpler and more robust in our environment.</p><p>Here’s how we designed and implemented this approach.</p><h4>What do we parse?</h4><p>Most data work at Karrot is done in BigQuery using SQL.</p><p>BigQuery stores all executed queries in INFORMATION_SCHEMA.JOBS.</p><p>That table/view includes fields like:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*z-qNJ6U1bx7-2EMzRcSOZA.png" /></figure><p>We use the query text from this metadata as the input for our lineage extraction.</p><h4>Tech stack</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sWLlFOyylGNl-zquVy5CPw.png" /></figure><p>We chose the following stack:</p><ul><li><strong>sqlglot: </strong>SQL parsing library that supports many dialects and exposes an AST (Abstract Syntax Tree) for query analysis.</li><li><strong>Spark: </strong>Used to parse large volumes of queries in parallel and generate lineage.</li><li><strong>Airflow: </strong>Schedules lineage extraction and updates periodically.</li><li><strong>BigQuery: </strong>Stores and serves the extracted lineage data.</li></ul><h4>Lineage extraction process</h4><p>The overall process looks like this:</p><p><strong>(1) Collect queries</strong></p><p>Fetch recently executed queries from INFORMATION_SCHEMA.JOBS for all GCP projects at Karrot.</p><p><strong>(2) Parse SQL</strong></p><p>Use sqlglot to parse each query and build an AST.</p><p><strong>(3) Extract dependencies</strong></p><p>Walk the AST to extract dependencies between tables and columns.</p><p><strong>(4) Store data</strong></p><p>Save the extracted lineage into data_catalog.lineage in BigQuery.</p><p>Suppose we have the following query:</p><pre>CREATE OR REPLACE TABLE `project.dataset.user_stats` AS<br>SELECT<br>  u.user_id,<br>  u.name,<br>  COUNT(o.order_id) AS order_count<br>FROM `project.dataset.users` u<br>LEFT JOIN `project.dataset.orders` o<br>  ON u.user_id = o.user_id<br>GROUP BY u.user_id, u.name</pre><p>Using sqlglot, we can extract column-level dependencies like this (simplified):</p><pre># 의존 관계 예시 (단순화된 버전)<br>lineage = [<br>  {<br>    &quot;destination_table&quot;: &quot;project.dataset.user_stats&quot;,<br>    &quot;destination_column&quot;: &quot;user_id&quot;,<br>    &quot;reference_table&quot;: &quot;project.dataset.users&quot;,<br>    &quot;reference_column&quot;: &quot;user_id&quot;<br>  },<br>  {<br>    &quot;destination_table&quot;: &quot;project.dataset.user_stats&quot;,<br>    &quot;destination_column&quot;: &quot;name&quot;,<br>    &quot;reference_table&quot;: &quot;project.dataset.users&quot;,<br>    &quot;reference_column&quot;: &quot;name&quot;<br>  },<br>  {<br>    &quot;destination_table&quot;: &quot;project.dataset.user_stats&quot;,<br>    &quot;destination_column&quot;: &quot;order_count&quot;,<br>    &quot;reference_table&quot;: &quot;project.dataset.orders&quot;,<br>    &quot;reference_column&quot;: &quot;order_id&quot;<br>  }<br>]</pre><h4>The complexity of SQL parsing</h4><p>Once we moved from prototype to production, things got a lot more complicated.</p><p>Here are a few of the tricky parts we had to handle.</p><p><strong>1) Handling CTEs (Common Table Expressions)</strong></p><p>We needed to track dependencies involving temporary tables defined with WITH.</p><pre>WITH temp_users AS (<br>  SELECT user_id, name FROM users WHERE status = &#39;active&#39;<br>),<br>user_orders AS (<br>  SELECT<br>    t.user_id,<br>    t.name,<br>    o.order_id<br>  FROM temp_users t<br>  JOIN orders o ON t.user_id = o.user_id<br>)<br>SELECT * FROM user_orders</pre><p>In this case, we need to:</p><ul><li>Track dependencies between CTEs (temp_users → user_orders)</li><li>And from those CTEs through to the final output</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*njAa4uKCKeE2bfH0GMOh4Q.png" /></figure><p><strong>2) Subqueries and alias handling</strong></p><p>Nested subqueries with aliases make it harder to trace where a column came from.</p><pre>SELECT<br>  a.user_id,<br>  b.total<br>FROM (<br>  SELECT user_id FROM users<br>) a<br>JOIN (<br>  SELECT user_id, SUM(amount) as total FROM orders GROUP BY user_id<br>) b ON a.user_id = b.user_id</pre><p>To understand lineage correctly, we need to:</p><ul><li>Resolve aliases (a, b)</li><li>Track how columns are passed through subqueries</li><li>Follow transformations like SUM(amount) AS total</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RAyFC9h9KQwmXyXcDAtFFw.png" /></figure><h4>Our approach to these problems</h4><p>To solve these issues, we process CTEs and subqueries step by step, tracking column origins at each stage instead of trying to interpret the entire query in one shot.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*W7L0tFhRJnV38a9X15_LuA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0z9YaN1Mb9spcb4EPTAagQ.png" /></figure><p>We don’t try to be perfect from day one. Instead, we gradually expand coverage as we encounter new query patterns.</p><h3>Data model design</h3><p>Extracting lineage is only half the story.</p><p>Since many people want to use this information for different purposes,</p><p>how we store and serve it is equally important.</p><p>Initially, we stored all parsed lineage in a single table.</p><p>But as usage grew and query patterns diversified, that single-table model could no longer satisfy everyone efficiently.</p><p>So we redesigned the data model to make it easier to query and operate.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*lTdJFIDxTEptquIqpR5dig.png" /></figure><h4>1) Raw table: preserving everything as-is</h4><p>First, we keep a raw table where we store the lineage results exactly as they were parsed from INFORMATION_SCHEMA.JOBS.</p><p>This table includes everything, even things like cache or temporary tables that we might filter out later.</p><p>Having this raw table gives us important benefits:</p><ul><li>If we change our filtering rules</li><li>If we want to create new views</li><li>If we need to deep-dive into specific edge cases</li></ul><p>…we can always regenerate derived data based on the full history.</p><h4>2) Purpose-specific views</h4><p>On top of the raw table, we created several <strong>views</strong>, each tailored to a specific usage pattern.</p><p>This made it much easier for internal users to get exactly what they needed without writing complex queries.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Z6eT5ZJ6PxyzcCwBdhjEhg.png" /></figure><h4>Why split it like this?</h4><p>Lineage data has many different use cases:</p><ul><li>For building graphs, you just need simple source → target edges</li><li>For analyzing failures or schema changes, you care about who uses what</li><li>For PII tracking or detailed impact analysis, you need fine-grained column-level lineage</li></ul><p>Trying to satisfy all of these with a single table leads to:</p><ul><li>Overly complex queries</li><li>Scanning unnecessary data</li><li>Hard-to-maintain schemas</li></ul><p>So we took this approach:</p><blockquote><em>Keep the </em><strong><em>raw </em></strong><em>and layer </em><strong><em>thin, purpose-specific views</em></strong></blockquote><p>This makes it much easier for different teams to use lineage as if it were a <strong>simple queryable API</strong>.</p><h3>How we use lineage internally</h3><p>Storing lineage nicely is good,</p><p>but making it <strong>easy to use in workflows</strong> is even more important.</p><p>To that end, we built an MCP (Model Context Protocol) server so that LLMs can query lineage directly.</p><h4>MCP Server</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kfcQ3LzFTjs2h3K34hW0Rw.png" /></figure><p>Our lineage MCP server does two main things:</p><ol><li><strong>Returns lineage information</strong> for given tables/columns</li><li><strong>Returns user information per table</strong>, such as who has been reading/writing it</li></ol><p>Everything after that — reasoning, decisions, messaging — is delegated to the LLM.</p><p>Currently, the MCP server uses queries from the <strong>last 30 days</strong> as its base.</p><p>Why 30 days?</p><ul><li>Our longest regular internal batch schedule is <strong>monthly</strong></li><li>Queries older than that are more likely to be ad-hoc / one-off jobs</li><li>For now, we’ve chosen to exclude those from usage-based logic</li></ul><h4>Example scenarios</h4><p>Here are some ways we actually use column-level lineage in daily work.</p><p><strong>Scenario 1: Impact analysis when modifying tables (via LLM platform)</strong></p><blockquote><em>“I want to change this table, but I don’t know who’s using it.”</em></blockquote><blockquote><em>“I want to understand the impact first.”</em></blockquote><p>In this scenario, the system:</p><ul><li>Looks up lineage for the table (and relevant columns)</li><li>Identifies downstream tables and their owners/consumers</li><li>Surfaces this info so the person making the change can proactively notify affected teams</li></ul><p>This reduces risk and avoids surprise breakages.</p><p><strong>Scenario 2: Tracing issues when data quality alerts fire (via Slack)</strong></p><blockquote><em>“A data quality test failed for this table. I need to understand the impact and notify relevant people.”</em></blockquote><p>When a data quality or other internal data system alert triggers, we send a Slack message that includes lineage information.</p><p>With that, the on-call person can:</p><ul><li>Quickly assess whether the issue is critical</li><li>See who recently used the table</li><li>Notify likely impacted stakeholders right away</li></ul><h4>Outcomes</h4><p>After about two months of development, here’s what we achieved:</p><p><strong>(1) Automated lineage extraction</strong></p><p>Every day, we automatically analyze the previous day’s queries and update lineage.</p><p><strong>(2) Table and column-level tracking</strong></p><p>We currently track:</p><ul><li>~15,000 tables per day</li><li>~800,000 column-level dependencies per day</li></ul><p><strong>(3) Fast retrieval</strong></p><p>Through the MCP server, lineage queries return in a few seconds.</p><p><strong>(4) Improved data reliability</strong></p><p>Because we can analyze impact before making changes, data operations are safer and more predictable.</p><h3>What we learned</h3><p>We learned a lot during this project. Here are a few key takeaways.</p><p><strong>(1) SQL parsing is more complex than it looks</strong></p><p>At first, we thought:</p><blockquote><em>“How hard can SQL parsing be?”</em></blockquote><p>In reality, the variety of query patterns was enormous.</p><p>CTEs, subqueries, window functions, nested transformations…</p><p>Each category required careful handling, and supporting them properly took a lot of time.</p><p>Going forward, our plan is <strong>not</strong> to cover everything at once, but to:</p><ul><li>Start by covering the most common patterns</li><li>Gradually expand coverage as we encounter new patterns in the wild</li></ul><p><strong>(2) Claude Code helped with fast prototyping</strong></p><p>Claude Code was extremely helpful when building the MCP server.</p><p>Whenever we had to:</p><ul><li>Write complex recursive queries</li><li>Interact with the BigQuery API</li><li>Iterate on tricky logic</li></ul><p>We could rely on Claude to quickly prototype and refine our approach.</p><p><strong>(3) Data governance works best with incremental improvement</strong></p><p>Instead of trying to build a perfect, all-encompassing system from day one, we took a different approach:</p><blockquote><em>Ship something small but useful, get it into real-world use, then evolve it based on feedback.</em></blockquote><p>Once we got to the point where “it works and people can use it”,</p><p>we used real usage and feedback to:</p><ul><li>Expand coverage</li><li>Add new usage scenarios</li><li>Harden the system</li></ul><p>That incremental, feedback-driven approach ultimately made the system more robust.</p><h3>Wrapping up</h3><p>Data governance isn’t about building a flawless system in one shot.</p><p>It’s a journey of continuous improvement.</p><p>Our column-level lineage system is just a small first step,</p><p>but it’s been a meaningful one in strengthening data trust at Karrot.</p><p>Looking ahead, we’re planning:</p><p><strong>(1) Near real-time lineage updates</strong></p><p>Currently, lineage is updated daily.</p><p>We aim to get closer to real-time.</p><p><strong>(2) Visualization tools</strong></p><p>We’re preparing a graph UI that helps people intuitively explore complex dependencies.</p><p>At Karrot, we’re constantly working to make better decisions and build better products using data.</p><p>Within that mission, the Data Team plays the role of a central data org that:</p><ul><li>Tries small experiments quickly</li><li>But solves problems in a robust, durable way</li></ul><p>Personally, I joined Karrot because I wanted to work on exactly these kinds of problems.</p><p>Seeing small insights scale up to shape teams and the wider organization has been very rewarding.</p><p>Thanks for reading all the way through.</p><p>If you have any questions, feel free to reach out anytime.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5914d67a554c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/daangn/mapping-karrots-data-how-we-built-column-level-lineage-5914d67a554c">Mapping Karrot’s Data: How We Built Column-Level Lineage</a> was originally published in <a href="https://medium.com/daangn">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[당근 데이터 지도를 그리다: 컬럼 레벨 리니지 구축기]]></title>
            <link>https://medium.com/daangn/%EB%8B%B9%EA%B7%BC-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A7%80%EB%8F%84%EB%A5%BC-%EA%B7%B8%EB%A6%AC%EB%8B%A4-%EC%BB%AC%EB%9F%BC-%EB%A0%88%EB%B2%A8-%EB%A6%AC%EB%8B%88%EC%A7%80-%EA%B5%AC%EC%B6%95%EA%B8%B0-15cd862c7743?source=rss----4505f82a2dbd---4</link>
            <guid isPermaLink="false">https://medium.com/p/15cd862c7743</guid>
            <category><![CDATA[data-lineage]]></category>
            <category><![CDATA[data-governance]]></category>
            <category><![CDATA[bigquery]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[big-data]]></category>
            <dc:creator><![CDATA[Jin-won Park]]></dc:creator>
            <pubDate>Thu, 04 Dec 2025 02:37:58 GMT</pubDate>
            <atom:updated>2025-12-10T01:40:53.933Z</atom:updated>
            <content:encoded><![CDATA[<h3>SQL 파싱으로 구축한 컬럼 레벨 데이터 리니지</h3><p>안녕하세요, 당근 데이터 가치화팀에서 Data Governance를 담당하고 있는 세이건이에요. 데이터 가치화팀은 중앙 데이터 조직으로서, 당근의 구성원들이 더 믿고 활용할 수 있는 데이터 환경을 만드는 일을 하고 있어요. 그래서 최근에는 데이터의 신뢰성과 투명성을 높이기 위한 여정을 시작했고, 이번 글에서는 그 첫 번째 결과물인 컬럼 레벨 데이터 리니지를 어떻게 구축했는지 공유하려고 해요.</p><p>이 글은 이런 분들께 도움이 될 거예요.</p><ul><li>BigQuery 같은 클라우드 환경에서 데이터 파이프라인을 운영하는 <strong>데이터 엔지니어와 분석가</strong></li><li>조직마다 제각각 작성되는 SQL 쿼리를 중앙에서 관리하고 싶은 <strong>데이터 플랫폼·거버넌스 담당자</strong></li><li>테이블 레벨 리니지는 갖추었지만, <strong>컬럼 레벨까지 확장</strong>할 방법을 고민하고 있는 분</li></ul><h3>문제: 데이터의 흐름이 보이지 않아 벌어졌던 일들</h3><p>먼저 왜 이 프로젝트를 시작했는지부터 설명해볼게요. 당근에서는 여러 조직이 BigQuery를 통해 다양한 파생 테이블을 생성하고 있어요. 데이터 엔지니어, 분석가, 프로덕트 매니저 등 많은 분들이 자신의 업무에 필요한 데이터를 가공하고 있죠.</p><p>하지만 ‘<strong>어떤</strong> <strong>테이블이 어떤 데이터에 영향을 주는지’</strong> 또는 ‘<strong>또 어떤</strong> <strong>테이블이 어떤 원천 데이터를 기반으로 만들어졌는지’</strong>를 파악하기가 너무 어렵단 문제가 있었어요. 이런 흐름이 보이지 않다 보니, 작업을 이어가거나 문제를 해결할 때마다 불편함이 생겼어요.</p><p>실제로 겪었던 사례를 공유해볼게요.</p><p><strong>사례 1: 연쇄 실패의 악몽</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Fe6A6mvtDxk65yNJHXlS9g.png" /></figure><p>어느 날 오전, Slack에 장애 알림이 쏟아지기 시작했어요. 한 테이블의 파이프라인이 실패했는데, 그 테이블을 참조하는 다른 테이블들까지 줄줄이 실패한 거예요.</p><p>그런데 여기서 더 큰 문제는 <strong>어떤 테이블들이 영향을 받았는지</strong> 빠르게 파악할 방법이 없었다는 점이에요. 결국 하나하나 쿼리를 뒤져가며 의존 관계를 찾아야 했고, 문제 해결까지 몇 시간이 걸렸어요.</p><p><strong>사례 2: MySQL 스키마 변경의 파장</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5dyizSJrLSttJ9hNSL-jUA.png" /></figure><p>MySQL에서 컬럼 하나를 삭제하려고할 때도 비슷한 어려움이 또 있었어요. 해당 컬럼을 삭제시에 어떤 테이블과 컬럼이 영향을 받는지 사전에 파악할 방법이 없었죠. 그래서 영향 범위를 파악하는 데만 많은 시간이 소요되었어요.</p><p>이런 경험들을 겪으며 데이터의 흐름을 확실히 추적하고 사전 관리할 수 있는 시스템이 필요하다고 느꼈어요. 그게 바로 <strong>데이터 리니지</strong>였어요.</p><h3>왜 컬럼 레벨까지 필요한가?</h3><p>데이터 리니지를 구축하기로 결정했을 때, 가장 먼저 고민했던 것은 ‘<strong>얼마나 세밀하게 추적해야 하나?’</strong>였어요. 테이블 레벨만 추적할지, 아니면 컬럼 레벨까지 추적할지 말이에요.</p><h4>테이블 레벨의 한계</h4><p>테이블 레벨 리니지는 비교적 쉽게 구축할 수 있었어요. BigQuery에는 JOBS View라는 것이 존재하고, 여기에는 참조 테이블과 목적지 테이블이 정의되어 있어요. 따라서 이를 조회하면 테이블 간 의존 관계는 쉽게 파악할 수 있죠.</p><p>하지만 실무에서는 이 정보만으로는 부족했어요. 테이블 A가 테이블 B를 참조한다는 사실만으로는 특정 컬럼 하나를 삭제하거나 타입을 바꿀 때 ‘<strong>어떤 하위 테이블의 어떤 컬럼이 영향을 받는지’</strong>, 또 <strong>‘PII</strong> <strong>(Personally Identifiable Information)</strong> <strong>같은 민감한 컬럼이 어디까지 흘러가는지’</strong>까지는 알기 어렵기 때문이에요.</p><h4>컬럼 레벨 리니지의 가치</h4><p>컬럼 레벨까지 추적하면 다음과 같은 문제들을 해결할 수 있어요.</p><ol><li><strong>정교한 영향도 분석</strong>: 예를 들어 users.email 컬럼을 수정하면, downstream에서 이 컬럼을 참조하는 user_stats.email, marketing.user_email 같은 컬럼이 어떻게 연쇄적으로 영향받는지 바로 알 수 있어요.</li><li><strong>데이터 보안</strong>: PII 데이터가 어디서 시작해서 어디까지 흘러가는지 추적할 수 있어요.</li><li><strong>더 빠른 문제 해결</strong>: 데이터 품질 이슈가 발생했을 때, 어떤 원천 컬럼의 문제인지 빠르게 찾을 수 있어요.</li></ol><p>문제는 이렇게까지 세밀한 컬럼 레벨 리니지를 어떤 방식으로 안정적으로 뽑아낼 수 있을까 하는 점이었어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fV9Y3OIH5ccJ01Bf6lzXmg.png" /></figure><h3>리니지 추출 방식에 대한 고민</h3><p>그래서 컬럼 레벨 리니지를 구현하기 위해 몇 가지 방식을 두고 검토하기 시작했어요.</p><p>당시 저희 팀은 그 과정에서 다음 두 가지를 특히 중요하게 생각했어요.</p><ul><li>컬럼 레벨 단위로 정확하게 추출할 수 있을 것</li><li>당근의 모든 조직에서 제출하는 쿼리를 파싱할 수 있을 것</li></ul><h4>1) BigQuery INFORMATION_SCHEMA 기반</h4><p>가장 먼저 고려한 방법이었지만, 두 가지 이유로 한계가 있었어요.</p><ul><li><strong>컬럼 레벨 추적 불가:</strong> 컬럼 레벨의 정교한 영향도 분석이 필요했지만, BigQuery 내부 기능만으로는 컬럼 레벨 리니지를 추출할 수 없었어요.</li><li><strong>View 단위 리니지 파악 불가</strong>: BigQuery JOBS에서는 View는 참조 테이블로 나타나지 않아요. 실제로 View를 조회하였더라도 이를 구성하는 기저 테이블이 참조 테이블로 나와요.그래서 View 기반 파이프라인의 흐름을 정확하게 파악하기 어려웠어요.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nQKUwUM2I-0PH5VyJqsUzA.png" /></figure><h4>2) OpenLineage 같은 오픈소스 활용</h4><p>당근에서는 여러 조직이 Airflow, cron, 노트북, 내부 배치 등 각자의 방식으로 쿼리를 제출하고 있어요. OpenLineage를 활용하려면 이 실행 경로마다 클라이언트 라이브러리나 플러그인을 설치하고, 작업 실행 시 리니지 이벤트를 함께 전송하도록 계측해야 해요.</p><p>하지만 중앙 데이터 조직 입장에서는 각 조직의 워크플로우마다 이런 의존성을 추가해달라고 요청하기가 현실적으로 어려웠어요. 다양한 도메인을 다루는 각각의 비즈니스 로직 개발/배포 흐름에 불편을 줄 수 있고, 새로운 파이프라인이 생길 때마다 빠짐없이 계측되었는지 확인해야 하는 운영 부담도 컸기 때문이죠.</p><p>OpenLineage처럼 표준화된 스펙과 풍부한 기능을 가진 오픈소스가 분명 매력적인 것은 맞지만 당근의 환경을 고려하면 몇 가지 부담이 있었어요.</p><ul><li><strong>다양한 실행 환경마다 계측 코드를 넣어야</strong> 하는 도입과 운영 비용이 크고</li><li>우리가 당장 필요한 기능도 end-to-end 오케스트레이션이 아니라 컬럼 레벨 <strong>리니지 파싱</strong>에 집중되어 있었기 때문이에요.</li></ul><p>그래서 OpenLineage를 그대로 도입하기보다 더 적합한 접근 방식을 찾는 게 현실적이라 판단했어요.</p><h3>선택한 방법: SQL 파싱 접근</h3><p>당근의 메인 데이터 웨어하우스는 BigQuery 하나였고, DW 관련 쿼리도 모두 BigQuery를 통해 실행되고 있었어요. 그래서 각 실행 환경에서 따로 이벤트를 수집하는 방식보다는 BigQuery에 남는 쿼리 로그를 한 번에 파싱하는 방식이 더 단순하고 안정적이라고 판단했어요.</p><p>아래에서 이 방식을 어떻게 설계하고 구현했는지 소개할게요.</p><h4>무엇으로부터 파싱하는가?</h4><p>당근에서는 대부분의 데이터 작업이 BigQuery에서 SQL로 이루어져요. 그리고 BigQuery는 실행된 모든 쿼리의 이력을 INFORMATION_SCHEMA.JOBS에 저장해요. 이 테이블/뷰에는 다음과 같은 정보가 들어있어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VyN0p_DMShOxBn0ETokX9A.png" /></figure><p>여기서 가져온 쿼리 텍스트를 기반으로 리니지 추출을 진행해요.</p><h4>기술 스택</h4><p>우리가 선택한 기술 스택은 다음과 같아요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sWLlFOyylGNl-zquVy5CPw.png" /></figure><ul><li><strong>sqlglot</strong>: SQL 파싱 라이브러리. 다양한 SQL 방언을 지원하고, AST(Abstract Syntax Tree)를 통해 쿼리 구조를 분석할 수 있어요.</li><li><strong>Spark</strong>: 대량의 쿼리를 병렬로 파싱하고 리니지를 생성하는 작업을 처리해요.</li><li><strong>Airflow</strong>: 주기적으로 리니지를 추출하고 업데이트하는 스케줄링을 담당해요.</li><li><strong>BigQuery</strong>: 추출한 리니지 데이터를 저장하고 조회하는 데이터 저장소로 사용해요.</li></ul><h4>리니지 추출 프로세스</h4><p>전체 추출 프로세스는 다음과 같아요.</p><ol><li><strong>쿼리 수집</strong>: 당근의 모든 GCP 프로젝트 INFORMATION_SCHEMA.JOBS에서 최근 실행된 쿼리들을 가져와요.</li><li><strong>SQL 파싱</strong>: sqlglot을 사용해 각 쿼리를 파싱하고 AST를 생성해요.</li><li><strong>의존성 추출</strong>: AST를 탐색하면서 테이블과 컬럼 간의 의존 관계를 추출해요.</li><li><strong>데이터 저장</strong>: 추출한 리니지 정보를 data_catalog.lineage 테이블에 저장해요.</li></ol><p>간단한 예시로 이런 쿼리가 있다고 가정해볼게요.</p><pre>CREATE OR REPLACE TABLE `project.dataset.user_stats` AS<br>SELECT<br>  u.user_id,<br>  u.name,<br>  COUNT(o.order_id) AS order_count<br>FROM `project.dataset.users` u<br>LEFT JOIN `project.dataset.orders` o<br>  ON u.user_id = o.user_id<br>GROUP BY u.user_id, u.name</pre><p>이때, sqlglot으로 파싱하면 다음과 같은 의존 관계를 추출할 수 있어요.</p><pre># 의존 관계 예시 (단순화된 버전)<br>lineage = [<br>  {<br>    &quot;destination_table&quot;: &quot;project.dataset.user_stats&quot;,<br>    &quot;destination_column&quot;: &quot;user_id&quot;,<br>    &quot;reference_table&quot;: &quot;project.dataset.users&quot;,<br>    &quot;reference_column&quot;: &quot;user_id&quot;<br>  },<br>  {<br>    &quot;destination_table&quot;: &quot;project.dataset.user_stats&quot;,<br>    &quot;destination_column&quot;: &quot;name&quot;,<br>    &quot;reference_table&quot;: &quot;project.dataset.users&quot;,<br>    &quot;reference_column&quot;: &quot;name&quot;<br>  },<br>  {<br>    &quot;destination_table&quot;: &quot;project.dataset.user_stats&quot;,<br>    &quot;destination_column&quot;: &quot;order_count&quot;,<br>    &quot;reference_table&quot;: &quot;project.dataset.orders&quot;,<br>    &quot;reference_column&quot;: &quot;order_id&quot;<br>  }<br>]</pre><h4>SQL 파싱의 복잡성</h4><p>하지만 실제 구현 단계로 들어가면 이야기가 훨씬 복잡해졌어요. 어떤 점들이 어려웠는지 구체적으로 몇 가지를 공유해볼게요.</p><p><strong>1) CTE (Common Table Expression) 처리</strong></p><p>WITH 절로 정의된 임시 테이블들의 의존 관계를 추적해야 했어요.</p><pre>WITH temp_users AS (<br>  SELECT user_id, name FROM users WHERE status = &#39;active&#39;<br>),<br>user_orders AS (<br>  SELECT<br>    t.user_id,<br>    t.name,<br>    o.order_id<br>  FROM temp_users t<br>  JOIN orders o ON t.user_id = o.user_id<br>)<br>SELECT * FROM user_orders</pre><p>이런 경우 CTE 간의 의존 관계와 최종 테이블까지의 흐름을 모두 추적해야 했어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*njAa4uKCKeE2bfH0GMOh4Q.png" /></figure><p><strong>2) Subquery와 Alias 관리</strong></p><p>서브쿼리가 중첩되고, 각 단계마다 alias가 붙으면 컬럼의 원천을 추적하기가 복잡해져요.</p><pre>SELECT<br>  a.user_id,<br>  b.total<br>FROM (<br>  SELECT user_id FROM users<br>) a<br>JOIN (<br>  SELECT user_id, SUM(amount) as total FROM orders GROUP BY user_id<br>) b ON a.user_id = b.user_id</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RAyFC9h9KQwmXyXcDAtFFw.png" /></figure><p>위 두 문제를 해결하기 위해, CTE와 서브쿼리를 단계적으로 풀어가며 컬럼의 원천을 추적하는 방식으로 접근했어요. 쿼리를 한 번에 해석하는 것이 아니라, 단계별로 풀어내면서 컬럼이 어디서 시작되고 어떻게 변형되는지를 따라가는 구조였어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*W7L0tFhRJnV38a9X15_LuA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VvceXZdcBXXAUmYceGmuHQ.png" /></figure><h3>데이터 모델 구조</h3><p>컬럼 레벨 리니지는 단순히 데이터를 추출하는 것만으로 끝나지 않아요. 많은 사람들이 이 정보를 다양한 목적으로 활용하기 때문에, <strong>어떻게 저장하고 제공할지</strong>가 굉장히 중요했어요.</p><p>처음에는 파싱된 모든 리니지를 하나의 테이블에 저장했어요. 하지만 시간이 지나면서 조회 목적이 다양해졌고, 한 테이블만으로는 여러 요구사항을 감당하기 어려워졌어요.</p><p>그래서 데이터를 더 효율적으로 조회하고 관리할 수 있도록 모델 구조를 다음과 같이 재정비했어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FDiQcLtCqVOOxO8TyPJICQ.png" /></figure><h4>1) Raw 테이블: 모든 정보를 있는 그대로 보존</h4><p>우선 INFORMATION_SCHEMA.JOBS에서 파싱된 리니지 결과를 <strong>있는 그대로 저장하는 원본(raw) 테이블</strong>을 구성했어요.</p><p>이 테이블에는 캐시/임시 테이블처럼 필터링이 필요한 항목들도 모두 포함되어 있어요.</p><p>이렇게 원본을 보존해두면</p><ul><li>필터 기준이 변경되거나</li><li>새로운 View를 만들거나</li><li>특정 케이스를 다시 분석할 때</li></ul><p>언제든지 전체 <strong>데이터를 기반으로 재생산</strong>할 수 있다는 장점이 있어요.</p><h4>2) 목적별로 분리한 View들</h4><p>원본 테이블 위에서 <strong>조회 목적에 따라 필요한 형태로 가공한 View들을 나누어 구성</strong>했어요.</p><p>이렇게 목적별 View를 두면 구성원들이 원하는 정보를 바로 조회할 수 있어 실제 사용자 경험이 훨씬 좋아졌어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LidoVrUI5hMA0BvZBqHrwg.png" /></figure><h4>왜 이렇게 분리했을까?</h4><p>리니지 데이터는 활용 목적이 정말 다양해요.</p><ul><li>그래프를 그릴 때는 <strong>간단한 source→target 엣지</strong>가 필요하고,</li><li>장애나 스키마 변경을 분석할 때는 <strong>소비자/생산자</strong>가 중요하고,</li><li>PII 추적이나 상세 영향도 분석을 할 때는 <strong>컬럼 단위 리니지</strong>가 필요하죠.</li></ul><p>이 모든 목적을 하나의 테이블에서 처리하려다 보면</p><ul><li>쿼리가 복잡해지고,</li><li>필요하지 않은 데이터까지 스캔하게 되고,</li><li>유지보수도 어려워져요.</li></ul><p>그래서 “<strong>원본(raw) 테이블은 가볍고 단단하게 유지하고</strong>”, 그 위에 <strong>역할별로 명확하게 구분된 View들을 얇게 쌓는 구조</strong>로 바꾼 거죠.</p><p>이 구조는 여러 팀이 리니지를 “<strong>조회 API</strong>”처럼 쉽게 활용할 수 있게 해주어요.</p><h3>활용 예시</h3><p>리니지를 잘 저장하는 것만큼, 이를 “어떻게 쉽게 꺼내 쓰게 해줄지”도 중요했어요. 그래서 내부에서는 LLM들이 바로 리니지를 조회할 수 있도록 MCP 서버를 구성했어요.</p><h4>MCP Server 구성</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*irveyKMbvKOJjiN9r_9c1A.png" /></figure><p>리니지 MCP는 크게 두 가지 일을 해요. 하나는 리니지를 조회하는 것, 다른 하나는 테이블별 사용자 정보를 파악하는 거예요. 이후의 행동과 판단은 LLM에게 맡기고 있어요.</p><p>이때 제공되는 데이터는 최근 30일 동안 실행된 쿼리 텍스트를 기반으로 해요. 내부 정기 파이프라인의 가장 긴 스케줄이 monthly라, 그보다 오래된 쿼리는 비정규 작업일 가능성이 높다고 보고 일단 제외하기로 했어요.</p><h4>사용 시나리오</h4><p>실제 업무에서 컬럼 레벨 리니지가 어떻게 활용하는지 몇 가지 사용 예시를 공유해볼게요.</p><p><strong>시나리오 1: 테이블 수정 시 영향도 분석 (LLM Platform)</strong></p><p>“테이블을 수정하고 싶은데, 누가 사용하고 있는지 모르겠어요. 영향도를 확인하고 싶어요.”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_x7StSwI1sU_4MPVSx72cQ.png" /></figure><p>이 정보를 통해 작업 담당자는 영향을 받을 조직에 사전에 안내할 수 있어 리스크를 줄일 수 있어요.</p><p><strong>시나리오 2: 데이터 이슈 발생 시 원인 추적 (Slack)</strong></p><p>“테이블의 Data Quality 테스트가 실패했을 때, 영향 범위를 파악해서 전파해야 해요”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hrZM5S9MnSd_OT9gklpLNw.png" /></figure><p>데이터 퀄리티와 같은 사내 데이터 시스템에 이상이 발생한 경우, 슬랙을 통하여 리니지 정보를 함께 제공하고 있어요. 이를 활용해 담당자는 테이블 이상 여부를 파악하고, 최근 사용자를 확인해 즉시 전파할 수 있어요.</p><h3>성과</h3><p>약 2개월의 개발 기간을 거쳐 다음과 같은 성과를 얻었어요.</p><ul><li><strong>자동화된 리니지 추출</strong>: 매일 자동으로 전날의 쿼리를 분석하고 리니지를 업데이트해요.</li><li><strong>테이블과 컬럼 레벨 추적</strong>: 하루 약 <strong>15,000개의 테이블</strong>과 <strong>80만 개의 컬럼 의존 관계</strong>를 추적하고 있어요.</li><li><strong>빠른 조회</strong>: MCP Server를 통해 몇 초 안에 원하는 리니지 정보를 조회할 수 있어요.</li><li><strong>데이터 신뢰성 향상</strong>: 변경 전에 영향도를 파악할 수 있어 더 안전한 데이터 운영이 가능해졌어요.</li></ul><h3>배운 점</h3><p>이 프로젝트를 진행하면서 많은 것을 배웠어요.</p><p><strong>(1) SQL 파싱은 생각보다 복잡하다</strong></p><p>처음에는 ‘SQL 파싱이면 쉽지 않을까?’라고 생각했지만, 실제로는 엄청나게 다양한 쿼리 패턴이 존재했어요. 특히 CTE, 서브쿼리, window function 등 각 케이스를 처리하는 데 많은 시간이 걸렸어요.</p><p>앞으로는 모든 케이스를 한 번에 처리하기보다, <strong>주요 쿼리 패턴을 중심으로 커버리지를 쌓아가면서 지속적으로 보완해 나가는 것</strong>을 목표로 하고 있어요.</p><p><strong>(2) Claude Code를 활용한 빠른 프로토타이핑</strong></p><p>MCP Server를 개발할 때 Claude Code가 큰 도움이 됐어요. 복잡한 재귀 쿼리를 작성하거나, BigQuery API를 다룰 때 Claude에게 물어보면서 빠르게 프로토타이핑할 수 있었어요.</p><p><strong>(3) 데이터 거버넌스는 점진적 개선이 중요하다</strong></p><p>처음부터 완벽한 시스템을 목표로 하기보다는, 작게라도 동작하는 버전을 먼저 만들고 실제로 쓰이게 한 뒤 점진적으로 확장하는 쪽을 선택했어요. 이렇게 ‘작게라도 실제로 쓰이는 상태’를 만든 뒤에, 운영 과정에서 받은 피드백을 바탕으로 커버리지를 넓히고 활용 시나리오를 조금씩 추가해 나갔어요. 이러한 점진적 접근이 시스템을 오히려 단단하게 만들어줬죠.</p><h3>마무리</h3><p>데이터 거버넌스는 완벽한 시스템을 한 번에 구축하는 것이 아니라, 지속적으로 개선해나가는 여정이에요. 이번 컬럼 레벨 리니지 구축은 작은 시작이지만, 당근의 데이터 신뢰성을 높이는 의미 있는 한 걸음이었어요.</p><p>앞으로 다음과 같은 개선을 계획하고 있어요.</p><ul><li><strong>실시간 리니지 업데이트</strong>: 현재는 하루 단위로 업데이트되지만, 실시간에 가깝게 업데이트할 계획이에요.</li><li><strong>시각화 도구 개발</strong>: 복잡한 의존 관계를 더 직관적으로 파악할 수 있는 그래프 UI를 준비하고 있어요.</li></ul><p>당근은 지금도 데이터를 기반으로 더 나은 의사결정을 하고, 사용자에게 더 좋은 서비스를 제공하기 위해 노력하고 있어요. 그리고 그중에서도 데이터 가치화팀은 중앙 데이터 조직으로서 작은 시도를 빠르게 실행하면서도, 단단하게 문제를 풀어나가고 있죠. 저도 이런 문제들을 풀고 싶어서 당근에 합류했어요. 저의 작은 인사이트 하나가 팀과 조직을 바꿔가는 것을 보면서 보람을 느끼기도 해요.</p><p>그리고 현재 저희 팀에서는 함께할 동료를 찾고 있어요. 데이터 거버넌스와 인프라에 관심 있는 분들이라면 성장할 기회가 활짝 열려 있으니, 오늘 공유드린 내용이 흥미롭게 다가왔다면 저희 팀에 합류해 앞으로의 남은 과제들을 함께 풀어나가요!</p><p><strong>👉 </strong><a href="https://team.daangn.com/jobs/4300801003/"><strong>당근 데이터 가치화팀 채용 공고 — Software Engineer, Data</strong></a></p><p><strong>👉 </strong><a href="https://team.daangn.com/jobs/7507320003/"><strong>당근 데이터 가치화팀 채용 공고 — Data Analytics Engineer</strong></a></p><p>긴 글 읽어주셔서 감사해요. 궁금한 점이 있다면 언제든 댓글로 남겨주세요!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=15cd862c7743" width="1" height="1" alt=""><hr><p><a href="https://medium.com/daangn/%EB%8B%B9%EA%B7%BC-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A7%80%EB%8F%84%EB%A5%BC-%EA%B7%B8%EB%A6%AC%EB%8B%A4-%EC%BB%AC%EB%9F%BC-%EB%A0%88%EB%B2%A8-%EB%A6%AC%EB%8B%88%EC%A7%80-%EA%B5%AC%EC%B6%95%EA%B8%B0-15cd862c7743">당근 데이터 지도를 그리다: 컬럼 레벨 리니지 구축기</a> was originally published in <a href="https://medium.com/daangn">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Journey to Daangn Pay’s AI-Powered FDS: From Building a Rule Engine to Applying LLMs]]></title>
            <link>https://medium.com/daangn/the-journey-to-daangn-pays-ai-powered-fds-from-building-a-rule-engine-to-applying-llms-695c58a78622?source=rss----4505f82a2dbd---4</link>
            <guid isPermaLink="false">https://medium.com/p/695c58a78622</guid>
            <category><![CDATA[english]]></category>
            <dc:creator><![CDATA[HyunwooKim]]></dc:creator>
            <pubDate>Thu, 27 Nov 2025 02:51:56 GMT</pubDate>
            <atom:updated>2025-11-27T02:51:55.190Z</atom:updated>
            <content:encoded><![CDATA[<p>Hello! I’m peter.kim, a backend engineer on the Daangn Pay Compliance &amp; Strategy team.</p><p>Our team builds and operates financial compliance systems — including FDS and AML — to ensure that Daangn Pay can be provided safely and securely. In simple terms, we detect abnormal patterns in user transactions and design systems that comply with laws and regulations. At the same time, we support Daangn Pay’s business strategy and think about how the service should grow.</p><p>It has already been 1 year and 4 months since I joined the team and began developing the FDS. In this post, I’d like to share how Daangn Pay’s FDS has evolved so far.</p><h3>Have you heard of <strong>FDS</strong> before?</h3><p>FDS stands for <strong>Fraud Detection System</strong>, commonly referred to as an abnormal transaction detection system in the financial industry.</p><p>Daangn Pay’s FDS detects and blocks various fraudulent or suspicious activities in real time — such as secondhand marketplace scams, telecom financial fraud like voice phishing or smishing, and identity theft–related transactions.</p><p>Have you ever experienced something like this?</p><ul><li>Someone impersonates a family member or law enforcement and pressures you into transferring money</li><li>You click a link you thought was a friend’s wedding invitation, only to find money withdrawn from your account without your knowledge</li><li>During a secondhand transaction, you’re asked to send money to a specific account, and the seller disappears afterward</li></ul><p>The goal of Daangn Pay’s FDS is to protect the financial assets of our users and minimize harm from cases like these.</p><h3>How Does the Daangn Pay FDS Work?</h3><p>The Daangn Pay FDS needs to detect and respond to a wide variety of abnormal transaction patterns quickly. To achieve this, we needed a structure that could flexibly adjust detection logic in response to evolving fraud trends.</p><p>That’s why the very first task I took on after joining the team was to build this foundation — and at the center of it was the <strong>rule engine</strong>.</p><p>A rule engine is a core component that allows FDS detection logic to be flexibly expanded and combined. Put simply, it works like LEGO blocks: you assemble different conditions to create detection rules, group those rules into policies, and operate them as units.</p><p>The rule engine consists of three main elements — <strong>conditions</strong>, <strong>rules</strong>, and <strong>policies</strong> — which can be combined to detect various suspicious patterns in real time.</p><p>Let’s take a closer look at each of these components.</p><h4>Conditions</h4><p>A <strong>condition</strong> is the most basic building block of FDS detection. Once you create a condition block, you can adjust specific thresholds <em>(a)</em> freely, allowing the system to respond quickly. For example, conditions can be structured like this:</p><ul><li>The user joined Daangn Pay within <em>(a)</em> days<br> (e.g., within 30 days, within 70 days)</li><li>The number of recent transfers is <em>(a)</em> or more<br> (e.g., 30 or more, 70 or more)</li></ul><h4>Rules</h4><p>A <strong>rule</strong> is a unit that determines whether a transaction is suspicious by combining multiple conditions. Using the examples above, we can create four rules like these:</p><ul><li>Joined Daangn Pay within 30 days + 30 or more recent transfers</li><li>Joined Daangn Pay within 30 days + 70 or more recent transfers</li><li>Joined Daangn Pay within 70 days + 30 or more recent transfers</li><li>Joined Daangn Pay within 70 days + 70 or more recent transfers</li></ul><h4>Policies</h4><p>A <strong>policy</strong> is a unit that groups together rules with a similar purpose. At the policy level, you can manage both the overall policy configuration and the settings for each individual rule.</p><p><strong>Policy-level configuration</strong></p><ul><li><strong>Policy type:</strong> Account transfer policy</li><li><strong>LLM prompt:</strong> Secondhand-trade-context transfer prompt</li></ul><p><strong>Rule-level configuration within the policy</strong></p><ul><li><strong>Rule 1:</strong> (Joined Daangn Pay within 30 days + 30 or more recent transfers)<br> — <em>Sanction Level: High, Alert: On</em></li><li><strong>Rule 2:</strong> (Joined Daangn Pay within 30 days + 70 or more recent transfers)<br> — <em>Sanction Level: High, Alert: Off</em></li><li><strong>Rule 3:</strong> (Joined Daangn Pay within 70 days + 30 or more recent transfers)<br> — <em>Sanction Level: High, Alert: Off</em></li><li><strong>Rule 4:</strong> (Joined Daangn Pay within 70 days + 70 or more recent transfers)<br> — <em>Sanction Level: High, Alert: On</em></li></ul><p>Within the FDS, the risk assessment process revolves around the rule engine and goes through the following three steps.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YswBaBQ6JQ2SLRygeoWEcg.png" /></figure><h4>1. Event Ingestion: Synchronous API vs. Asynchronous Stream</h4><p>Events entering the FDS flow through one of two paths. Since these paths differ in purpose and processing characteristics, we operate them separately.</p><h4>Synchronous API</h4><ul><li><strong>Characteristics:</strong> Works in a request–response pattern, requiring extremely low latency.</li><li><strong>Use case:</strong> When an immediate block decision is needed during a transfer.</li><li><strong>How it works:</strong> The rule engine is evaluated at the moment the API is called, and the result is returned instantly in the response.</li></ul><h4>Asynchronous Stream</h4><ul><li><strong>Characteristics:</strong> Processes large volumes of events through real-time streaming, where a certain level of latency is acceptable.</li><li><strong>Use case:</strong> When an immediate block is <em>not</em> required during a transfer.</li><li><strong>How it works:</strong> Events collected in the stream pass through the pipeline, are evaluated by the rule engine, and — if necessary — trigger alerts or sanctions during the post-processing stage.</li></ul><h4>2. Risk Assessment via the Rule Engine</h4><p>Each incoming event goes through the full evaluation flow defined by the rule engine — from <strong>policies → rules → conditions</strong> — to produce a final risk assessment.<br> In this process, we designed the system so that both event types — those requiring immediate blocking via API and those suitable for deferred handling via Stream — are processed within the same rule framework.</p><h4>3. Post-Processing (LLM Evaluation, Alerts, and Sanctions)</h4><p>Once the risk assessment is complete, the system goes through the following post-processing steps:</p><p><strong>LLM Evaluation</strong></p><ul><li>We generate an LLM prompt by combining the rule engine output with additional contextual data, then use LLM-based context and behavior analysis to assign an additional risk score.</li></ul><p><strong>Alert Handling</strong></p><ul><li>If the triggered rule has alerts enabled, a notification is sent to the Customer Service team.</li></ul><p><strong>Sanction Handling</strong></p><ul><li>Based on the sanction level specified in the rule, the system applies sanctions to the user and emits events to synchronize the user’s status across the platform.</li></ul><h4>What Has Improved After Introducing the Rule Engine?</h4><p>As the system evolved, our team added multiple FDS policies tailored to new scenarios. This allowed us to build a robust foundation capable of adapting quickly to rapidly changing fraud patterns.<br> By continuously adding and modifying FDS rules in real time to minimize user harm — and by reflecting new fraud trends promptly — the number of fraud- or damage-related information requests from financial institutions and law enforcement has decreased.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AY8MkszVoyPB17-dj4JtQA.png" /></figure><h3>The FDS Is Still Hungry… Enter the AI-Powered FDS!</h3><p>Even after completing the rule engine and improving many parts of the system, our team still had one persistent question:</p><p><strong>How can we provide fast and consistent results during FDS review?</strong></p><ul><li>Reviewing a single suspicious transaction detected by the FDS can take anywhere from <strong>5 minutes to as long as 20 minutes</strong>, since the reviewer needs to examine transaction history, related secondhand-trade listings, and more.</li><li>On top of that, monitoring responsibilities rotate depending on the day and time, and reviewers often handle other tasks such as phone support — which naturally leads to variations in judgment.</li></ul><p>To solve this, our team decided to introduce <strong>LLMs</strong>.</p><h4>Applying LLMs in Electronic Financial Services</h4><p>Our goal for introducing LLMs was clear:<br> <strong>to evaluate whether a transaction is fraudulent — considering its full context — and provide both the assessment and reasoning to monitoring staff, enabling faster and more consistent decision-making.</strong></p><p>However, because generative AI in electronic financial services must comply with strict regulations such as network separation, the biggest challenge was securing regulatory exemptions through the <em>Innovative Financial Service</em> program. Fortunately, with the help of many team members, we were able to earn this designation.</p><p>Daangn Pay operates in an AWS environment, and our approval for the Innovative Financial Service program was based on using <strong>Claude Sonnet 3.5</strong> via <strong>AWS Bedrock</strong>.<br> We wanted to use newer, more powerful models, but the designation required that any generative AI model we use be available <strong>within a domestic AWS region</strong>, which limited our options.</p><h4>Step 01: Prompt + Converse API</h4><p>We started simple — combining a prompt with the Converse API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8cjtl5psQCrUtjuXJXYdSg.png" /></figure><p>Following the approach in the diagram above, we quickly began implementing the workflow needed to call the LLM with a structured prompt.</p><ol><li>Data Retrieval &amp; Storage: A scheduler queries the fraud-history table stored in BigQuery each day, classifies the data, and saves it in Redis Cache.</li><li>Data Collection: After risk assessment is complete, a DataCollector tailored to the prompt type gathers all necessary data from the detected event.</li><li>Prompt Construction: Based on the data collected, the prompt builder constructs the final prompt, which is then sent to the Bedrock model.</li></ol><p>Our DS team member, Devin, iterated on and optimized the prompts through extensive testing. During this process, we learned an important lesson:<br> <strong>explicit, step-by-step, and well-structured prompts consistently produce better results.</strong></p><p>As a result, we ended up adopting prompts in the following structured format.</p><pre>&lt;role&gt;<br>&lt;/role&gt;<br>&lt;evaluation_flow&gt;<br>step1<br>...<br>step3<br>&lt;/evaluation_flow&gt;<br>---<br><br>&lt;current_transaction&gt;<br>{{current_transaction}}<br>&lt;/current_transaction&gt;<br><br>---<br><br>&lt;recent_frauds&gt;<br>{{recent_frauds}}<br>&lt;/recent_frauds&gt;<br><br>---<br><br>&lt;evaluation_criteria&gt;<br>&lt;/evaluation_criteria&gt;<br><br>&lt;output_format&gt;<br>```json<br>{<br>    &quot;fraud&quot;: &quot;Y or N&quot;,<br>    &quot;reasoning&quot;: &quot;reason1 (korean) || reason2 (korean) || reason3 (korean)&quot;<br>}<br>```<br>&lt;/output_format&gt;</pre><p>In the prompt above, the required data is defined as <strong>{{current_transaction}}</strong> and <strong>{{recent_frauds}}</strong>.</p><p>The collected data is then formatted by the prompt builder according to the predefined structure, completing the final prompt.</p><p>When we send the completed prompt to the model via the Converse API, the output is generated according to the specified &lt;output_format&gt;, and the result is delivered to the monitoring team through a Slack alert.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zgyPVbTgQ-7fcpOehplPqw.png" /></figure><h4>Step 02: RAG</h4><p>As we accumulated data from LLM requests and responses made through the Converse API, we began analyzing the model’s performance.</p><p>The graph below illustrates <strong>precision</strong> and <strong>recall</strong> for each prompt type.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DEqUP5uG0yVKv9HDBN-t4g.png" /></figure><p>For <strong>Prompt Type 2</strong>, we included recent fraud data in the LLM evaluation, but we found that both <strong>precision</strong> and <strong>recall</strong> gradually declined.<br> While reviewing this issue, we identified the following limitations:</p><p><strong>Recent fraud patterns may not be related to the current transaction.</strong></p><ul><li>Including fraud-related information generally improves LLM performance.</li><li>However, simply inserting <em>recent</em> fraud cases can cause mismatches, because those cases may not resemble the current transaction pattern at all — leading to incorrect evaluations.</li></ul><p>To address this, we realized we needed a system that could <strong>retrieve fraud cases similar to the current transaction</strong> and feed them into the LLM. That’s where <strong>RAG (Retrieval-Augmented Generation)</strong> comes in.</p><p>RAG combines <em>retrieval</em> and <em>generation</em>, allowing an LLM to reference external knowledge when generating answers.<br> With RAG, we could ensure the LLM considers <strong>only fraud histories highly similar to the current transaction</strong>, improving decision quality.</p><p>AWS Bedrock supports RAG through its <strong>Knowledge Bases</strong>, so we structured our system as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*C75VDW3XncKloPGiyy2Dwg.png" /></figure><p>1. Knowledge Base</p><ul><li>We created a Knowledge Base in Bedrock Service dedicated to fraud-history data.</li><li>We connected the data source and set the data deletion policy to <strong>DELETE</strong>.</li></ul><p>2. Vector DB</p><ul><li>We chose <strong>OpenSearch Serverless</strong>.</li><li>Among the vector databases that integrate easily with Bedrock Knowledge Bases, OpenSearch was the best fit.</li><li>Since LLM calls within the FDS occur intermittently and in relatively low volume, OpenSearch Serverless was the most cost-efficient option.</li><li>For the index’s vector field settings, we prioritized accuracy over speed and configured the parameters as follows:</li><li><strong>dimension:</strong> 1024 (default)</li><li><strong>space_type:</strong> cosinesimil (cosine similarity)</li><li><strong>ef_construction:</strong> 256 (range of exploration when building the HNSW graph; higher values increase accuracy)</li><li><strong>m:</strong> 48 (number of neighbors per node; increased above typical recommendations to maximize search accuracy, at the cost of more memory usage)</li></ul><p>3. Data Source</p><ul><li>We selected <strong>S3</strong> as the data source for the fraud-history Knowledge Base.</li><li>Data is separated by domain and managed based on date for embedding.</li><li>The data to be embedded is stored in <strong>JSON</strong> format.</li></ul><pre>s3://karrotpay-fds/<br>└── knowledge-base/                   # FDS 관련 데이터 통합 관리<br>    ├── marketplace/                  # 도메인 1: 마켓플레이스 사기 탐지<br>    │   └── raw/                      # 원본 데이터 (Bedrock 소스)<br>    │       └── year=2025/<br>    │           └── month=10/<br>    │               └── day=22/<br>    │                   ├── fraud_FRAUD-2025-000001.json<br>    │                   ├── fraud_FRAUD-2025-000002.json<br>    │                   └── _SUCCESS   <br>    └── atm/                          # 도메인 2: ATM 모니터링 (추가 도메인)<br>        └── raw/</pre><h4><strong>Lessons Learned</strong></h4><p><strong>The Problem</strong></p><p>We generate about <strong>200 new fraud-history records</strong> each day that need to be embedded.<br> At first, we stored <strong>20 records per JSONL file</strong>, with each line containing a single JSON object, and proceeded with embedding.</p><p>However, during retrieval, we discovered that the returned <strong>content was not a complete JSON object</strong> — parts of the JSON were cut off, resulting in incomplete or truncated data.</p><pre>{<br>  &quot;results&quot;: [<br>    {<br>      &quot;score&quot;: 0.87383044,<br>      &quot;content&quot;: ...\&quot;verify_updated_days\&quot;:0} {\&quot;fraud_id\&quot;:\&quot;FRAUD-2025-000004\&quot;...,<br>      &quot;location&quot;: &quot;s3://karrotpay-fds/knowledge-base/marketplace/raw/year=2025/month=10/day=28/fraud_job_20251028_001.jsonl&quot;<br>    }<br>  ]<br>}</pre><p><strong>Root Cause Analysis</strong></p><p>The issue stemmed from how the Knowledge Base handled <strong>chunking</strong>.<br> In our FDS setup, we used <strong>fixed-size chunking</strong>, where text is split into segments based on the maxTokens parameter before embedding.</p><p>Because of this, the retrieval results were returned <strong>not by JSON record</strong>, but by <strong>chunk boundaries</strong>.<br> As a result, chunks often split a JSON line in the middle, causing the retrieved content to be incomplete or truncated.</p><pre>{&quot;fraud_id&quot;:&quot;FRAUD-2025-000003&quot;,...,&quot;verify_updated_days&quot;:0}<br>{&quot;fraud_id&quot;:&quot;FRAUD-2025-000004&quot;,...,&quot;verify_updated_days&quot;:0}<br>{&quot;fraud_id&quot;:&quot;FRAUD-2025-000005&quot;,...,&quot;verify_updated_days&quot;:0}<br><br>Bedrock Chunking 후:<br>Chunk 1: {&quot;fraud_id&quot;:&quot;FRAUD-2025-000003&quot;,...,&quot;verify_upd<br>Chunk 2: ated_days&quot;:0} {&quot;fraud_id&quot;:&quot;FRAUD-2025-000004&quot;,...<br>Chunk 3: ...,&quot;verify_updated_days&quot;:0}</pre><p><strong>Solution</strong></p><p>We identified two potential approaches to resolve the issue:</p><p>1. Switch the Knowledge Base Chunking Strategy to Semantic</p><ul><li>This method finds natural boundaries based on semantic similarity.</li><li>It can be adjusted using parameters such as breakpointPercentileThreshold, bufferSize, and maxTokens.</li><li>However, it requires careful tuning to determine the correct chunk size and semantic boundaries.</li><li>If fraud-history data varies significantly in structure, achieving perfectly consistent chunking becomes difficult — making this approach less reliable.</li></ul><p>2. Split Each Fraud Record Into Its Own JSON File for Embedding</p><ul><li>Each file is treated as a single document, and chunking can be disabled (ChunkingStrategy = NONE), or—if the file is small enough—it naturally fits into a single chunk.</li><li>With NONE, the entire file becomes one chunk, ensuring the JSON is never cut.</li><li>With FIXED_SIZE (default), if the file size is smaller than maxTokens, it also becomes a single chunk, safely preserving content.</li><li>Storing each fraud record in a separate file also improves clarity, making retrieval and categorization easier.</li></ul><p>Given these advantages, <strong>we chose approach #2</strong>.</p><p>Additionally, we found that embedding fraud-history data as <strong>natural language text</strong> rather than structured JSON improved retrieval accuracy slightly.<br> Considering that Step 03 uses a <em>Retrieve-and-Generate</em> pattern, embedding natural-language text would likely have been even more effective.</p><h4>Step 03: Retrieve + Prompt + ConverseAPI</h4><p>We tested two different methods for applying RAG within the FDS:</p><p>1. Retrieve-and-Generate Method</p><ul><li>We initially expected that combining RAG with the existing Converse API workflow would allow us to obtain more accurate results in a single step.</li><li>However, during testing, we observed several issues: The longer the prompt became, the more frequently response errors occurred. The output format became unstable and inconsistent.</li><li>Although the model provided citations, we had limited control over retrieval results.</li><li>It was difficult to understand or customize how documents were selected and combined during generation.</li></ul><p>2. Retrieve + ConverseAPI Method</p><ul><li>Using Retrieve alone, we could explicitly inspect the top-N scores, the retrieved document content, the S3 document paths, and metadata.</li><li>Based on this information, we could determine exactly how to combine the retrieved data and insert it into the prompt.</li><li>This gave us full control over the retrieval logic, data selection criteria, and prompt construction.</li></ul><p>Ultimately, we finalized the <strong>Retrieve + Prompt + ConverseAPI</strong> pipeline as follows.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*igDdWZVPFSSN-7e1XbEgXg.png" /></figure><h3>What’s Next</h3><p>There are still several improvements we want to apply to the FDS moving forward.</p><p>1. Adding Guardrails Before LLM Invocation</p><ul><li>Guardrails are safety mechanisms and constraints that help ensure an AI model’s input and output remain safe and appropriate.</li><li>Although we already sanitize data to avoid including any personal information, we are currently working on adding dedicated guardrail prompts to strengthen safety even further.</li></ul><p>2. Embedding Both True Positives and False Positives Detected by the FDS</p><ul><li>Currently, we only embed actual fraud-history data.</li><li>In the future, we hope to embed both true-positive and false-positive cases detected by the FDS and use this embedded data to automatically generate prompts for the Converse API.</li><li>By doing so, we expect to significantly improve the accuracy of LLM-based detection.</li></ul><h3>Wrapping Up</h3><p>Applying LLMs to Daangn Pay was more challenging than we initially expected. It wouldn’t have been possible without the hard work and collaboration of many teams, including Compliance, Security, and SRE. I’d like to take this opportunity to thank everyone involved.</p><p>In many ways, our journey toward an <strong>AI-Powered FDS</strong> is just beginning. Progress may not always be fast due to various constraints, but the FDS continues to evolve to ensure our users can enjoy safer financial services. We will keep enhancing the system for more precise detection, faster response, and a more secure financial environment. Please stay tuned as Daangn Pay — and our FDS — continue to grow with AI!</p><p>And right now, Daangn Pay is looking for teammates who want to build better financial experiences together. Engineers from various domains collaborate as one team, taking on technical challenges and learning from one another as we work toward safer financial services.<br> If you feel excited about solving problems like these, check out our job openings at the link below:</p><p>👉 <a href="https://about.daangn.com/jobs/?corp=KARROT_PAY#_filter"><strong>View Daangn Pay Job Openings</strong></a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=695c58a78622" width="1" height="1" alt=""><hr><p><a href="https://medium.com/daangn/the-journey-to-daangn-pays-ai-powered-fds-from-building-a-rule-engine-to-applying-llms-695c58a78622">The Journey to Daangn Pay’s AI-Powered FDS: From Building a Rule Engine to Applying LLMs</a> was originally published in <a href="https://medium.com/daangn">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[당근페이 AI Powered FDS로 가는 여정: 룰엔진구축부터 LLM 적용까지]]></title>
            <link>https://medium.com/daangn/%EB%8B%B9%EA%B7%BC%ED%8E%98%EC%9D%B4-ai-powered-fds%EB%A1%9C-%EA%B0%80%EB%8A%94-%EC%97%AC%EC%A0%95-%EB%A3%B0%EC%97%94%EC%A7%84%EA%B5%AC%EC%B6%95%EB%B6%80%ED%84%B0-llm-%EC%A0%81%EC%9A%A9%EA%B9%8C%EC%A7%80-725c3a1c388d?source=rss----4505f82a2dbd---4</link>
            <guid isPermaLink="false">https://medium.com/p/725c3a1c388d</guid>
            <category><![CDATA[ai]]></category>
            <dc:creator><![CDATA[HyunwooKim]]></dc:creator>
            <pubDate>Mon, 24 Nov 2025 11:40:12 GMT</pubDate>
            <atom:updated>2025-12-23T01:43:14.535Z</atom:updated>
            <content:encoded><![CDATA[<p>안녕하세요. 당근페이 Compliance &amp; Strategy 팀에서 백엔드 엔지니어로 일하고 있는 peter.kim이에요.</p><p>저희 팀은 FDS, AML 등을 포함한 금융 컴플라이언스 시스템을 만들고 운영하면서, 당근페이가 안전하게 서비스될 수 있도록 하는 역할을 하고 있어요. 쉽게 말하면, 사용자의 거래에서 이상 패턴을 감지하고, 법과 규제를 준수하도록 시스템을 설계하고 유지하는 팀이죠. 동시에 당근페이의 사업 전략을 지원하고, 성장 방향을 고민하기도 합니다. 제가 팀에 합류해 FDS를 개발한 지도 어느덧 1년 4개월이 지났는데요. 이번 글에서는 당근페이 FDS가 지금까지 어떻게 발전해 왔는지 소개해보려고 해요.</p><h3>FDS에 대해 들어본 적 있으신가요?</h3><p>FDS는 Fraud Detection System의 약자로, 금융 분야에서는 흔히 이상거래 탐지 시스템이라고 불러요.</p><p>당근페이의 FDS는 중고거래 사기, 보이스피싱이나 스미싱 같은 전기통신금융사기 의심거래와 명의도용 의심거래 등을 실시간으로 탐지하고 차단하고 있어요.</p><p>혹시 이런 상황을 겪어보신 적 있나요?</p><ul><li>가족이나 수사기관을 사칭해 돈을 이체하도록 유도당한 경우</li><li>지인의 청첩장 링크인줄 알고 눌렀는데, 나도 모르는 사이에 돈이 빠져 나간 경우</li><li>중고거래에서 특정 계좌로 송금하도록 안내받고, 상대방이 잠적한 경우</li></ul><p>당근페이 FDS의 목표는 이런 사례로부터 당근페이를 사용하는 고객의 금융자산을 보호하고, 피해를 최소화하는 것이에요.</p><h3>당근페이 FDS는 어떤 구조로 동작할까요?</h3><p>당근페이 FDS는 다양한 이상거래 패턴을 빠르게 탐지하고 대응해야 해요. 이를 위해서는 변화하는 사기 트렌드에 맞춰 탐지 로직을 유연하게 조정할 수 있는 구조가 필요했어요.</p><p>그래서 제가 팀에 합류한 후 가장 먼저 맡은 일도 바로 이 기반을 만드는 것이었고, 그 핵심은 <strong>룰엔진(rule engine)</strong> 구축이었어요.</p><p>룰엔진은 FDS의 탐지 로직을 유연하게 확장하고 조합할 수 있는 핵심 컴포넌트로, 쉽게 말하면 <strong>레고 블록처럼 조건들을 조립해 탐지 규칙을 만들고, 정책 단위로 묶어 운영할 수 있는 구조</strong>예요.</p><p>이 룰엔진은 조건, 규칙, 정책 세 가지 요소로 구성되어 있으며, 이들을 조합해 실시간으로 다양한 의심 패턴을 탐지하도록 설계했어요.</p><p>이제 룰엔진을 구성하는 요소들을 하나씩 살펴볼게요.</p><h4>조건</h4><p>조건은 FDS 탐지의 가장 기본 단위, 즉 빌딩 블록이에요. 하나의 조건 블록을 만들어 두면 특정 임계값(a)을 자유롭게 바꾸면서 빠르게 대응할 수 있답니다. 예시를 들어보면 이렇게 구성할 수 있어요.</p><ul><li>당근페이 가입 후 (a)일 이내인 경우 (ex. 30일 이내, 70일 이내)</li><li>최근 송금 횟수가 (a)건 이상인 경우 (ex. 30건 이상, 70건 이상)</li></ul><h4>규칙</h4><p>규칙은 조건을 조합해 이상거래 여부를 판단하는 기본 단위예요. 앞서 예시로 든 조건을 조합하면, 다음과 같이 4개의 규칙을 만들 수 있어요.</p><ul><li>당근페이 가입 후 30일 이내 + 최근 송금 횟수가 30건 이상인 경우</li><li>당근페이 가입 후 30일 이내 + 최근 송금 횟수가 70건 이상인 경우</li><li>당근페이 가입 후 70일 이내 + 최근 송금 횟수가 30건 이상인 경우</li><li>당근페이 가입 후 70일 이내 + 최근 송금 횟수가 70건 이상인 경우</li></ul><h4>정책</h4><p>정책은 비슷한 목적의 규칙들을 묶어 관리하는 단위예요. 정책 단위에서는 정책 자체 설정과 각 규칙 별 설정을 함께 관리할 수 있어요.</p><p><strong>정책 자체 설정</strong></p><ul><li>정책 타입: 계좌 송금 정책</li><li>LLM 프롬프트: 중고거래 맥락 송금 프롬프트</li></ul><p><strong>정책 내 규칙 설정</strong></p><ul><li>규칙1: (당근페이 가입 후 30일 이내 + 최근 송금 횟수가 30건 이상인 경우) [제재 수준: 강함, 알람 O]</li><li>규칙2: (당근페이 가입 후 30일 이내 + 최근 송금 횟수가 70건 이상인 경우) [제재 수준: 강함, 알람 X]</li><li>규칙3: (당근페이 가입 후 70일 이내 + 최근 송금 횟수가 30건 이상인 경우) [제재 수준: 강함, 알람 X]</li><li>규칙4: (당근페이 가입 후 70일 이내 + 최근 송금 횟수가 70건 이상인 경우) [제재 수준: 강함, 알람 O]</li></ul><p>FDS 내에서 위험 평가 프로세스는 룰엔진을 중심으로 다음의 세 단계를 거치고 있어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cM_VLmseOHWpV6vfBt3MGA.png" /></figure><h4>1. 이벤트 인입: 동기 API와 비동기 Stream 경로</h4><p>FDS로 들어오는 이벤트는 두 가지 경로 중 하나를 통해 유입돼요. 이 두 경로는 처리 방식과 목적이 다르기 때문에 분리해서 운영하고 있어요.</p><p><strong>동기 API (Synchronous API)</strong></p><ul><li>특징: 요청-응답 패턴으로 동작하며, 응답 지연 시간이 매우 짧아야 해요.</li><li>사용 사례: 송금시 즉시 차단 여부 판단이 필요한 경우</li><li>처리 방식: API 호출과 동시에 룰엔진 평가를 수행하고, 결과를 즉시 응답으로 반환해요.</li></ul><p><strong>비동기 Stream (Asynchronous Stream)</strong></p><ul><li>특징: 대량의 이벤트를 실시간 스트리밍으로 처리하며, 일정 수준의 지연이 허용돼요.</li><li>사용 사례: 송금시 즉시 차단이 필요 없는 경우</li><li>처리 방식: Stream으로 수집된 이벤트는 파이프라인을 통해 순차적으로 룰엔진 평가 후, 필요한 경우 후처리 단계에서 알람 또는 제재를 발생시켜요.</li></ul><h4>2. 룰엔진을 통한 위험 평가</h4><p>유입된 이벤트는 룰엔진이 설정한 <strong>정책 → 규칙 → 조건</strong>의 평가 단계를 모두 거치면서 최종적인 위험 판단을 생성해요. 이 과정에서 즉시 차단이 필요한 이벤트(API), 후속 처리가 가능한 이벤트(Stream)가 동일한 룰 구조 안에서 처리되도록 설계했어요.</p><h4>3. 평가 결과 후처리 (LLM 평가 및 알람/제재 처리)</h4><p>위험 평가가 끝나면 다음과 같은 후처리 단계를 거쳐요.</p><p><strong>LLM 평가</strong></p><ul><li>룰엔진 결과와 프롬프트 생성을 위한 데이터를 조합해 LLM 프롬프트를 만들고 LLM 기반의 맥락·행동 분석을 통해 추가적인 위험 점수를 평가해요.</li></ul><p><strong>알람 처리</strong></p><ul><li>해당 규칙에 알람 설정이 되어 있으면 고객 서비스팀으로 알람을 전달해요.</li></ul><p><strong>제재 처리</strong></p><ul><li>규칙에 지정된 제재 등급에 따라 유저에게 제재를 적용하고, 유저의 상태를 시스템 전반에 동기화 시키기 위한 이벤트를 발생시켜요.</li></ul><h3>룰엔진 도입 결과는?</h3><p>팀에서 시스템의 변화에 맞춰 FDS 운영을 위한 여러 정책을 추가 했고, 덕분에 급변하는 사기 패턴도 유연하고 빠르게 대응할 수 있는 기본 환경을 갖출 수 있었어요. 사용자 피해를 최소화 하기 위해 실시간으로 FDS 규칙을 추가하거나 변경하며, 사기 트렌드를 빠르게 반영한 결과, 금융기관과 수사기관으로부터 사기 또는 피해 관련 정보 요청 건수가 감소했어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*K7mj-kSvjy0JWZLLXvSAaw.png" /></figure><h3>FDS는 아직 배고프다… AI Powered FDS 시작!</h3><p>FDS의 룰엔진 구축을 완료하고 많은 부분이 개선되었지만, 팀에서 항상 고민하는 부분이 있었어요.</p><p>FDS 탐지에 대한 검토과정에서 어떻게 하면 일관되고 빠른 결과를 제공할 수 있을까?</p><ul><li>FDS로 탐지된 한 건의 이상 거래를 검토하는 데에는 거래 내역, 관련 중고거래 게시글 이력 등을 모두 확인해야 해서 짧게는 5분, 길게는 20분까지 소요되기도 해요.</li><li>또한, 요일과 시간에 따라 모니터링 담당자가 변경되고 전화상담과 같은 다른 업무도 함께 봐주시기 때문에 판단 결과에 따른 편차도 발생할 수 있어요.</li></ul><p>결국 이 문제를 해결하기 위해 팀에서는 LLM을 도입 하기로 결정했어요.</p><h4>전자금융업에서 LLM 적용하기</h4><p>LLM 도입 목표는 명확했어요. 거래 맥락을 고려해 사기 여부와 근거를 평가하고, 모니터링 담당자에게 제공해 일관되고 빠른 판단을 돕는 것이었어요.</p><p>하지만 전자금융업 특성상 생성형 AI를 사용하려면 망분리 등 규제를 준수해야 했고, 가장 중요한 과제는 혁신금융서비스 신청을 통해 일부 규제 예외를 받는 것이었어요. 다행히 많은 구성원의 도움 덕분에 혁신금융서비스로 지정될 수 있었어요.</p><p>당근페이는 AWS 환경을 사용하며 AWS bedrock 서비스를 통해 제공되는 Claude Sonnet 3.5 모델을 기준으로 혁신금융서비스로 지정받았어요. 더 좋은 사양의 최신 모델을 사용하고 싶었지만, 혁신금융서비스 지정에도 사용하고자 하는 생성형 AI 모델이 국내 리전에 있어야 하는 제약이 있었어요.</p><h4>Step01 : Prompt + ConverseAPI</h4><p>첫 시작은 단순하게 프롬프트와 converseAPI의 조합이었어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*G4hWRAz5dHwW7lLx5KnfMg.png" /></figure><p>위의 그림과 같이 프롬프트를 적용해서 LLM을 호출하기 위한 작업을 빠르게 시작해서 적용했어요.</p><ol><li>데이터 조회 및 저장: 스케쥴러를 통해 매일 BigQuery에 저장되어 있는 사기 이력 테이블을 조회해서 분류하고 Redis Cache에 저장해요.</li><li>데이터 수집: 리스크 평가를 마치고 탐지된 데이터를 대상으로 프롬프트에 맞는 DataCollector를 생성해서 데이터를 수집해요.</li><li>프롬프트 완성: DataCollector를 통해 수집한 데이터를 기반으로 프롬프트 빌더를 통해서 최종적으로 프롬프트를 완성시켜서 bedrock model을 호출해요.</li></ol><p>팀 내 DS 담당인 데빈이 프롬프트를 테스트하며 최적화 했고, 이 과정을 거치며 함께 단계별로 명시적이고 구조화된 프롬프트일수록 더 좋은 결과를 얻을 수 있다는 점을 배웠어요. 그래서 최종적으로 다음과 같은 형태의 프롬프트를 적용하게 되었습니다.</p><pre>&lt;role&gt;<br>&lt;/role&gt;<br>&lt;evaluation_flow&gt;<br>step1<br>...<br>step3<br>&lt;/evaluation_flow&gt;<br>---<br><br>&lt;current_transaction&gt;<br>{{current_transaction}}<br>&lt;/current_transaction&gt;<br><br>---<br><br>&lt;recent_frauds&gt;<br>{{recent_frauds}}<br>&lt;/recent_frauds&gt;<br><br>---<br><br>&lt;evaluation_criteria&gt;<br>&lt;/evaluation_criteria&gt;<br><br>&lt;output_format&gt;<br>```json<br>{<br>    &quot;fraud&quot;: &quot;Y or N&quot;,<br>    &quot;reasoning&quot;: &quot;이유1 (한국어) || 이유2 (한국어) || 이유3 (한국어)&quot;<br>}<br>```<br>&lt;/output_format&gt;</pre><p>위의 프롬프트에서 필요한 데이터는 {{current_transaction}}, {{recent_frauds}} 로 정의했어요.</p><p>수집한 데이터는 프롬프트 빌더를 통해 지정된 형식으로 적용되고, 최종적으로 프롬프트가 완성돼요. 완성된 프롬프트로 converseAPI를 통해 모델을 호출하면 &lt;output_format&gt; 에 맞춰 결과가 생성되고, 슬랙 알람으로 모니터링 담당자들에게 전달됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oKz3Y7oQVpQjYj0VbHVNCg.png" /></figure><h4>Step02 : RAG</h4><p>ConverseAPI를 통한 LLM 호출과 응답에 대한 데이터를 따로 쌓으면서 성능을 분석했어요.</p><p>아래 그래프는 프롬프트 타입별 정탐률(Precision)과 재현률(Recall)을 나타낸 그래프예요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wiMczPzRhF1IazNe5DzMgA.png" /></figure><p>프롬프트2의 경우에는 최근 사기 데이터를 LLM 평가에 활용했지만, 정탐률과 재현률이 점차 하락하는 현상을 발견했어요. 이런 상황을 검토하면서 다음과 같은 한계점을 확인할 수 있었어요.</p><p>최근 사기 패턴은 현재 거래와 연관성이 없을 수 있다는 것이에요.</p><ul><li>LLM 평가를 진행할때 사기 관련 정보를 포함하는 것이 일반적으로 성능을 높였지만</li><li>단순히 최근 사기 거래만을 포함하면 현재 거래 패턴과 다를 수도 있기 때문에 잘못된 판단 결과를 내릴 가능성이 높았던 거예요.</li></ul><p>이 문제를 해결하기 위해 현재 거래와 유사한 사기 관련 정보를 검색해 LLM 평가에 활용할 수 있는 RAG(Retrieval Augmented Generation) 시스템 구축에 대한 필요성을 느끼게 되었어요.</p><p>RAG는 검색과 생성을 결합한 방식으로 LLM이 답변을 생성할 때 외부 지식을 검색해서 참고하는 기술이에요. RAG를 통해서 현재 거래와 유사도가 높은 사기 이력들을 참고하게 해서 문제를 해결하고자 했어요.</p><p>AWS Bedrock에서는 Knowledge base를 통해 RAG 구축을 지원하며, 저희는 다음과 같은 구조로 구성했어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZK5vTa78u_P4mNPqT2hcug.png" /></figure><ol><li>Knwoledge Base</li></ol><ul><li>Bedrock Service에서 사기이력에 관련된 Knowledge Base를 생성했어요.</li><li>Data Source를 연결하고 데이터 삭제 정책을 DELETE로 결정했어요.</li></ul><p>2. Vector DB</p><ul><li>Opensearch serverless를 선택했어요.</li><li>Bedrock Knowledge Base에서 연결이 쉬운 Vector DB는 Opensearch라고 생각했어요.</li><li>FDS에서 LLM 호출이 간헐적이고 건수가 적기 때문에 비용 효율 측면에서 Opensearch serverless를 선택했어요.</li><li>인덱스 벡터 필드 설정값은 속도보다는 정확도에 초점을 맞춰서 아래와 같이 설정했어요.</li><li>dimension: 1024 (기본값)</li><li>space_type: cosinesimil (코사인 유사도)</li><li>ef_construction: 256 (그래프 구성 시 탐색 범위, 높을수록 정확도 향상)</li><li>m: 48 (노드당 이웃 연결 수, 일반 권장값보다 높여 검색 정확도 최대화, 값이 높을수록 메모리 사용량 증가)</li></ul><p>3. Data Source</p><ul><li>사기이력 Knowledge Base의 데이터 소스로 S3를 선택했어요.</li><li>도메인별로 분리시키고 날짜 기준으로 임베딩 대상 데이터를 관리하도록 했어요.</li><li>임베딩 대상이 되는 데이터는 Json형태로 저장해요.</li></ul><pre>s3://karrotpay-fds/<br>└── knowledge-base/                   # FDS 관련 데이터 통합 관리<br>    ├── marketplace/                  # 도메인 1: 마켓플레이스 사기 탐지<br>    │   └── raw/                      # 원본 데이터 (Bedrock 소스)<br>    │       └── year=2025/<br>    │           └── month=10/<br>    │               └── day=22/<br>    │                   ├── fraud_FRAUD-2025-000001.json<br>    │                   ├── fraud_FRAUD-2025-000002.json<br>    │                   └── _SUCCESS   <br>    └── atm/                          # 도메인 2: ATM 모니터링 (추가 도메인)<br>        └── raw/</pre><h4><strong>Lessons Learned</strong></h4><p><strong>문제 상황</strong></p><p>매일 새롭게 임베딩 해야 하는 사기 이력 데이터는 평균적으로 약 200건 정도 돼요. 처음에는 하나의 Jsonl 파일에 20건씩, 한 줄에 하나의 Json 객체로 데이터를 넣고 임베딩을 진행했어요.</p><p>하지만 이후 retrieve 시, 결과의 content가 완전한 Json 형태가 아니라 일부가 잘려서 나오는 문제가 발생했어요.</p><pre>{<br>  &quot;results&quot;: [<br>    {<br>      &quot;score&quot;: 0.87383044,<br>      &quot;content&quot;: ...\&quot;verify_updated_days\&quot;:0} {\&quot;fraud_id\&quot;:\&quot;FRAUD-2025-000004\&quot;...,<br>      &quot;location&quot;: &quot;s3://karrotpay-fds/knowledge-base/marketplace/raw/year=2025/month=10/day=28/fraud_job_20251028_001.jsonl&quot;<br>    }<br>  ]<br>}</pre><p><strong>원인 분석</strong></p><p>문제의 원인은 Knowledge Base의 Chunking 방식에 있었어요. FDS에서는 Fixed size chunking을 사용했고, maxTokens 파라미터 기준으로 텍스트를 일정 크기로 잘라 임베딩하고 있었어요. 이 때문에 유사도가 높은 Json 라인이 아니라 연속된 chunk 단위로 결과가 반환되면서 retrieval 시 content가 잘리게 된 거예요.</p><pre>{&quot;fraud_id&quot;:&quot;FRAUD-2025-000003&quot;,...,&quot;verify_updated_days&quot;:0}<br>{&quot;fraud_id&quot;:&quot;FRAUD-2025-000004&quot;,...,&quot;verify_updated_days&quot;:0}<br>{&quot;fraud_id&quot;:&quot;FRAUD-2025-000005&quot;,...,&quot;verify_updated_days&quot;:0}<br><br>Bedrock Chunking 후:<br>Chunk 1: {&quot;fraud_id&quot;:&quot;FRAUD-2025-000003&quot;,...,&quot;verify_upd<br>Chunk 2: ated_days&quot;:0} {&quot;fraud_id&quot;:&quot;FRAUD-2025-000004&quot;,...<br>Chunk 3: ...,&quot;verify_updated_days&quot;:0}</pre><p><strong>해결 방법</strong></p><p>해결 방법은 크게 두 가지가 있었어요.</p><ol><li>Knowledge Base Chunking 설정을 Semantic으로 변경</li></ol><ul><li>의미적 유사도를 기반으로 자연스러운 경계를 찾아 청킹하는 방식이에요.</li><li>breakpointPercentileThreshold, bufferSize, maxTokens 등의 파라미터로 제어할 수 있어요</li><li>하지만 청크 사이즈를 적절히 설정하고 의미 단위를 정확히 찾아야 하며, 사기 이력 데이터의 가변성이 크면 완전한 해결이 어려운 게 단점이에요.</li></ul><p>2. 각 사기 이력을 Json 파일로 분리하여 임베딩</p><ul><li>각 파일을 하나의 문서로 인식하고, ChunkingStrategy를 NONE으로 설정하거나, 파일이 충분히 작으면 하나의 청크 내에 들어가도록 처리해요.</li><li>NONE으로 설정 시, 파일 전체가 하나의 청크가 되어 JSON 내용이 잘리지 않아요.</li><li>FIXED_SIZE(기본값)으로 설정 시, 파일이 maxTokens보다 작으면 하나의 청크로 처리되어 내용이 안전하게 유지돼요.</li><li>사기 이력 마다 저장 위치가 명확해져 검색과 구분이 쉬워요.</li></ul><p>이러한 이유로 저희는 2번 방식을 선택했어요. 추가로, 임베딩 대상 데이터를 구조화된 JSON 형태보다는 자연어 문장 형태로 작성했을 때 유사도 검색 정확도가 조금 더 높았어요. 아래 Step03에서 설명드릴 RetrieveAndGenerate 방식을 사용한다면 자연어 텍스트 방식으로 임베딩하는 방식이 더 유리했을 것 같아요.</p><h4>Step03 : Retrieve + Prompt + ConverseAPI</h4><p>FDS에서 RAG를 활용하는 방식으로 두 가지를 테스트 해봤어요.</p><ol><li>RetrieveAndGenerate 방식</li></ol><ul><li>기존 converseAPI 방식에 RAG를 결합하면 더 정확한 결과를 한 번에 받을 수 있을 것이라고 예상했어요.</li><li>하지만 테스트를 진행하면서 프롬프트가 길수록 응답 오류가 늘어나고, 출력 포맷도 안정적으로 유지되지 않는다는 점을 확인했어요.</li><li>Citations을 통해 참조 문서를 제공하긴 하지만, Retrieve 결과에 대한 직접 제어가 어렵고, 어떤 기준으로 문서가 선택되고 조합되었는지 세밀하게 파악하거나 커스터마이징하기 어려웠어요.</li></ul><p>2. Retrieve + ConverseAPI 방식</p><ul><li>Retrieve의 결과로 상위 N개의 스코어값, 검색된 문서 내용, S3의 문서 위치, 메타데이터 등을 명시적으로 볼 수 있었어요.</li><li>이를 바탕으로 어떤 기준으로 Retrieve로 추출한 데이터를 조합해서 프롬프트에 넣을 수 있을지 직접 판단하고 구현할 수 있었어요.</li></ul><p>최종적으로 Retrive + Prompt + ConverseAPI 의 파이프라인은 아래와 같이 완성되었어요.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oj1Abe2UNVITDYXHwG1sCA.png" /></figure><h3>앞으로 더 해야 할 일들</h3><p>아직 FDS에 적용하고 싶은 개선 과제가 몇 가지 추가로 남아있어요.</p><ol><li>LLM 호출 전에 가드레일을 추가하는 일</li></ol><ul><li>가드레일은 AI 모델의 입/출력을 안전하고 적절하게 유지하기 위한 안전장치 및 제약 조건이에요.</li><li>현재도 개인정보가 포함되지 않도록 데이터를 추출하고 있지만, 더 안전하게 운영할 수 있도록 가드레일용 프롬프트를 추가하는 작업을 진행 중이에요.</li></ul><p>2. FDS에서 탐지된 정탐/오탐 데이터를 모두 임베딩해서 활용하는 일</p><ul><li>지금은 실제 사기이력 데이터만 임베딩해서 사용하고 있지만, FDS에서 탐지된 정탐/오탐 데이터를 임베딩한 해당 데이터를 기반으로 converseAPI 호출 프롬프트를 자동 생성하는 일을 기대하고 있어요.</li><li>이렇게 되면 converseAPI를 통한 탐지에서도 정확도를 더 높일 수 있을 것으로 보고 있어요.</li></ul><h3>마무리</h3><p>당근페이에 LLM을 적용하는 과정은 생각보다 쉽지 않았어요. 컴플라이언스팀, 보안팀, SRE팀 등 많은 팀의 노력과 협업이 없었다면 어려웠을 거예요. 이 글을 빌려 감사 인사를 전하고 싶어요.</p><p>AI Powered FDS의 여정은 이제부터 막 시작된 셈이에요. 여러 제약 때문에 속도는 빠르지 않을 수 있지만, FDS는 사용자들이 더 안전하게 금융 서비스를 이용할 수 있도록 계속 진화하고 있어요. 앞으로도 더 정교한 탐지, 더 빠른 대응, 더 안전한 금융 환경을 위해 꾸준히 시스템을 고도화해 나갈 예정이에요. AI와 함께 발전해 나가는 당근페이, 그리고 FDS를 지켜봐주세요!!</p><p>그리고 지금 당근페이에서는 지금 이 순간에도 더 나은 금융 경험을 함께 만들어갈 동료들을 찾고 있어요. 다양한 분야의 엔지니어들이 하나의 팀으로 협업하며 기술적 도전을 이어가고 있고, 더 안전한 금융 서비스를 만들기 위해 서로 배우고 성장하고 있답니다. 만약 이런 문제를 같이 풀고 싶은 마음이 든다면, 아래 링크에서 당근페이 채용공고를 확인해 보세요.</p><p>👉 <a href="https://about.daangn.com/jobs/?corp=KARROT_PAY#_filter">당근페이 채용 공고 바로 가기</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=725c3a1c388d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/daangn/%EB%8B%B9%EA%B7%BC%ED%8E%98%EC%9D%B4-ai-powered-fds%EB%A1%9C-%EA%B0%80%EB%8A%94-%EC%97%AC%EC%A0%95-%EB%A3%B0%EC%97%94%EC%A7%84%EA%B5%AC%EC%B6%95%EB%B6%80%ED%84%B0-llm-%EC%A0%81%EC%9A%A9%EA%B9%8C%EC%A7%80-725c3a1c388d">당근페이 AI Powered FDS로 가는 여정: 룰엔진구축부터 LLM 적용까지</a> was originally published in <a href="https://medium.com/daangn">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>
